{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Improving the Pipeline \n",
    "\n",
    "CAPP 30235 Machine Learning for Public Policy\n",
    "\n",
    "Jonathan Tan\n",
    "\n",
    "May 2, 2019\n",
    "\n",
    "# Part 1: Coding Assignment\n",
    "\n",
    "Since I use most of the new functions prompted below in my analysis of the DonorsChoose data, I'll just briefly explain in this section which new functions I wrote and what they do.\n",
    "\n",
    "### 1. Fix and improve the pipeline code you submitted for the last assignment based on the feedback from the TA. if something critical was pointed out in the feedback, you need to fix it. You'll get the last homework back by ends of thursday so you'll still have time before this one is due to address those comments.\n",
    "\n",
    "I received no feedback from my last assignment, so I have skipped this step.\n",
    "\n",
    "### 2. Add more classifiers to the pipeline. It should at least have Logistic Regression, KÂ­Nearest Neighbor, Decision Trees, SVM, Random Forests, Boosting, and Bagging. The code should have a parameter for running one or more of these classifiers and your analysis should run all of them.\n",
    "\n",
    "#### 2.1 `train_classifier()`\n",
    "\n",
    "`train_classifier()` takes 2 pandas DataFrames (features and labels of training data) and the name of a classifier to fit. It optionally takes a nested dictionary of hyperparameters to use for each. (For this notebook, these parameters can be changed in the `pipeline_config.py` file.) It returns a trained classifier object.\n",
    "\n",
    "### 3. Experiment with different parameters for these classifiers (different values of k for example, as well as parameters that other classifiers have). You should look at the sklearn documentation to see what parameter each classifier can take and what the default values sklearn selects. The labs should be helpful here.\n",
    "\n",
    "Results of experimenting with different parameters (e.g. penalty and C for LogisticRegression, n_estimators for Boosting) can be seen in the parameter dictionary in the `pipeline_config.py` file. I looped over several ranges of reasonable values for each parameter and selected the configuration producing the largest AUC-ROC. These will be the default parameters used in the DonorsChoose analysis part of this assignment.\n",
    "\n",
    "### 4. Add additional evaluation metrics that we've covered in class to the pipeline (accuracy, precision at different levels, recall at different levels, F1, area under curve, and precision-recall curves).\n",
    "\n",
    "#### 4.1 `validate_classifier()`\n",
    "\n",
    "`validate_classifier()` takes 2 dataframes (features and labels for test data) and a pre-trained classifier object as inputs, calculates several evaluation metrics (accuracy, precision, recall, F1, etc.) and returns a dictionary of those metrics.\n",
    "\n",
    "### 5. Create temporal validation function in your pipeline that can create training and test sets over time. You can choose the length of these splits based on analyzing the data. For example, the test sets could be six months long and the training sets could be all the data before each test set.\n",
    "\n",
    "#### 5.1 `split_data_temporal()`\n",
    "\n",
    "`split_data_temporal()` takes a pandas DataFrame and specified label/date column names as inputs, then splits the dataframe on the specified timeframe. The default test set duration is the most recent 1 year of data. It returns two dataframes and two series in order:\n",
    " 1. training features\n",
    " 2. test features\n",
    " 3. training labels\n",
    " 4. test labels\n",
    "\n",
    "\n",
    "# Part 2: Analysis\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "\n",
    "# Import pipeline library, hardcoded config file values\n",
    "import pipeline_library as library\n",
    "import pipeline_config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124976, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectid</th>\n",
       "      <th>teacher_acctid</th>\n",
       "      <th>schoolid</th>\n",
       "      <th>school_ncesid</th>\n",
       "      <th>school_latitude</th>\n",
       "      <th>school_longitude</th>\n",
       "      <th>school_city</th>\n",
       "      <th>school_state</th>\n",
       "      <th>school_metro</th>\n",
       "      <th>school_district</th>\n",
       "      <th>...</th>\n",
       "      <th>secondary_focus_subject</th>\n",
       "      <th>secondary_focus_area</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>datefullyfunded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001ccc0e81598c4bd86bacb94d7acb</td>\n",
       "      <td>96963218e74e10c3764a5cfb153e6fea</td>\n",
       "      <td>9f3f9f2c2da7edda5648ccd10554ed8c</td>\n",
       "      <td>1.709930e+11</td>\n",
       "      <td>41.807654</td>\n",
       "      <td>-87.673257</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>urban</td>\n",
       "      <td>Pershing Elem Network</td>\n",
       "      <td>...</td>\n",
       "      <td>Visual Arts</td>\n",
       "      <td>Music &amp; The Arts</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>1498.61</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>4/14/13</td>\n",
       "      <td>5/2/13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000fa3aa8f6649abab23615b546016d</td>\n",
       "      <td>2a578595fe351e7fce057e048c409b18</td>\n",
       "      <td>3432ed3d4466fac2f2ead83ab354e333</td>\n",
       "      <td>6.409801e+10</td>\n",
       "      <td>34.296596</td>\n",
       "      <td>-119.296596</td>\n",
       "      <td>Ventura</td>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Ventura Unif School District</td>\n",
       "      <td>...</td>\n",
       "      <td>Literature &amp; Writing</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Books</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>282.47</td>\n",
       "      <td>28.0</td>\n",
       "      <td>t</td>\n",
       "      <td>4/7/12</td>\n",
       "      <td>4/18/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000134f07d4b30140d63262c871748ff</td>\n",
       "      <td>26bd60377bdbffb53a644a16c5308e82</td>\n",
       "      <td>dc8dcb501c3b2bb0b10e9c6ee2cd8afd</td>\n",
       "      <td>6.227100e+10</td>\n",
       "      <td>34.078625</td>\n",
       "      <td>-118.257834</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Los Angeles Unif Sch Dist</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Sciences</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>1012.38</td>\n",
       "      <td>56.0</td>\n",
       "      <td>f</td>\n",
       "      <td>1/30/12</td>\n",
       "      <td>4/15/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001f2d0b3827bba67cdbeaa248b832d</td>\n",
       "      <td>15d900805d9d716c051c671827109f45</td>\n",
       "      <td>8bea7e8c6e4279fca6276128db89292e</td>\n",
       "      <td>3.600090e+11</td>\n",
       "      <td>40.687286</td>\n",
       "      <td>-73.988217</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "      <td>urban</td>\n",
       "      <td>New York City Dept Of Ed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Books</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>175.33</td>\n",
       "      <td>23.0</td>\n",
       "      <td>f</td>\n",
       "      <td>10/11/12</td>\n",
       "      <td>12/5/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004536db996ba697ca72c9e058bfe69</td>\n",
       "      <td>400f8b82bb0143f6a40b217a517fe311</td>\n",
       "      <td>fbdefab6fe41e12c55886c610c110753</td>\n",
       "      <td>3.606870e+11</td>\n",
       "      <td>40.793018</td>\n",
       "      <td>-73.205635</td>\n",
       "      <td>Central Islip</td>\n",
       "      <td>NY</td>\n",
       "      <td>suburban</td>\n",
       "      <td>Central Islip Union Free SD</td>\n",
       "      <td>...</td>\n",
       "      <td>Literature &amp; Writing</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>3591.11</td>\n",
       "      <td>150.0</td>\n",
       "      <td>f</td>\n",
       "      <td>1/8/13</td>\n",
       "      <td>3/25/13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          projectid                    teacher_acctid  \\\n",
       "0  00001ccc0e81598c4bd86bacb94d7acb  96963218e74e10c3764a5cfb153e6fea   \n",
       "1  0000fa3aa8f6649abab23615b546016d  2a578595fe351e7fce057e048c409b18   \n",
       "2  000134f07d4b30140d63262c871748ff  26bd60377bdbffb53a644a16c5308e82   \n",
       "3  0001f2d0b3827bba67cdbeaa248b832d  15d900805d9d716c051c671827109f45   \n",
       "4  0004536db996ba697ca72c9e058bfe69  400f8b82bb0143f6a40b217a517fe311   \n",
       "\n",
       "                           schoolid  school_ncesid  school_latitude  \\\n",
       "0  9f3f9f2c2da7edda5648ccd10554ed8c   1.709930e+11        41.807654   \n",
       "1  3432ed3d4466fac2f2ead83ab354e333   6.409801e+10        34.296596   \n",
       "2  dc8dcb501c3b2bb0b10e9c6ee2cd8afd   6.227100e+10        34.078625   \n",
       "3  8bea7e8c6e4279fca6276128db89292e   3.600090e+11        40.687286   \n",
       "4  fbdefab6fe41e12c55886c610c110753   3.606870e+11        40.793018   \n",
       "\n",
       "   school_longitude    school_city school_state school_metro  \\\n",
       "0        -87.673257        Chicago           IL        urban   \n",
       "1       -119.296596        Ventura           CA        urban   \n",
       "2       -118.257834    Los Angeles           CA        urban   \n",
       "3        -73.988217       Brooklyn           NY        urban   \n",
       "4        -73.205635  Central Islip           NY     suburban   \n",
       "\n",
       "                school_district       ...       secondary_focus_subject  \\\n",
       "0         Pershing Elem Network       ...                   Visual Arts   \n",
       "1  Ventura Unif School District       ...          Literature & Writing   \n",
       "2     Los Angeles Unif Sch Dist       ...               Social Sciences   \n",
       "3      New York City Dept Of Ed       ...                           NaN   \n",
       "4   Central Islip Union Free SD       ...          Literature & Writing   \n",
       "\n",
       "  secondary_focus_area resource_type    poverty_level    grade_level  \\\n",
       "0     Music & The Arts      Supplies  highest poverty  Grades PreK-2   \n",
       "1  Literacy & Language         Books  highest poverty     Grades 3-5   \n",
       "2     History & Civics    Technology     high poverty     Grades 3-5   \n",
       "3                  NaN         Books     high poverty  Grades PreK-2   \n",
       "4  Literacy & Language    Technology     high poverty  Grades PreK-2   \n",
       "\n",
       "  total_price_including_optional_support students_reached  \\\n",
       "0                                1498.61             31.0   \n",
       "1                                 282.47             28.0   \n",
       "2                                1012.38             56.0   \n",
       "3                                 175.33             23.0   \n",
       "4                                3591.11            150.0   \n",
       "\n",
       "  eligible_double_your_impact_match date_posted datefullyfunded  \n",
       "0                                 f     4/14/13          5/2/13  \n",
       "1                                 t      4/7/12         4/18/12  \n",
       "2                                 f     1/30/12         4/15/12  \n",
       "3                                 f    10/11/12         12/5/12  \n",
       "4                                 f      1/8/13         3/25/13  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = library.read_data(config.DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['projectid', 'teacher_acctid', 'schoolid', 'school_ncesid',\n",
       "       'school_latitude', 'school_longitude', 'school_city', 'school_state',\n",
       "       'school_metro', 'school_district', 'school_county', 'school_charter',\n",
       "       'school_magnet', 'teacher_prefix', 'primary_focus_subject',\n",
       "       'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area',\n",
       "       'resource_type', 'poverty_level', 'grade_level',\n",
       "       'total_price_including_optional_support', 'students_reached',\n",
       "       'eligible_double_your_impact_match', 'date_posted', 'datefullyfunded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a32a48b38>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAGhdJREFUeJzt3X+MXeWd3/H3pyawjt1gE8LIsd3akSZ0Ae86eETcpomuAwFD2JisltYIgQmsJolgm1SWFrNpRRoWle2GpEViiZzg2nRTJjRAsMBZ4nW5pSsBsZ1Q/wgQD+CFwa69wfyagMhO+u0f5xntyTx3PHPvnZn7w5+XdHXP+Z7nnPt8de7c75znnHuPIgIzM7Oyf9TqDpiZWftxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWOanVHWjU6aefHkuWLOGXv/wlc+bMaXV3ppRz6gzOqf11Wz7QfE67d+/+RUR8YKJ2HVsclixZwq5du6hWq1QqlVZ3Z0o5p87gnNpft+UDzeck6W8n087DSmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpbp2G9IW32WbHikJa978LZPt+R1zaw5PnIwM7OMi4OZmWVOyGElD7GYmR3fhEcOkhZLekzSM5L2S/pSip8mabukA+l5fopL0h2SBiXtkXRuaVvrUvsDktaV4isk7U3r3CFJ05GsmZlNzmSGlUaA9RHx28BK4HpJZwEbgB0R0QvsSPMAFwO96dEP3AVFMQFuBj4KnAfcPFpQUpv+0nqrm0/NzMwaNWFxiIjDEfGTNP0W8AywEFgDbEnNtgCXpek1wD1ReBKYJ2kBcBGwPSKORcRrwHZgdVr2voh4IiICuKe0LTMzawEVn8eTbCwtAR4HzgFeioh5pWWvRcR8SQ8Dt0XE36T4DuBGoAL8VkT8aYr/e+AdoJraX5DiHwdujIhLa7x+P8URBj09PSsGBgYYHh5m7ty5dSW995U36mo/VZYtPHVS7RrJaSKtznk6cmo159T+ui0faD6nVatW7Y6IvonaTfqEtKS5wP3AlyPizeOcFqi1IBqI58GIjcBGgL6+vqhUKg3dFemaVp2QvrIyqXbTcfeqVufsO3J1hm7LqdvygZnLaVKXskp6D0Vh+G5EPJDCR9KQEOn5aIoPAYtLqy8CDk0QX1QjbmZmLTKZq5UE3A08ExHfKC3aCoxecbQOeKgUvzpdtbQSeCMiDgOPAhdKmp9ORF8IPJqWvSVpZXqtq0vbMjOzFpjMsNLHgKuAvZKeTrE/AW4D7pN0HfAScHlatg24BBgE3gY+BxARxyTdAuxM7b4WEcfS9BeBzcBs4IfpYWZmLTJhcUgnlsc7wXB+jfYBXD/OtjYBm2rEd1Gc5DYzszbgn88wM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnmhLyfQ6tM9j4S65eNtOznLqbaaM6tyMn3zzBrnI8czMws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLDOZ24RuknRU0r5S7HuSnk6Pg6N3iJO0RNI7pWXfKq2zQtJeSYOS7ki3BEXSaZK2SzqQnudPR6JmZjZ5kzly2AysLgci4l9HxPKIWA7cDzxQWvz86LKI+EIpfhfQD/Smx+g2NwA7IqIX2JHmzcyshSYsDhHxOHCs1rL03/+/Au493jYkLQDeFxFPpNuI3gNclhavAbak6S2luJmZtYiKz+oJGklLgIcj4pwx8U8A34iIvlK7/cDPgTeBfxcR/1tSH3BbRFyQ2n0cuDEiLpX0ekTMK23ztYioObQkqZ/i6IOenp4VAwMDDA8PM3fu3LqS3vvKG3W1n2k9s+HIO63uxdRqRU7LFp46rdtv5L3X7rotp27LB5rPadWqVbtHP7OPp9lfZb2C3zxqOAz8k4h4VdIK4AeSzgZUY92Jq9LYFSI2AhsB+vr6olKpUK1WqVQqdW2n3X/xdP2yEW7f210/mNuKnA5eWZnW7Tfy3mt33ZZTt+UDM5dTw3+tkk4Cfh9YMRqLiHeBd9P0bknPAx8GhoBFpdUXAYfS9BFJCyLicBp+Otpon8zMbGo0cynrBcCzETE0GpD0AUmz0vSHKE48vxARh4G3JK1M5ymuBh5Kq20F1qXpdaW4mZm1yGQuZb0XeAI4U9KQpOvSorXkJ6I/AeyR9H+A7wNfiIjRk9lfBL4DDALPAz9M8duAT0k6AHwqzZuZWQtNOKwUEVeME7+mRux+iktba7XfBZxTI/4qcP5E/TAzs5njb0ibmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8tM5k5wmyQdlbSvFPuqpFckPZ0el5SW3SRpUNJzki4qxVen2KCkDaX4UklPSTog6XuSTp7KBM3MrH6TOXLYDKyuEf9mRCxPj20Aks6iuH3o2Wmdv5A0K91X+k7gYuAs4IrUFuDP0rZ6gdeA68a+kJmZzawJi0NEPA4cm6hdsgYYiIh3I+JFivtFn5cegxHxQkT8ChgA1kgS8EmK+00DbAEuqzMHMzObYhPeQ/o4bpB0NbALWB8RrwELgSdLbYZSDODlMfGPAu8HXo+IkRrtM5L6gX6Anp4eqtUqw8PDVKvVujq+ftnIxI1aqGd2+/exXq3Iqd73Rb0aee+1u27LqdvygZnLqdHicBdwCxDp+XbgWkA12ga1j1DiOO1rioiNwEaAvr6+qFQqVKtVKpVKXZ2/ZsMjdbWfaeuXjXD73mbqdvtpRU4Hr6xM6/Ybee+1u27LqdvygZnLqaG/1og4Mjot6dvAw2l2CFhcaroIOJSma8V/AcyTdFI6eii3NzOzFmnoUlZJC0qznwVGr2TaCqyVdIqkpUAv8GNgJ9Cbrkw6meKk9daICOAx4A/S+uuAhxrpk5mZTZ0Jjxwk3QtUgNMlDQE3AxVJyymGgA4CnweIiP2S7gN+BowA10fEr9N2bgAeBWYBmyJif3qJG4EBSX8K/BS4e8qyMzOzhkxYHCLiihrhcT/AI+JW4NYa8W3AthrxFyiuZjIzszbhb0ibmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmaZCYuDpE2SjkraV4r9uaRnJe2R9KCkeSm+RNI7kp5Oj2+V1lkhaa+kQUl3SFKKnyZpu6QD6Xn+dCRqZmaTN5kjh83A6jGx7cA5EfE7wM+Bm0rLno+I5enxhVL8LqCf4tahvaVtbgB2REQvsCPNm5lZC01YHCLiceDYmNiPImIkzT4JLDreNtI9p98XEU+k+0bfA1yWFq8BtqTpLaW4mZm1yFScc7gW+GFpfqmkn0r6X5I+nmILgaFSm6EUA+iJiMMA6fmMKeiTmZk1QcU/8hM0kpYAD0fEOWPiXwH6gN+PiJB0CjA3Il6VtAL4AXA2cCbwHyPigrTex4E/jojfk/R6RMwrbfO1iKh53kFSP8XQFD09PSsGBgYYHh5m7ty5dSW995U36mo/03pmw5F3Wt2LqdWKnJYtPHVat9/Ie6/ddVtO3ZYPNJ/TqlWrdkdE30TtTmr0BSStAy4Fzk9DRUTEu8C7aXq3pOeBD1McKZSHnhYBh9L0EUkLIuJwGn46Ot5rRsRGYCNAX19fVCoVqtUqlUqlrr5fs+GRutrPtPXLRrh9b8O7pi21IqeDV1amdfuNvPfaXbfl1G35wMzl1NCwkqTVwI3AZyLi7VL8A5JmpekPUZx4fiENF70laWW6Sulq4KG02lZgXZpeV4qbmVmLTPivnKR7gQpwuqQh4GaKq5NOAbanK1KfTFcmfQL4mqQR4NfAFyJi9GT2FymufJpNcY5i9DzFbcB9kq4DXgIun5LMzMysYRMWh4i4okb47nHa3g/cP86yXcA5NeKvAudP1A8zM5s5/oa0mZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7PMpIqDpE2SjkraV4qdJmm7pAPpeX6KS9IdkgYl7ZF0bmmddan9gXQP6tH4Ckl70zp3pFuJmplZi0z2yGEzsHpMbAOwIyJ6gR1pHuBiintH9wL9wF1QFBOKW4x+FDgPuHm0oKQ2/aX1xr6WmZnNoAlvEwoQEY9LWjImvIbi3tIAW4AqcGOK3xMRATwpaZ6kBant9tF7SkvaDqyWVAXeFxFPpPg9wGX8wz2mzRqyZMMj07r99ctGuKbGaxy87dPT+rpmM6GZcw49EXEYID2fkeILgZdL7YZS7HjxoRpxMzNrkUkdOdSp1vmCaCCeb1jqpxh+oqenh2q1yvDwMNVqta4Orl82Ulf7mdYzu/37WK8TKad634/tpJG/p3bWbfnAzOXUTHE4ImlBRBxOw0ZHU3wIWFxqtwg4lOKVMfFqii+q0T4TERuBjQB9fX1RqVSoVqtUKpVazcdVayignaxfNsLte6ejbrfOiZTTwSsrM9+ZKdLI31M767Z8YOZyamZYaSswesXROuChUvzqdNXSSuCNNOz0KHChpPnpRPSFwKNp2VuSVqarlK4ubcvMzFpgUv/KSbqX4r/+0yUNUVx1dBtwn6TrgJeAy1PzbcAlwCDwNvA5gIg4JukWYGdq97XRk9PAFymuiJpNcSLaJ6PNzFposlcrXTHOovNrtA3g+nG2swnYVCO+CzhnMn0xM7Pp529Im5lZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLNFwcJJ0p6enS401JX5b0VUmvlOKXlNa5SdKgpOckXVSKr06xQUkbmk3KzMya0/Ad3yPiOWA5gKRZwCvAgxS3Bf1mRHy93F7SWcBa4Gzgg8BfS/pwWnwn8ClgCNgpaWtE/KzRvpmZWXMaLg5jnA88HxF/K2m8NmuAgYh4F3hR0iBwXlo2GBEvAEgaSG1dHMzqtGTDI02tv37ZCNc0sI2Dt326qddtxvFybjSfdrZ59ZwZeZ2pOuewFri3NH+DpD2SNkman2ILgZdLbYZSbLy4mZm1iCKiuQ1IJwOHgLMj4oikHuAXQAC3AAsi4lpJdwJPRMRfpvXuBrZRFKiLIuIPU/wq4LyI+KMar9UP9AP09PSsGBgYYHh4mLlz59bV572vvNFgtjOjZzYceafVvZhaJ1JOyxaeOvOdSZp9bze6n9o152583y09dVbdn3llq1at2h0RfRO1m4phpYuBn0TEEYDRZwBJ3wYeTrNDwOLSeosoigrHif+GiNgIbATo6+uLSqVCtVqlUqnU1eF2P8xcv2yE2/dO1YhfeziRcjp4ZWXmO5M0+95udD+1a87d+L7bvHpO3Z95jZiKYaUrKA0pSVpQWvZZYF+a3gqslXSKpKVAL/BjYCfQK2lpOgpZm9qamVmLNFVSJb2X4iqjz5fC/0nScophpYOjyyJiv6T7KE40jwDXR8Sv03ZuAB4FZgGbImJ/M/0yM7PmNFUcIuJt4P1jYlcdp/2twK014tsozj+YWQdq9iopaz/+hrSZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy3TXD52btQH/CJ11Ax85mJlZxsXBzMwyLg5mZpZpujhIOihpr6SnJe1KsdMkbZd0ID3PT3FJukPSoKQ9ks4tbWddan9A0rpm+2VmZo2bqiOHVRGxPCL60vwGYEdE9AI70jzAxRT3ju4F+oG7oCgmwM3AR4HzgJtHC4qZmc286RpWWgNsSdNbgMtK8Xui8CQwT9IC4CJge0Qci4jXgO3A6mnqm5mZTWAqikMAP5K0W1J/ivVExGGA9HxGii8EXi6tO5Ri48XNzKwFpuJ7Dh+LiEOSzgC2S3r2OG1VIxbHif/mykXx6Qfo6emhWq0yPDxMtVqtq8Prl43U1X6m9cxu/z7Wyzl1hm7LqdvyARr6zGtE08UhIg6l56OSHqQ4Z3BE0oKIOJyGjY6m5kPA4tLqi4BDKV4ZE6/WeK2NwEaAvr6+qFQqVKtVKpXK2KbHdU2bf0lp/bIRbt/bXd9PdE6dodty6rZ8ADavnlP3Z14jmhpWkjRH0j8enQYuBPYBW4HRK47WAQ+l6a3A1emqpZXAG2nY6VHgQknz04noC1PMzMxaoNmS2gM8KGl0W/89Iv5K0k7gPknXAS8Bl6f224BLgEHgbeBzABFxTNItwM7U7msRcazJvpmZWYOaKg4R8QLwuzXirwLn14gHcP0429oEbGqmP2ZmNjX8DWkzM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZpmGi4OkxZIek/SMpP2SvpTiX5X0iqSn0+OS0jo3SRqU9Jyki0rx1Sk2KGlDcymZmVmzmrkT3AiwPiJ+ku4jvVvS9rTsmxHx9XJjSWcBa4GzgQ8Cfy3pw2nxncCngCFgp6StEfGzJvpmZmZNaLg4RMRh4HCafkvSM8DC46yyBhiIiHeBFyUNAuelZYPplqNIGkhtXRzMzFpkSs45SFoCfAR4KoVukLRH0iZJ81NsIfByabWhFBsvbmZmLdLMsBIAkuYC9wNfjog3Jd0F3AJEer4duBZQjdWD2gUqxnmtfqAfoKenh2q1yvDwMNVqta4+r182Ulf7mdYzu/37WC/n1Bm6Laduywdo6DOvEU0VB0nvoSgM342IBwAi4khp+beBh9PsELC4tPoi4FCaHi/+GyJiI7ARoK+vLyqVCtVqlUqlUle/r9nwSF3tZ9r6ZSPcvrfput1WnFNn6Lacui0fgM2r59T9mdeIZq5WEnA38ExEfKMUX1Bq9llgX5reCqyVdIqkpUAv8GNgJ9AraamkkylOWm9ttF9mZta8Zkrqx4CrgL2Snk6xPwGukLScYmjoIPB5gIjYL+k+ihPNI8D1EfFrAEk3AI8Cs4BNEbG/iX6ZmVmTmrla6W+ofR5h23HWuRW4tUZ82/HWMzOzmeVvSJuZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7RNcZC0WtJzkgYlbWh1f8zMTmRtURwkzQLuBC4GzqK41ehZre2VmdmJqy2KA3AeMBgRL0TEr4ABYE2L+2RmdsJql+KwEHi5ND+UYmZm1gKKiFb3AUmXAxdFxB+m+auA8yLij8a06wf60+yZwHPA6cAvZrC7M8E5dQbn1P66LR9oPqd/GhEfmKjRSU28wFQaAhaX5hcBh8Y2ioiNwMZyTNKuiOib3u7NLOfUGZxT++u2fGDmcmqXYaWdQK+kpZJOBtYCW1vcJzOzE1ZbHDlExIikG4BHgVnApojY3+JumZmdsNqiOABExDZgWwOrbpy4ScdxTp3BObW/bssHZiintjghbWZm7aVdzjmYmVkb6dji0A0/tyFpsaTHJD0jab+kL6X4aZK2SzqQnue3uq/1kjRL0k8lPZzml0p6KuX0vXThQceQNE/S9yU9m/bXP+/0/STp36b33T5J90r6rU7bT5I2SToqaV8pVnO/qHBH+szYI+nc1vV8fOPk9OfpvbdH0oOS5pWW3ZRyek7SRVPVj44sDl30cxsjwPqI+G1gJXB9ymMDsCMieoEdab7TfAl4pjT/Z8A3U06vAde1pFeN+y/AX0XEPwN+lyK3jt1PkhYC/wboi4hzKC4EWUvn7afNwOoxsfH2y8VAb3r0A3fNUB/rtZk8p+3AORHxO8DPgZsA0ufFWuDstM5fpM/HpnVkcaBLfm4jIg5HxE/S9FsUHzgLKXLZkpptAS5rTQ8bI2kR8GngO2lewCeB76cmHZWTpPcBnwDuBoiIX0XE63T4fqK4IGW2pJOA9wKH6bD9FBGPA8fGhMfbL2uAe6LwJDBP0oKZ6enk1copIn4UESNp9kmK74JBkdNARLwbES8CgxSfj03r1OLQdT+3IWkJ8BHgKaAnIg5DUUCAM1rXs4b8Z+CPgf+X5t8PvF56c3fa/voQ8HfAf01DZd+RNIcO3k8R8QrwdeAliqLwBrCbzt5Po8bbL93yuXEt8MM0PW05dWpxUI1Yx152JWkucD/w5Yh4s9X9aYakS4GjEbG7HK7RtJP210nAucBdEfER4Jd00BBSLWkcfg2wFPggMIdi2GWsTtpPE+n09yGSvkIxHP3d0VCNZlOSU6cWh0n93EYnkPQeisLw3Yh4IIWPjB7upuejrepfAz4GfEbSQYrhvk9SHEnMS8MX0Hn7awgYioin0vz3KYpFJ++nC4AXI+LvIuLvgQeAf0Fn76dR4+2Xjv7ckLQOuBS4Mv7hOwjTllOnFoeu+LmNNBZ/N/BMRHyjtGgrsC5NrwMemum+NSoiboqIRRGxhGK//M+IuBJ4DPiD1KzTcvq/wMuSzkyh84Gf0cH7iWI4aaWk96b34WhOHbufSsbbL1uBq9NVSyuBN0aHn9qdpNXAjcBnIuLt0qKtwFpJp0haSnGy/cdT8qIR0ZEP4BKKs/bPA19pdX8azOFfUhwC7gGeTo9LKMbodwAH0vNpre5rg/lVgIfT9IfSm3YQ+B/AKa3uX525LAd2pX31A2B+p+8n4D8AzwL7gP8GnNJp+wm4l+Kcyd9T/Bd93Xj7hWII5s70mbGX4kqtlucwyZwGKc4tjH5OfKvU/ispp+eAi6eqH/6GtJmZZTp1WMnMzKaRi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlvn/1Hq4Sn9uMVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of number of days to being fully funded\n",
    "df['date_posted'] = pd.to_datetime(df['date_posted'], format='%m/%d/%y')\n",
    "df['datefullyfunded'] = pd.to_datetime(df['datefullyfunded'], format='%m/%d/%y')\n",
    "df['days_to_funded'] = (df['datefullyfunded'] - df['date_posted']).dt.days\n",
    "df['days_to_funded'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7118646780181795"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of projects are funded within 60 days?\n",
    "len(df.loc[df['days_to_funded'] <= 60]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>124976.000000</td>\n",
       "      <td>124917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>654.011811</td>\n",
       "      <td>95.445760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1098.015854</td>\n",
       "      <td>163.481912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>345.810000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>510.500000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>752.960000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>164382.840000</td>\n",
       "      <td>12143.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_price_including_optional_support  students_reached\n",
       "count                           124976.000000     124917.000000\n",
       "mean                               654.011811         95.445760\n",
       "std                               1098.015854        163.481912\n",
       "min                                 92.000000          1.000000\n",
       "25%                                345.810000         23.000000\n",
       "50%                                510.500000         30.000000\n",
       "75%                                752.960000        100.000000\n",
       "max                             164382.840000      12143.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = ['total_price_including_optional_support', 'students_reached']\n",
    "\n",
    "library.describe_data(df, varlist=numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-process data / Generate features/predictors\n",
    "\n",
    "To avoid pre-processing data that will be omitted from the training data, below I pre-select the features I want to use and focus on cleaning those.\n",
    "\n",
    "Data cleaning steps:\n",
    "1. Select features to be used in final model(s)\n",
    "2. Define label: `funded_in_60_days`\n",
    "3. Clean missing data\n",
    "4. Normalize numeric data\n",
    "5. Make binary data into true binary\n",
    "6. Make categorical data into dummies\n",
    "\n",
    "### 3.1 Select features for the final model(s)\n",
    "\n",
    "Here, I've excluded extraneous features like the ID columns and lat/long data, as well as geographical features like `school_district` and `school_city` that should be highly collinear with the remaining `school_county` and `school_state` features. The remaining features are, as decribed by the [datasource](https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/data):\n",
    "\n",
    "| Feature | Type | Description\n",
    "| --- | --- | ---\n",
    "`school_state` | categorical | State in which school is located\n",
    "`school_metro` | categorical | Rural, urban, or suburban\n",
    "`school_charter` | boolean | Is the school a public charter school?\n",
    "`school_magnet` | boolean | Is the school a public magnet school?\n",
    "`teacher_prefix` | categorical | Dr., Mr., Mrs., or Ms.\n",
    "`primary_focus_area`| categorical | Main subject area for which project materials are intended\n",
    "`secondary_focus_area` | categorical | Secondary subject area for which project materials are intended\n",
    "`resource_type` | categorical | Books, supplies, technology, etc.\n",
    "`poverty_level` | categorical | Low, moderate, high, highest\n",
    "`grade_level` | categorical | Grade level for which project materials are intended\n",
    "`total_price_including_optional_support` | categorical | Project cost including optional tip that donors give to DonorsChoose.org while funding a project \n",
    "`students_reached` | numeric | Number of students impacted by a project (if funded)\n",
    "`eligible_double_your_impact_match` | boolean | Project was eligible for a 50% off offer by a corporate partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade_level\n",
       "Grades 3-5       39242\n",
       "Grades 6-8       21479\n",
       "Grades 9-12      16522\n",
       "Grades PreK-2    47730\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade_level').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['school_state', 'school_metro', 'school_charter',\n",
    "       'school_magnet', 'teacher_prefix', 'primary_focus_area', 'secondary_focus_area',\n",
    "       'resource_type', 'poverty_level', 'grade_level',\n",
    "       'total_price_including_optional_support', 'students_reached',\n",
    "       'eligible_double_your_impact_match', 'date_posted', 'datefullyfunded']\n",
    "\n",
    "df = df[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define label: `funded_in_60_days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_posted</th>\n",
       "      <th>datefullyfunded</th>\n",
       "      <th>fully_funded_in_60_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>2012-04-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>2012-04-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-11</td>\n",
       "      <td>2012-12-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>2013-03-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_posted datefullyfunded  fully_funded_in_60_days\n",
       "0  2013-04-14      2013-05-02                        1\n",
       "1  2012-04-07      2012-04-18                        1\n",
       "2  2012-01-30      2012-04-15                        0\n",
       "3  2012-10-11      2012-12-05                        1\n",
       "4  2013-01-08      2013-03-25                        0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fully_funded_in_60_days'] = \\\n",
    "    (df['datefullyfunded'] - df['date_posted'] <= pd.to_timedelta(60, unit='days')).astype('int')\n",
    "\n",
    "df[['date_posted', 'datefullyfunded', 'fully_funded_in_60_days']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=['datefullyfunded'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clean missing data\n",
    "\n",
    "First, let's see which columns are missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "school_state                                  0\n",
       "school_metro                              15224\n",
       "school_charter                                0\n",
       "school_magnet                                 0\n",
       "teacher_prefix                                0\n",
       "primary_focus_area                           15\n",
       "secondary_focus_area                      40556\n",
       "resource_type                                17\n",
       "poverty_level                                 0\n",
       "grade_level                                   3\n",
       "total_price_including_optional_support        0\n",
       "students_reached                             59\n",
       "eligible_double_your_impact_match             0\n",
       "date_posted                                   0\n",
       "fully_funded_in_60_days                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of these, mostly categorical features, we have no reliable way of filling in the missing data with useful placeholders. One way to handle it is to leave missing values as they are, and when converting categorical data to a series of dummy features later, these will have a specific binary column. \n",
    "\n",
    "For the numeric column with missing data - students reached - we will fill it in with the mean value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# school_metro - 15224 missing, categorical - leave as is\n",
    "# primary_focus_area - 15 missing, categorical - leave as is\n",
    "# secondary_focus_area - 40556 missing, categorical - leave as is\n",
    "# resource_type - 17 missing, categorical - leave as is\n",
    "# grade_level - 3 missing, leave as is\n",
    "\n",
    "# students_reached - 59 missing, integer, fill with mean\n",
    "df['students_reached'] = library.fill_missing(df['students_reached'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Rescale/standardize numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.249760e+05</td>\n",
       "      <td>1.249760e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.330760e-17</td>\n",
       "      <td>8.715237e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000004e+00</td>\n",
       "      <td>1.000004e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.118451e-01</td>\n",
       "      <td>-5.778526e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.806909e-01</td>\n",
       "      <td>-4.432488e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.307016e-01</td>\n",
       "      <td>-3.943019e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.011581e-02</td>\n",
       "      <td>2.786445e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.491140e+02</td>\n",
       "      <td>7.371120e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_price_including_optional_support  students_reached\n",
       "count                            1.249760e+05      1.249760e+05\n",
       "mean                            -5.330760e-17      8.715237e-17\n",
       "std                              1.000004e+00      1.000004e+00\n",
       "min                             -5.118451e-01     -5.778526e-01\n",
       "25%                             -2.806909e-01     -4.432488e-01\n",
       "50%                             -1.307016e-01     -3.943019e-01\n",
       "75%                              9.011581e-02      2.786445e-02\n",
       "max                              1.491140e+02      7.371120e+01"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in numeric_features:\n",
    "    df[i] = scale(df[i])\n",
    "    \n",
    "df[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Make binary data into true binary format\n",
    "\n",
    "The `school_charter`, `school_magnet` and `eligible_double_your_impact_match` features are also binary variables coded as string variables (\"t\" or \"f\"), so we'll convert them to a true binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_charter  school_magnet  eligible_double_your_impact_match\n",
       "0               0              0                                  0\n",
       "1               0              0                                  1\n",
       "2               0              0                                  0\n",
       "3               0              1                                  0\n",
       "4               0              0                                  0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features = ['school_charter', 'school_magnet', 'eligible_double_your_impact_match']\n",
    "\n",
    "for i in binary_features:\n",
    "    df[i] = np.where(df[i] == 't', 1, 0)\n",
    "\n",
    "df[binary_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Make categorical data into dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>fully_funded_in_60_days</th>\n",
       "      <th>school_state_AK</th>\n",
       "      <th>school_state_AL</th>\n",
       "      <th>school_state_AR</th>\n",
       "      <th>...</th>\n",
       "      <th>secondary_focus_area_Music &amp; The Arts</th>\n",
       "      <th>secondary_focus_area_Special Needs</th>\n",
       "      <th>poverty_level_high poverty</th>\n",
       "      <th>poverty_level_highest poverty</th>\n",
       "      <th>poverty_level_low poverty</th>\n",
       "      <th>poverty_level_moderate poverty</th>\n",
       "      <th>grade_level_Grades 3-5</th>\n",
       "      <th>grade_level_Grades 6-8</th>\n",
       "      <th>grade_level_Grades 9-12</th>\n",
       "      <th>grade_level_Grades PreK-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769207</td>\n",
       "      <td>-0.394302</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.338377</td>\n",
       "      <td>-0.412657</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.326379</td>\n",
       "      <td>-0.241343</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.435953</td>\n",
       "      <td>-0.443249</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.674925</td>\n",
       "      <td>0.333782</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_charter  school_magnet  total_price_including_optional_support  \\\n",
       "0               0              0                                0.769207   \n",
       "1               0              0                               -0.338377   \n",
       "2               0              0                                0.326379   \n",
       "3               0              1                               -0.435953   \n",
       "4               0              0                                2.674925   \n",
       "\n",
       "   students_reached  eligible_double_your_impact_match date_posted  \\\n",
       "0         -0.394302                                  0  2013-04-14   \n",
       "1         -0.412657                                  1  2012-04-07   \n",
       "2         -0.241343                                  0  2012-01-30   \n",
       "3         -0.443249                                  0  2012-10-11   \n",
       "4          0.333782                                  0  2013-01-08   \n",
       "\n",
       "   fully_funded_in_60_days  school_state_AK  school_state_AL  school_state_AR  \\\n",
       "0                        1                0                0                0   \n",
       "1                        1                0                0                0   \n",
       "2                        0                0                0                0   \n",
       "3                        1                0                0                0   \n",
       "4                        0                0                0                0   \n",
       "\n",
       "             ...              secondary_focus_area_Music & The Arts  \\\n",
       "0            ...                                                  1   \n",
       "1            ...                                                  0   \n",
       "2            ...                                                  0   \n",
       "3            ...                                                  0   \n",
       "4            ...                                                  0   \n",
       "\n",
       "   secondary_focus_area_Special Needs  poverty_level_high poverty  \\\n",
       "0                                   0                           0   \n",
       "1                                   0                           0   \n",
       "2                                   0                           1   \n",
       "3                                   0                           1   \n",
       "4                                   0                           1   \n",
       "\n",
       "   poverty_level_highest poverty  poverty_level_low poverty  \\\n",
       "0                              1                          0   \n",
       "1                              1                          0   \n",
       "2                              0                          0   \n",
       "3                              0                          0   \n",
       "4                              0                          0   \n",
       "\n",
       "   poverty_level_moderate poverty  grade_level_Grades 3-5  \\\n",
       "0                               0                       0   \n",
       "1                               0                       1   \n",
       "2                               0                       1   \n",
       "3                               0                       0   \n",
       "4                               0                       0   \n",
       "\n",
       "   grade_level_Grades 6-8  grade_level_Grades 9-12  grade_level_Grades PreK-2  \n",
       "0                       0                        0                          1  \n",
       "1                       0                        0                          0  \n",
       "2                       0                        0                          0  \n",
       "3                       0                        0                          1  \n",
       "4                       0                        0                          1  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['school_state', 'school_metro', 'teacher_prefix', \\\n",
    "                        'resource_type', 'primary_focus_area', 'secondary_focus_area', \\\n",
    "                        'poverty_level', 'grade_level']\n",
    "\n",
    "for i in categorical_features:\n",
    "    df = library.make_dummy_vars(df, i)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124976, 93)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_charter', 'school_magnet',\n",
       "       'total_price_including_optional_support', 'students_reached',\n",
       "       'eligible_double_your_impact_match', 'date_posted',\n",
       "       'fully_funded_in_60_days', 'school_state_AK', 'school_state_AL',\n",
       "       'school_state_AR', 'school_state_AZ', 'school_state_CA',\n",
       "       'school_state_CO', 'school_state_CT', 'school_state_DC',\n",
       "       'school_state_DE', 'school_state_FL', 'school_state_GA',\n",
       "       'school_state_HI', 'school_state_IA', 'school_state_ID',\n",
       "       'school_state_IL', 'school_state_IN', 'school_state_KS',\n",
       "       'school_state_KY', 'school_state_LA', 'school_state_MA',\n",
       "       'school_state_MD', 'school_state_ME', 'school_state_MI',\n",
       "       'school_state_MN', 'school_state_MO', 'school_state_MS',\n",
       "       'school_state_MT', 'school_state_NC', 'school_state_ND',\n",
       "       'school_state_NE', 'school_state_NH', 'school_state_NJ',\n",
       "       'school_state_NM', 'school_state_NV', 'school_state_NY',\n",
       "       'school_state_OH', 'school_state_OK', 'school_state_OR',\n",
       "       'school_state_PA', 'school_state_RI', 'school_state_SC',\n",
       "       'school_state_SD', 'school_state_TN', 'school_state_TX',\n",
       "       'school_state_UT', 'school_state_VA', 'school_state_VT',\n",
       "       'school_state_WA', 'school_state_WI', 'school_state_WV',\n",
       "       'school_state_WY', 'school_metro_rural', 'school_metro_suburban',\n",
       "       'school_metro_urban', 'teacher_prefix_Dr.', 'teacher_prefix_Mr.',\n",
       "       'teacher_prefix_Mrs.', 'teacher_prefix_Ms.', 'resource_type_Books',\n",
       "       'resource_type_Other', 'resource_type_Supplies',\n",
       "       'resource_type_Technology', 'resource_type_Trips',\n",
       "       'resource_type_Visitors', 'primary_focus_area_Applied Learning',\n",
       "       'primary_focus_area_Health & Sports',\n",
       "       'primary_focus_area_History & Civics',\n",
       "       'primary_focus_area_Literacy & Language',\n",
       "       'primary_focus_area_Math & Science',\n",
       "       'primary_focus_area_Music & The Arts',\n",
       "       'primary_focus_area_Special Needs',\n",
       "       'secondary_focus_area_Applied Learning',\n",
       "       'secondary_focus_area_Health & Sports',\n",
       "       'secondary_focus_area_History & Civics',\n",
       "       'secondary_focus_area_Literacy & Language',\n",
       "       'secondary_focus_area_Math & Science',\n",
       "       'secondary_focus_area_Music & The Arts',\n",
       "       'secondary_focus_area_Special Needs', 'poverty_level_high poverty',\n",
       "       'poverty_level_highest poverty', 'poverty_level_low poverty',\n",
       "       'poverty_level_moderate poverty', 'grade_level_Grades 3-5',\n",
       "       'grade_level_Grades 6-8', 'grade_level_Grades 9-12',\n",
       "       'grade_level_Grades PreK-2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124976 entries, 0 to 124975\n",
      "Data columns (total 93 columns):\n",
      "school_charter                              124976 non-null int64\n",
      "school_magnet                               124976 non-null int64\n",
      "total_price_including_optional_support      124976 non-null float64\n",
      "students_reached                            124976 non-null float64\n",
      "eligible_double_your_impact_match           124976 non-null int64\n",
      "date_posted                                 124976 non-null datetime64[ns]\n",
      "fully_funded_in_60_days                     124976 non-null int64\n",
      "school_state_AK                             124976 non-null int64\n",
      "school_state_AL                             124976 non-null int64\n",
      "school_state_AR                             124976 non-null int64\n",
      "school_state_AZ                             124976 non-null int64\n",
      "school_state_CA                             124976 non-null int64\n",
      "school_state_CO                             124976 non-null int64\n",
      "school_state_CT                             124976 non-null int64\n",
      "school_state_DC                             124976 non-null int64\n",
      "school_state_DE                             124976 non-null int64\n",
      "school_state_FL                             124976 non-null int64\n",
      "school_state_GA                             124976 non-null int64\n",
      "school_state_HI                             124976 non-null int64\n",
      "school_state_IA                             124976 non-null int64\n",
      "school_state_ID                             124976 non-null int64\n",
      "school_state_IL                             124976 non-null int64\n",
      "school_state_IN                             124976 non-null int64\n",
      "school_state_KS                             124976 non-null int64\n",
      "school_state_KY                             124976 non-null int64\n",
      "school_state_LA                             124976 non-null int64\n",
      "school_state_MA                             124976 non-null int64\n",
      "school_state_MD                             124976 non-null int64\n",
      "school_state_ME                             124976 non-null int64\n",
      "school_state_MI                             124976 non-null int64\n",
      "school_state_MN                             124976 non-null int64\n",
      "school_state_MO                             124976 non-null int64\n",
      "school_state_MS                             124976 non-null int64\n",
      "school_state_MT                             124976 non-null int64\n",
      "school_state_NC                             124976 non-null int64\n",
      "school_state_ND                             124976 non-null int64\n",
      "school_state_NE                             124976 non-null int64\n",
      "school_state_NH                             124976 non-null int64\n",
      "school_state_NJ                             124976 non-null int64\n",
      "school_state_NM                             124976 non-null int64\n",
      "school_state_NV                             124976 non-null int64\n",
      "school_state_NY                             124976 non-null int64\n",
      "school_state_OH                             124976 non-null int64\n",
      "school_state_OK                             124976 non-null int64\n",
      "school_state_OR                             124976 non-null int64\n",
      "school_state_PA                             124976 non-null int64\n",
      "school_state_RI                             124976 non-null int64\n",
      "school_state_SC                             124976 non-null int64\n",
      "school_state_SD                             124976 non-null int64\n",
      "school_state_TN                             124976 non-null int64\n",
      "school_state_TX                             124976 non-null int64\n",
      "school_state_UT                             124976 non-null int64\n",
      "school_state_VA                             124976 non-null int64\n",
      "school_state_VT                             124976 non-null int64\n",
      "school_state_WA                             124976 non-null int64\n",
      "school_state_WI                             124976 non-null int64\n",
      "school_state_WV                             124976 non-null int64\n",
      "school_state_WY                             124976 non-null int64\n",
      "school_metro_rural                          124976 non-null int64\n",
      "school_metro_suburban                       124976 non-null int64\n",
      "school_metro_urban                          124976 non-null int64\n",
      "teacher_prefix_Dr.                          124976 non-null int64\n",
      "teacher_prefix_Mr.                          124976 non-null int64\n",
      "teacher_prefix_Mrs.                         124976 non-null int64\n",
      "teacher_prefix_Ms.                          124976 non-null int64\n",
      "resource_type_Books                         124976 non-null int64\n",
      "resource_type_Other                         124976 non-null int64\n",
      "resource_type_Supplies                      124976 non-null int64\n",
      "resource_type_Technology                    124976 non-null int64\n",
      "resource_type_Trips                         124976 non-null int64\n",
      "resource_type_Visitors                      124976 non-null int64\n",
      "primary_focus_area_Applied Learning         124976 non-null int64\n",
      "primary_focus_area_Health & Sports          124976 non-null int64\n",
      "primary_focus_area_History & Civics         124976 non-null int64\n",
      "primary_focus_area_Literacy & Language      124976 non-null int64\n",
      "primary_focus_area_Math & Science           124976 non-null int64\n",
      "primary_focus_area_Music & The Arts         124976 non-null int64\n",
      "primary_focus_area_Special Needs            124976 non-null int64\n",
      "secondary_focus_area_Applied Learning       124976 non-null int64\n",
      "secondary_focus_area_Health & Sports        124976 non-null int64\n",
      "secondary_focus_area_History & Civics       124976 non-null int64\n",
      "secondary_focus_area_Literacy & Language    124976 non-null int64\n",
      "secondary_focus_area_Math & Science         124976 non-null int64\n",
      "secondary_focus_area_Music & The Arts       124976 non-null int64\n",
      "secondary_focus_area_Special Needs          124976 non-null int64\n",
      "poverty_level_high poverty                  124976 non-null int64\n",
      "poverty_level_highest poverty               124976 non-null int64\n",
      "poverty_level_low poverty                   124976 non-null int64\n",
      "poverty_level_moderate poverty              124976 non-null int64\n",
      "grade_level_Grades 3-5                      124976 non-null int64\n",
      "grade_level_Grades 6-8                      124976 non-null int64\n",
      "grade_level_Grades 9-12                     124976 non-null int64\n",
      "grade_level_Grades PreK-2                   124976 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(90)\n",
      "memory usage: 88.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and evaluate classifiers\n",
    "\n",
    "Model building and validation steps:\n",
    "1. Split data into test and train temporal sets\n",
    "2. Train series of classifiers on training data, then print evaluation metrics\n",
    "\n",
    "### 4.1 Split data into test and train sets by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get three different temporal test/train sets and append to list\n",
    "test_train = []\n",
    "TEST_DUR = 6\n",
    "TEST_UNITS = 'M'\n",
    "new_df = df.copy(deep=True)\n",
    "\n",
    "while True:\n",
    "    # split_data_temporal returns (x_train, x_test, y_train, y_test)\n",
    "    data = library.split_data_temporal(new_df, label=config.LABEL, date_col='date_posted', \\\n",
    "                                       test_dur=TEST_DUR, test_units=TEST_UNITS)\n",
    "    test_train.append(data)\n",
    "    \n",
    "    # if length of training set is shorter than length of test set, break\n",
    "    train_length = data[0]['date_posted'].max() - data[0]['date_posted'].min()\n",
    "    test_length = data[1]['date_posted'].max() - data[1]['date_posted'].min()\n",
    "    \n",
    "    if train_length <= test_length:\n",
    "        break\n",
    "    else:\n",
    "        new_max = new_df['date_posted'].max() - pd.to_timedelta(TEST_DUR, TEST_UNITS)\n",
    "        new_df = new_df.loc[new_df['date_posted'] < new_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET 1: start: 2012-01-01, end: 2013-07-01, dur: 547 days, 0:00:00\n",
      "TEST SET 1: start: 2013-07-02, end: 2013-12-31, dur: 182 days, 0:00:00\n",
      "\n",
      "TRAINING SET 2: start: 2012-01-01, end: 2012-12-30, dur: 364 days, 0:00:00\n",
      "TEST SET 2: start: 2012-12-31, end: 2013-07-01, dur: 182 days, 0:00:00\n",
      "\n",
      "TRAINING SET 3: start: 2012-01-01, end: 2012-06-30, dur: 181 days, 0:00:00\n",
      "TEST SET 3: start: 2012-07-01, end: 2012-12-30, dur: 182 days, 0:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure temporal splits worked correctly\n",
    "# Each entry in list is a 4-tuple of (x_train, x_test, y_train, y_test)\n",
    "# Drop date_posted field after validating\n",
    "\n",
    "for i in range(len(test_train)):\n",
    "    train_start = test_train[i][0]['date_posted'].min().date()\n",
    "    train_end = test_train[i][0]['date_posted'].max().date()\n",
    "    train_dur = train_end - train_start\n",
    "    test_start = test_train[i][1]['date_posted'].min().date()\n",
    "    test_end = test_train[i][1]['date_posted'].max().date()\n",
    "    test_dur = test_end - test_start\n",
    "    print(f'TRAINING SET {i + 1}: start: {train_start}, end: {train_end}, dur: {train_dur}')\n",
    "    print(f'TEST SET {i + 1}: start: {test_start}, end: {test_end}, dur: {test_dur}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train and evaluate classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'random_state': 0}.\n",
      "Validating LogisticRegression\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training DecisionTreeClassifier with params {'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating DecisionTreeClassifier\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1.0, 'max_iter': 1000, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating LinearSVC\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n",
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training RandomForestClassifier with params {'criterion': 'entropy', 'max_features': 'auto', 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating RandomForestClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating AdaBoostClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0}.\n",
      "Validating BaggingClassifier\n"
     ]
    }
   ],
   "source": [
    "# Set test/train splits, classifiers, precision-recall thresholds to loop over\n",
    "# TODO - WHY DOES KNN BREAK PREDICT_PROBA?\n",
    "classifiers = ['LogisticRegression', 'DecisionTreeClassifier',\n",
    "               'LinearSVC', 'RandomForestClassifier', 'AdaBoostClassifier', \n",
    "               'BaggingClassifier']\n",
    "class_thresholds = [0.4, 0.5, 0.55, 0.6, 0.7]\n",
    "pr_thresholds = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5, 1]\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop\n",
    "for i in classifiers:\n",
    "    for j in test_train:\n",
    "        for k in class_thresholds:\n",
    "            # select correct test-train split\n",
    "            x_train, x_test, y_train, y_test = j\n",
    "\n",
    "            test_start = str(x_test['date_posted'].min().date())\n",
    "            test_end = str(x_test['date_posted'].max().date())\n",
    "\n",
    "            # drop date cols before training\n",
    "            x_train = x_train.drop(labels=['date_posted'], axis=1)\n",
    "            x_test = x_test.drop(labels=['date_posted'], axis=1)\n",
    "\n",
    "            # train model\n",
    "            trained = library.train_classifier(x_train, y_train, \n",
    "                                               method=i, \n",
    "                                               param_dict=config.MODEL_PARAMS)\n",
    "\n",
    "            # evalute results\n",
    "            results_dict = library.validate_classifier(x_test, y_test, trained,\n",
    "                                                       label_threshold=k,\n",
    "                                                       pr_threshold=pr_thresholds)\n",
    "            results_dict['threshold'] = k\n",
    "            results_dict['test-start'] = test_start\n",
    "            results_dict['test-end'] = test_end\n",
    "\n",
    "            # save to dict\n",
    "            results_df = results_df.append(results_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_0.01</th>\n",
       "      <th>precision_0.02</th>\n",
       "      <th>precision_0.05</th>\n",
       "      <th>precision_0.1</th>\n",
       "      <th>precision_0.2</th>\n",
       "      <th>precision_0.3</th>\n",
       "      <th>precision_0.5</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0.01</th>\n",
       "      <th>recall_0.02</th>\n",
       "      <th>recall_0.05</th>\n",
       "      <th>recall_0.1</th>\n",
       "      <th>recall_0.2</th>\n",
       "      <th>recall_0.3</th>\n",
       "      <th>recall_0.5</th>\n",
       "      <th>recall_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>test-start</th>\n",
       "      <th>test-end</th>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">LogisticRegression</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.714497</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.831797</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.879882</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842970</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.719263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.713952</td>\n",
       "      <td>0.655241</td>\n",
       "      <td>0.827432</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.728235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.709116</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.819116</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.879882</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842970</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.738180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.655241</td>\n",
       "      <td>0.800381</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.753401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.609581</td>\n",
       "      <td>0.655239</td>\n",
       "      <td>0.687505</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811734</td>\n",
       "      <td>0.805073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.599899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.685542</td>\n",
       "      <td>0.647781</td>\n",
       "      <td>0.812406</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.883379</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834214</td>\n",
       "      <td>0.813993</td>\n",
       "      <td>0.775879</td>\n",
       "      <td>0.686294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.689814</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.812888</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.692028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.692707</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.810716</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.700582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.687379</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.799186</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.712857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.618582</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.697130</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.763114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.741052</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.849416</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.747961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.729151</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.837436</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817161</td>\n",
       "      <td>0.755894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.711971</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.820562</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.764019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.685179</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.778355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.574654</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.658282</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.816865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">DecisionTreeClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.715813</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.834240</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.716173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.715314</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.833495</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.716939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.714746</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.832926</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.717184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.698876</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.813268</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.731282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.595482</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.686348</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.771337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.618229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.812270</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.684324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.683797</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.811777</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.684730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.684808</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.810618</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.688188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.681547</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.804632</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.693290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.599385</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.737991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.851938</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.743474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.731500</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.843303</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.744607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.727228</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.839955</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.744714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.693754</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.486070</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.807986</td>\n",
       "      <td>0.807986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759538</td>\n",
       "      <td>0.404640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">AdaBoostClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.715904</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.834434</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.904529</td>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.715904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.829282</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.904529</td>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.724910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.353275</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.186306</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796143</td>\n",
       "      <td>0.409262</td>\n",
       "      <td>0.283787</td>\n",
       "      <td>0.181681</td>\n",
       "      <td>0.103419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.284096</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.284096</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.684119</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.812435</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.797649</td>\n",
       "      <td>0.684119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.685634</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.809443</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.797649</td>\n",
       "      <td>0.691463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.400845</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.242569</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.549737</td>\n",
       "      <td>0.379335</td>\n",
       "      <td>0.240557</td>\n",
       "      <td>0.140239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.315881</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.315881</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.743188</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.852677</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.925847</td>\n",
       "      <td>0.898383</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>0.828207</td>\n",
       "      <td>0.743188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.723536</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.833523</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.925847</td>\n",
       "      <td>0.898383</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>0.828207</td>\n",
       "      <td>0.754349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.326508</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.181980</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809163</td>\n",
       "      <td>0.416950</td>\n",
       "      <td>0.286699</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.100801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.256934</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.256812</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">BaggingClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.706527</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.820467</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.729897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.694812</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.804058</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.743998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.665342</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.768341</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.761590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.665206</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.768181</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.761643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.615416</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.704489</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.683889</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.804854</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.696638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.674199</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.790317</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.706010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.655461</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.762640</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.721245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.655461</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.762565</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.721377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.808741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.617663</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.740608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.721308</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.830905</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.756643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.697263</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.768649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.761806</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.655061</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.761770</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.594397</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.689686</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.799340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        accuracy   auc-roc  \\\n",
       "classifier             test-start test-end   threshold                       \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40       0.714497  0.655238   \n",
       "                                             0.50       0.713952  0.655241   \n",
       "                                             0.55       0.709116  0.655238   \n",
       "                                             0.60       0.695175  0.655241   \n",
       "                                             0.70       0.609581  0.655239   \n",
       "                       2012-12-31 2013-07-01 0.40       0.685542  0.647781   \n",
       "                                             0.50       0.689814  0.647798   \n",
       "                                             0.55       0.692707  0.647798   \n",
       "                                             0.60       0.687379  0.647798   \n",
       "                                             0.70       0.618582  0.647798   \n",
       "                       2012-07-01 2012-12-30 0.40       0.741052  0.630703   \n",
       "                                             0.50       0.729151  0.630703   \n",
       "                                             0.55       0.711971  0.630703   \n",
       "                                             0.60       0.685179  0.630703   \n",
       "                                             0.70       0.574654  0.630703   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40       0.715813  0.612613   \n",
       "                                             0.50       0.715314  0.612613   \n",
       "                                             0.55       0.714746  0.612613   \n",
       "                                             0.60       0.698876  0.612613   \n",
       "                                             0.70       0.595482  0.612613   \n",
       "                       2012-12-31 2013-07-01 0.40       0.684073  0.610808   \n",
       "                                             0.50       0.683797  0.610808   \n",
       "                                             0.55       0.684808  0.610808   \n",
       "                                             0.60       0.681547  0.610808   \n",
       "                                             0.70       0.599385  0.610808   \n",
       "                       2012-07-01 2012-12-30 0.40       0.742333  0.592765   \n",
       "                                             0.50       0.731500  0.592765   \n",
       "                                             0.55       0.727228  0.592765   \n",
       "                                             0.60       0.693754  0.592765   \n",
       "                                             0.70       0.486070  0.592765   \n",
       "...                                                          ...       ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40       0.715904  0.663427   \n",
       "                                             0.50       0.714451  0.663427   \n",
       "                                             0.55       0.353275  0.663427   \n",
       "                                             0.60       0.284096  0.663427   \n",
       "                                             0.70       0.284096  0.663427   \n",
       "                       2012-12-31 2013-07-01 0.40       0.684119  0.666147   \n",
       "                                             0.50       0.685634  0.666147   \n",
       "                                             0.55       0.400845  0.666147   \n",
       "                                             0.60       0.315881  0.666147   \n",
       "                                             0.70       0.315881  0.666147   \n",
       "                       2012-07-01 2012-12-30 0.40       0.743188  0.648891   \n",
       "                                             0.50       0.723536  0.648891   \n",
       "                                             0.55       0.326508  0.648891   \n",
       "                                             0.60       0.256934  0.648891   \n",
       "                                             0.70       0.256812  0.648891   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40       0.706527  0.636271   \n",
       "                                             0.50       0.694812  0.636271   \n",
       "                                             0.55       0.665342  0.636271   \n",
       "                                             0.60       0.665206  0.636271   \n",
       "                                             0.70       0.615416  0.636271   \n",
       "                       2012-12-31 2013-07-01 0.40       0.683889  0.621075   \n",
       "                                             0.50       0.674199  0.621075   \n",
       "                                             0.55       0.655461  0.621075   \n",
       "                                             0.60       0.655461  0.621075   \n",
       "                                             0.70       0.617663  0.621075   \n",
       "                       2012-07-01 2012-12-30 0.40       0.721308  0.614359   \n",
       "                                             0.50       0.697263  0.614359   \n",
       "                                             0.55       0.655091  0.614359   \n",
       "                                             0.60       0.655061  0.614359   \n",
       "                                             0.70       0.594397  0.614359   \n",
       "\n",
       "                                                              f1  \\\n",
       "classifier             test-start test-end   threshold             \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40       0.831797   \n",
       "                                             0.50       0.827432   \n",
       "                                             0.55       0.819116   \n",
       "                                             0.60       0.800381   \n",
       "                                             0.70       0.687505   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812406   \n",
       "                                             0.50       0.812888   \n",
       "                                             0.55       0.810716   \n",
       "                                             0.60       0.799186   \n",
       "                                             0.70       0.697130   \n",
       "                       2012-07-01 2012-12-30 0.40       0.849416   \n",
       "                                             0.50       0.837436   \n",
       "                                             0.55       0.820562   \n",
       "                                             0.60       0.791874   \n",
       "                                             0.70       0.658282   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40       0.834240   \n",
       "                                             0.50       0.833495   \n",
       "                                             0.55       0.832926   \n",
       "                                             0.60       0.813268   \n",
       "                                             0.70       0.686348   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812270   \n",
       "                                             0.50       0.811777   \n",
       "                                             0.55       0.810618   \n",
       "                                             0.60       0.804632   \n",
       "                                             0.70       0.686955   \n",
       "                       2012-07-01 2012-12-30 0.40       0.851938   \n",
       "                                             0.50       0.843303   \n",
       "                                             0.55       0.839955   \n",
       "                                             0.60       0.807230   \n",
       "                                             0.70       0.539232   \n",
       "...                                                          ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40       0.834434   \n",
       "                                             0.50       0.829282   \n",
       "                                             0.55       0.186306   \n",
       "                                             0.60       0.000000   \n",
       "                                             0.70       0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812435   \n",
       "                                             0.50       0.809443   \n",
       "                                             0.55       0.242569   \n",
       "                                             0.60       0.000000   \n",
       "                                             0.70       0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40       0.852677   \n",
       "                                             0.50       0.833523   \n",
       "                                             0.55       0.181980   \n",
       "                                             0.60       0.000328   \n",
       "                                             0.70       0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40       0.820467   \n",
       "                                             0.50       0.804058   \n",
       "                                             0.55       0.768341   \n",
       "                                             0.60       0.768181   \n",
       "                                             0.70       0.704489   \n",
       "                       2012-12-31 2013-07-01 0.40       0.804854   \n",
       "                                             0.50       0.790317   \n",
       "                                             0.55       0.762640   \n",
       "                                             0.60       0.762565   \n",
       "                                             0.70       0.708417   \n",
       "                       2012-07-01 2012-12-30 0.40       0.830905   \n",
       "                                             0.50       0.806302   \n",
       "                                             0.55       0.761806   \n",
       "                                             0.60       0.761770   \n",
       "                                             0.70       0.689686   \n",
       "\n",
       "                                                        precision_0.01  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.854545   \n",
       "                                             0.50             0.854545   \n",
       "                                             0.55             0.854545   \n",
       "                                             0.60             0.854545   \n",
       "                                             0.70             0.854545   \n",
       "                       2012-12-31 2013-07-01 0.40             0.899083   \n",
       "                                             0.50             0.899083   \n",
       "                                             0.55             0.899083   \n",
       "                                             0.60             0.899083   \n",
       "                                             0.70             0.899083   \n",
       "                       2012-07-01 2012-12-30 0.40             0.908537   \n",
       "                                             0.50             0.908537   \n",
       "                                             0.55             0.908537   \n",
       "                                             0.60             0.908537   \n",
       "                                             0.70             0.908537   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.847727   \n",
       "                                             0.50             0.847727   \n",
       "                                             0.55             0.847727   \n",
       "                                             0.60             0.847727   \n",
       "                                             0.70             0.847727   \n",
       "                       2012-12-31 2013-07-01 0.40             0.876147   \n",
       "                                             0.50             0.876147   \n",
       "                                             0.55             0.876147   \n",
       "                                             0.60             0.876147   \n",
       "                                             0.70             0.876147   \n",
       "                       2012-07-01 2012-12-30 0.40             0.820122   \n",
       "                                             0.50             0.820122   \n",
       "                                             0.55             0.820122   \n",
       "                                             0.60             0.820122   \n",
       "                                             0.70             0.820122   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.988636   \n",
       "                                             0.50             0.988636   \n",
       "                                             0.55             0.988636   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.972477   \n",
       "                                             0.50             0.972477   \n",
       "                                             0.55             0.972477   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.966463   \n",
       "                                             0.50             0.966463   \n",
       "                                             0.55             0.966463   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.884091   \n",
       "                                             0.50             0.884091   \n",
       "                                             0.55             0.884091   \n",
       "                                             0.60             0.884091   \n",
       "                                             0.70             0.884091   \n",
       "                       2012-12-31 2013-07-01 0.40             0.830275   \n",
       "                                             0.50             0.830275   \n",
       "                                             0.55             0.830275   \n",
       "                                             0.60             0.830275   \n",
       "                                             0.70             0.830275   \n",
       "                       2012-07-01 2012-12-30 0.40             0.868902   \n",
       "                                             0.50             0.868902   \n",
       "                                             0.55             0.868902   \n",
       "                                             0.60             0.868902   \n",
       "                                             0.70             0.868902   \n",
       "\n",
       "                                                        precision_0.02  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.866061   \n",
       "                                             0.50             0.866061   \n",
       "                                             0.55             0.866061   \n",
       "                                             0.60             0.866061   \n",
       "                                             0.70             0.866061   \n",
       "                       2012-12-31 2013-07-01 0.40             0.917241   \n",
       "                                             0.50             0.917241   \n",
       "                                             0.55             0.917241   \n",
       "                                             0.60             0.917241   \n",
       "                                             0.70             0.917241   \n",
       "                       2012-07-01 2012-12-30 0.40             0.908397   \n",
       "                                             0.50             0.908397   \n",
       "                                             0.55             0.908397   \n",
       "                                             0.60             0.908397   \n",
       "                                             0.70             0.908397   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.827469   \n",
       "                                             0.50             0.827469   \n",
       "                                             0.55             0.827469   \n",
       "                                             0.60             0.827469   \n",
       "                                             0.70             0.827469   \n",
       "                       2012-12-31 2013-07-01 0.40             0.882759   \n",
       "                                             0.50             0.882759   \n",
       "                                             0.55             0.882759   \n",
       "                                             0.60             0.882759   \n",
       "                                             0.70             0.882759   \n",
       "                       2012-07-01 2012-12-30 0.40             0.796947   \n",
       "                                             0.50             0.796947   \n",
       "                                             0.55             0.796947   \n",
       "                                             0.60             0.796947   \n",
       "                                             0.70             0.796947   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.968218   \n",
       "                                             0.50             0.968218   \n",
       "                                             0.55             0.968218   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.917241   \n",
       "                                             0.50             0.917241   \n",
       "                                             0.55             0.917241   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.945038   \n",
       "                                             0.50             0.945038   \n",
       "                                             0.55             0.945038   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.874007   \n",
       "                                             0.50             0.874007   \n",
       "                                             0.55             0.874007   \n",
       "                                             0.60             0.874007   \n",
       "                                             0.70             0.874007   \n",
       "                       2012-12-31 2013-07-01 0.40             0.818391   \n",
       "                                             0.50             0.818391   \n",
       "                                             0.55             0.818391   \n",
       "                                             0.60             0.818391   \n",
       "                                             0.70             0.818391   \n",
       "                       2012-07-01 2012-12-30 0.40             0.870229   \n",
       "                                             0.50             0.870229   \n",
       "                                             0.55             0.870229   \n",
       "                                             0.60             0.870229   \n",
       "                                             0.70             0.870229   \n",
       "\n",
       "                                                        precision_0.05  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.886467   \n",
       "                                             0.50             0.886467   \n",
       "                                             0.55             0.886467   \n",
       "                                             0.60             0.886467   \n",
       "                                             0.70             0.886467   \n",
       "                       2012-12-31 2013-07-01 0.40             0.883379   \n",
       "                                             0.50             0.884298   \n",
       "                                             0.55             0.884298   \n",
       "                                             0.60             0.884298   \n",
       "                                             0.70             0.884298   \n",
       "                       2012-07-01 2012-12-30 0.40             0.892007   \n",
       "                                             0.50             0.892007   \n",
       "                                             0.55             0.892007   \n",
       "                                             0.60             0.892007   \n",
       "                                             0.70             0.892007   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.870572   \n",
       "                                             0.50             0.870572   \n",
       "                                             0.55             0.870572   \n",
       "                                             0.60             0.870572   \n",
       "                                             0.70             0.870572   \n",
       "                       2012-12-31 2013-07-01 0.40             0.876033   \n",
       "                                             0.50             0.876033   \n",
       "                                             0.55             0.876033   \n",
       "                                             0.60             0.876033   \n",
       "                                             0.70             0.876033   \n",
       "                       2012-07-01 2012-12-30 0.40             0.855400   \n",
       "                                             0.50             0.855400   \n",
       "                                             0.55             0.855400   \n",
       "                                             0.60             0.855400   \n",
       "                                             0.70             0.855400   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.953224   \n",
       "                                             0.50             0.953224   \n",
       "                                             0.55             0.953224   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.909091   \n",
       "                                             0.50             0.909091   \n",
       "                                             0.55             0.909091   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.942038   \n",
       "                                             0.50             0.942038   \n",
       "                                             0.55             0.942038   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.880109   \n",
       "                                             0.50             0.880109   \n",
       "                                             0.55             0.880109   \n",
       "                                             0.60             0.880109   \n",
       "                                             0.70             0.880109   \n",
       "                       2012-12-31 2013-07-01 0.40             0.837466   \n",
       "                                             0.50             0.837466   \n",
       "                                             0.55             0.837466   \n",
       "                                             0.60             0.837466   \n",
       "                                             0.70             0.837466   \n",
       "                       2012-07-01 2012-12-30 0.40             0.871263   \n",
       "                                             0.50             0.871263   \n",
       "                                             0.55             0.871263   \n",
       "                                             0.60             0.871263   \n",
       "                                             0.70             0.871263   \n",
       "\n",
       "                                                        precision_0.1  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.879882   \n",
       "                                             0.50            0.880109   \n",
       "                                             0.55            0.879882   \n",
       "                                             0.60            0.880109   \n",
       "                                             0.70            0.880109   \n",
       "                       2012-12-31 2013-07-01 0.40            0.867249   \n",
       "                                             0.50            0.867249   \n",
       "                                             0.55            0.867249   \n",
       "                                             0.60            0.867249   \n",
       "                                             0.70            0.867249   \n",
       "                       2012-07-01 2012-12-30 0.40            0.878853   \n",
       "                                             0.50            0.878853   \n",
       "                                             0.55            0.878853   \n",
       "                                             0.60            0.878853   \n",
       "                                             0.70            0.878853   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.875114   \n",
       "                                             0.50            0.875114   \n",
       "                                             0.55            0.875114   \n",
       "                                             0.60            0.875114   \n",
       "                                             0.70            0.875114   \n",
       "                       2012-12-31 2013-07-01 0.40            0.861736   \n",
       "                                             0.50            0.861736   \n",
       "                                             0.55            0.861736   \n",
       "                                             0.60            0.861736   \n",
       "                                             0.70            0.861736   \n",
       "                       2012-07-01 2012-12-30 0.40            0.873360   \n",
       "                                             0.50            0.873360   \n",
       "                                             0.55            0.873360   \n",
       "                                             0.60            0.873360   \n",
       "                                             0.70            0.873360   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.930064   \n",
       "                                             0.50            0.930064   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.899862   \n",
       "                                             0.50            0.899862   \n",
       "                                             0.55            0.899862   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.925847   \n",
       "                                             0.50            0.925847   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.878974   \n",
       "                                             0.50            0.878974   \n",
       "                                             0.55            0.878974   \n",
       "                                             0.60            0.878974   \n",
       "                                             0.70            0.878974   \n",
       "                       2012-12-31 2013-07-01 0.40            0.843822   \n",
       "                                             0.50            0.843822   \n",
       "                                             0.55            0.843822   \n",
       "                                             0.60            0.843822   \n",
       "                                             0.70            0.843822   \n",
       "                       2012-07-01 2012-12-30 0.40            0.867562   \n",
       "                                             0.50            0.867562   \n",
       "                                             0.55            0.867562   \n",
       "                                             0.60            0.867562   \n",
       "                                             0.70            0.867562   \n",
       "\n",
       "                                                        precision_0.2  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.858894   \n",
       "                                             0.50            0.858894   \n",
       "                                             0.55            0.858894   \n",
       "                                             0.60            0.858894   \n",
       "                                             0.70            0.858894   \n",
       "                       2012-12-31 2013-07-01 0.40            0.834214   \n",
       "                                             0.50            0.834443   \n",
       "                                             0.55            0.834443   \n",
       "                                             0.60            0.834443   \n",
       "                                             0.70            0.834443   \n",
       "                       2012-07-01 2012-12-30 0.40            0.861153   \n",
       "                                             0.50            0.861153   \n",
       "                                             0.55            0.861153   \n",
       "                                             0.60            0.861153   \n",
       "                                             0.70            0.861153   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.843456   \n",
       "                                             0.50            0.843456   \n",
       "                                             0.55            0.843456   \n",
       "                                             0.60            0.843456   \n",
       "                                             0.70            0.843456   \n",
       "                       2012-12-31 2013-07-01 0.40            0.822273   \n",
       "                                             0.50            0.822273   \n",
       "                                             0.55            0.822273   \n",
       "                                             0.60            0.822273   \n",
       "                                             0.70            0.822273   \n",
       "                       2012-07-01 2012-12-30 0.40            0.852151   \n",
       "                                             0.50            0.852151   \n",
       "                                             0.55            0.852151   \n",
       "                                             0.60            0.852151   \n",
       "                                             0.70            0.852151   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.904529   \n",
       "                                             0.50            0.904529   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.872560   \n",
       "                                             0.50            0.872560   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.898383   \n",
       "                                             0.50            0.898383   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.853105   \n",
       "                                             0.50            0.853105   \n",
       "                                             0.55            0.853105   \n",
       "                                             0.60            0.853105   \n",
       "                                             0.70            0.853105   \n",
       "                       2012-12-31 2013-07-01 0.40            0.810792   \n",
       "                                             0.50            0.810792   \n",
       "                                             0.55            0.810792   \n",
       "                                             0.60            0.810792   \n",
       "                                             0.70            0.810792   \n",
       "                       2012-07-01 2012-12-30 0.40            0.849710   \n",
       "                                             0.50            0.849710   \n",
       "                                             0.55            0.849710   \n",
       "                                             0.60            0.849710   \n",
       "                                             0.70            0.849710   \n",
       "\n",
       "                                                        precision_0.3  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.842970   \n",
       "                                             0.50            0.842894   \n",
       "                                             0.55            0.842970   \n",
       "                                             0.60            0.842894   \n",
       "                                             0.70            0.842894   \n",
       "                       2012-12-31 2013-07-01 0.40            0.813993   \n",
       "                                             0.50            0.814299   \n",
       "                                             0.55            0.814299   \n",
       "                                             0.60            0.814299   \n",
       "                                             0.70            0.814299   \n",
       "                       2012-07-01 2012-12-30 0.40            0.846506   \n",
       "                                             0.50            0.846506   \n",
       "                                             0.55            0.846506   \n",
       "                                             0.60            0.846506   \n",
       "                                             0.70            0.846506   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.815877   \n",
       "                                             0.50            0.815877   \n",
       "                                             0.55            0.815877   \n",
       "                                             0.60            0.815877   \n",
       "                                             0.70            0.815877   \n",
       "                       2012-12-31 2013-07-01 0.40            0.787048   \n",
       "                                             0.50            0.787048   \n",
       "                                             0.55            0.787048   \n",
       "                                             0.60            0.787048   \n",
       "                                             0.70            0.787048   \n",
       "                       2012-07-01 2012-12-30 0.40            0.819449   \n",
       "                                             0.50            0.819449   \n",
       "                                             0.55            0.819449   \n",
       "                                             0.60            0.819449   \n",
       "                                             0.70            0.819449   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.869608   \n",
       "                                             0.50            0.869608   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.843080   \n",
       "                                             0.50            0.843080   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.871020   \n",
       "                                             0.50            0.871020   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.833132   \n",
       "                                             0.50            0.833132   \n",
       "                                             0.55            0.833132   \n",
       "                                             0.60            0.833132   \n",
       "                                             0.70            0.833132   \n",
       "                       2012-12-31 2013-07-01 0.40            0.795622   \n",
       "                                             0.50            0.795622   \n",
       "                                             0.55            0.795622   \n",
       "                                             0.60            0.795622   \n",
       "                                             0.70            0.795622   \n",
       "                       2012-07-01 2012-12-30 0.40            0.834401   \n",
       "                                             0.50            0.834401   \n",
       "                                             0.55            0.834401   \n",
       "                                             0.60            0.834401   \n",
       "                                             0.70            0.834401   \n",
       "\n",
       "                                                        precision_0.5  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.811779   \n",
       "                                             0.50            0.811779   \n",
       "                                             0.55            0.811779   \n",
       "                                             0.60            0.811779   \n",
       "                                             0.70            0.811734   \n",
       "                       2012-12-31 2013-07-01 0.40            0.775879   \n",
       "                                             0.50            0.775696   \n",
       "                                             0.55            0.775696   \n",
       "                                             0.60            0.775696   \n",
       "                                             0.70            0.775696   \n",
       "                       2012-07-01 2012-12-30 0.40            0.817222   \n",
       "                                             0.50            0.817161   \n",
       "                                             0.55            0.817222   \n",
       "                                             0.60            0.817222   \n",
       "                                             0.70            0.817222   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.779629   \n",
       "                                             0.50            0.779629   \n",
       "                                             0.55            0.779629   \n",
       "                                             0.60            0.779629   \n",
       "                                             0.70            0.779629   \n",
       "                       2012-12-31 2013-07-01 0.40            0.751630   \n",
       "                                             0.50            0.751630   \n",
       "                                             0.55            0.751630   \n",
       "                                             0.60            0.751630   \n",
       "                                             0.70            0.751630   \n",
       "                       2012-07-01 2012-12-30 0.40            0.791834   \n",
       "                                             0.50            0.791834   \n",
       "                                             0.55            0.791834   \n",
       "                                             0.60            0.791834   \n",
       "                                             0.70            0.807986   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.815049   \n",
       "                                             0.50            0.815049   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.797649   \n",
       "                                             0.50            0.797649   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.828207   \n",
       "                                             0.50            0.828207   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.794887   \n",
       "                                             0.50            0.794887   \n",
       "                                             0.55            0.794887   \n",
       "                                             0.60            0.794887   \n",
       "                                             0.70            0.794887   \n",
       "                       2012-12-31 2013-07-01 0.40            0.759897   \n",
       "                                             0.50            0.759897   \n",
       "                                             0.55            0.759897   \n",
       "                                             0.60            0.759897   \n",
       "                                             0.70            0.759897   \n",
       "                       2012-07-01 2012-12-30 0.40            0.805505   \n",
       "                                             0.50            0.805505   \n",
       "                                             0.55            0.805505   \n",
       "                                             0.60            0.805505   \n",
       "                                             0.70            0.805505   \n",
       "\n",
       "                                                        precision_1  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          0.719263   \n",
       "                                             0.50          0.728235   \n",
       "                                             0.55          0.738180   \n",
       "                                             0.60          0.753401   \n",
       "                                             0.70          0.805073   \n",
       "                       2012-12-31 2013-07-01 0.40          0.686294   \n",
       "                                             0.50          0.692028   \n",
       "                                             0.55          0.700582   \n",
       "                                             0.60          0.712857   \n",
       "                                             0.70          0.763114   \n",
       "                       2012-07-01 2012-12-30 0.40          0.747961   \n",
       "                                             0.50          0.755894   \n",
       "                                             0.55          0.764019   \n",
       "                                             0.60          0.778355   \n",
       "                                             0.70          0.816865   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          0.716173   \n",
       "                                             0.50          0.716939   \n",
       "                                             0.55          0.717184   \n",
       "                                             0.60          0.731282   \n",
       "                                             0.70          0.771337   \n",
       "                       2012-12-31 2013-07-01 0.40          0.684324   \n",
       "                                             0.50          0.684730   \n",
       "                                             0.55          0.688188   \n",
       "                                             0.60          0.693290   \n",
       "                                             0.70          0.737991   \n",
       "                       2012-07-01 2012-12-30 0.40          0.743474   \n",
       "                                             0.50          0.744607   \n",
       "                                             0.55          0.744714   \n",
       "                                             0.60          0.758400   \n",
       "                                             0.70          0.807986   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          0.715904   \n",
       "                                             0.50          0.724910   \n",
       "                                             0.55          0.938417   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          0.684119   \n",
       "                                             0.50          0.691463   \n",
       "                                             0.55          0.897337   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          0.743188   \n",
       "                                             0.50          0.754349   \n",
       "                                             0.55          0.934882   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          0.729897   \n",
       "                                             0.50          0.743998   \n",
       "                                             0.55          0.761590   \n",
       "                                             0.60          0.761643   \n",
       "                                             0.70          0.782931   \n",
       "                       2012-12-31 2013-07-01 0.40          0.696638   \n",
       "                                             0.50          0.706010   \n",
       "                                             0.55          0.721245   \n",
       "                                             0.60          0.721377   \n",
       "                                             0.70          0.740608   \n",
       "                       2012-07-01 2012-12-30 0.40          0.756643   \n",
       "                                             0.50          0.768649   \n",
       "                                             0.55          0.782535   \n",
       "                                             0.60          0.782550   \n",
       "                                             0.70          0.799340   \n",
       "\n",
       "                                                        recall_0.01  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.012618   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.02  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.006462   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.05  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.002591   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.1  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.796143   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.809163   \n",
       "                                             0.60         0.001318   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.2  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.409262   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.549737   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.416950   \n",
       "                                             0.60         0.000679   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.3  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.283787   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.379335   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.286699   \n",
       "                                             0.60         0.000467   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.5  recall_1  \n",
       "classifier             test-start test-end   threshold                        \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000  0.986078  \n",
       "                                             0.50         1.000000  0.957916  \n",
       "                                             0.55         1.000000  0.919986  \n",
       "                                             0.60         1.000000  0.853609  \n",
       "                                             0.70         1.000000  0.599899  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.995301  \n",
       "                                             0.50         1.000000  0.984895  \n",
       "                                             0.55         1.000000  0.961936  \n",
       "                                             0.60         1.000000  0.909305  \n",
       "                                             0.70         1.000000  0.641649  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.982714  \n",
       "                                             0.50         1.000000  0.938698  \n",
       "                                             0.55         1.000000  0.886142  \n",
       "                                             0.60         1.000000  0.805871  \n",
       "                                             0.70         1.000000  0.551263  \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000  0.998922  \n",
       "                                             0.50         1.000000  0.995306  \n",
       "                                             0.55         1.000000  0.993213  \n",
       "                                             0.60         1.000000  0.915958  \n",
       "                                             0.70         1.000000  0.618229  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.999060  \n",
       "                                             0.50         1.000000  0.996711  \n",
       "                                             0.55         1.000000  0.986037  \n",
       "                                             0.60         1.000000  0.958579  \n",
       "                                             0.70         1.000000  0.642521  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.997454  \n",
       "                                             0.50         1.000000  0.972162  \n",
       "                                             0.55         1.000000  0.963129  \n",
       "                                             0.60         1.000000  0.862780  \n",
       "                                             0.70         0.759538  0.404640  \n",
       "...                                                            ...       ...  \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.968762  \n",
       "                                             0.55         0.181681  0.103419  \n",
       "                                             0.60         0.000000  0.000000  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.975967  \n",
       "                                             0.55         0.240557  0.140239  \n",
       "                                             0.60         0.000000  0.000000  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.931267  \n",
       "                                             0.55         0.180900  0.100801  \n",
       "                                             0.60         0.000295  0.000164  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000  0.936699  \n",
       "                                             0.50         1.000000  0.874667  \n",
       "                                             0.55         1.000000  0.775212  \n",
       "                                             0.60         1.000000  0.774832  \n",
       "                                             0.70         1.000000  0.640334  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.952873  \n",
       "                                             0.50         1.000000  0.897489  \n",
       "                                             0.55         1.000000  0.809076  \n",
       "                                             0.60         1.000000  0.808741  \n",
       "                                             0.70         1.000000  0.678907  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.921330  \n",
       "                                             0.50         1.000000  0.847834  \n",
       "                                             0.55         1.000000  0.742147  \n",
       "                                             0.60         1.000000  0.742065  \n",
       "                                             0.70         1.000000  0.606487  \n",
       "\n",
       "[90 rows x 19 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.set_index(['classifier', 'test-start', 'test-end', 'threshold'])\n",
    "results_df.to_excel(\"output/results.xlsx\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Report\n",
    "\n",
    "### 3.1 Overview\n",
    "\n",
    "DonorsChoose is a platform on which teachers may start projects to raise funds for required materials for their classrooms. Once projects are fully funded, DonorsChoose purchases the materials with the funds they collected on the projects' behalf. \n",
    "\n",
    "We were asked to develop a model to predict which projects would be fully funded within 60 days. By using several classification techniques, we have trained a series of models which may be used towards this purpose. The information from each project that we've used to train the models are as follows: \n",
    "\n",
    "| Feature | Type | Description\n",
    "| --- | --- | ---\n",
    "`school_state` | categorical | State in which school is located\n",
    "`school_metro` | categorical | Rural, urban, or suburban\n",
    "`school_charter` | boolean | Is the school a public charter school?\n",
    "`school_magnet` | boolean | Is the school a public magnet school?\n",
    "`teacher_prefix` | categorical | Dr., Mr., Mrs., or Ms.\n",
    "`primary_focus_area`| categorical | Main subject area for which project materials are intended\n",
    "`secondary_focus_area` | categorical | Secondary subject area for which project materials are intended\n",
    "`resource_type` | categorical | Books, supplies, technology, etc.\n",
    "`poverty_level` | categorical | Low, moderate, high, highest\n",
    "`grade_level` | categorical | Grade level for which project materials are intended\n",
    "`total_price_including_optional_support` | categorical | Project cost including optional tip that donors give to DonorsChoose.org while funding a project \n",
    "`students_reached` | numeric | Number of students impacted by a project (if funded)\n",
    "`eligible_double_your_impact_match` | boolean | Project was eligible for a 50% off offer by a corporate partner\n",
    "\n",
    "### 3.2 Which classifier does better on which metrics?\n",
    "\n",
    "The models using logistic regression, decision trees, and support vector machines all seemed to perform comparably at overall accuracy. Depending on the length of time used for the training data, and the probability score threshold beyond which we would consider a project likely to be fully funded in 60 days, 71-73% of their predictions were correct.\n",
    "\n",
    "Interestingly enough, if considering instead the precision metric, the models using a boosted ensemble of logistic regression classifiers performed the best. Overall, 84-87% of the projects predicted to be fully funded within 60 days actually turned out to be so. When looking only at the top 1% of projects, ranked by likelihood of being fully funded in 60 days, 96-99% of them are correctly predicted.\n",
    "\n",
    "Given that a baseline of 71% of projects in the dataset are actually funded within 60 days, most of our measures of recall - the proportion of projects actually fully funded within 60 days that were  correctly predicted by the model - are irrelevant. Overall, here, models using decision trees performed the best, correctly identifying 97-99% of projects actually meeting the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot precision-recall curve for suggested model. Code adapted from Lab 4.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get 18-month training data\n",
    "x_train, x_test, y_train, y_test = test_train[0]\n",
    "x_train = x_train.drop(labels=['date_posted'], axis=1)\n",
    "x_test = x_test.drop(labels=['date_posted'], axis=1)\n",
    "\n",
    "# Train\n",
    "boost = AdaBoostClassifier(n_estimators=10, random_state=0)\n",
    "trained = boost.fit(x_train, y_train)\n",
    "y_scores = boost.predict_proba(x_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores, pos_label=1)\n",
    "population = [sum(y_scores > threshold)/len(y_scores) for threshold in thresholds]\n",
    "\n",
    "# Plot\n",
    "p, = plt.plot(population, precision[:-1], color='b')\n",
    "r, = plt.plot(population, recall[:-1], color='r')\n",
    "plt.title('Precision-recall curves for AdaBoost classifier w/ 10 estimators')\n",
    "plt.xlabel('Threshold for Classification')\n",
    "plt.ylabel('Precision/recall score')\n",
    "plt.legend([p, r], ['precision', 'recall'])\n",
    "plt.savefig('output/best_model_precision.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 How do the results change over time?\n",
    "\n",
    "For each classifier used, we trained models on training data over three different lengths: 6, 12, and 18 months prior to the 6 months of data used to test our models. For example, since we have data from Jan 1, 2012 to Dec 31, 2013, the longest training data used was from Jan 1, 2012 to June 30, 2013, with July 1, 2013 to Dec 1, 2013 forming the test set. \n",
    "\n",
    "In almost every case, models trained on 18 months of training data outperformed the other two variants. However, models trained on 6 months of training data outperformed those trained on 12 months of training data across almost every metric (accuracy, f1, precision at various thresholds). If looking only at the AUC-ROC - a measure of balance between true and false positive rates, called the area under the curve (AUC) for the receiver operating characteristics (ROC) curve - the expected order appears, where models trained on 18th months of data outperform those trained on 12 months of data, which themselves outperform those trained on 6 months of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What would be your recommendation to someone who's working on this model to identify 5% of posted projects to intervene with? Which model should they decide to go forward with and deploy?\n",
    "\n",
    "As always, this depends on the specific intervention and its goal. Is it to maximize overall fundraising dollars? To maximize the number of projects that are successfuly funded? (Fortunately, since DonorsChoose stops fundraising for a project once the intended goal amount is met, these are functionally the same thing.)\n",
    "\n",
    "But the question remains. Which 5% of projects do we want to intervene on? If selecting projects deemed *most* likely to be fully funded in 60 days, those would be the projects probably likely to be fully funded without intervention at all. In this case, DonorsChoose could use \"exciting\" projects like these as a source of marketing material, or target the donor pools to those projects for additional giving opportunities on other projects. If so, I would recommend using the **boosting ensemble model trained on 18 months of data**, which performed the best on precision at 5%. In other words, of the top 5% of projects it identified as likely to be fully funded in 60 days, 95% of them were actually so. \n",
    "\n",
    "If intervening on the 5% of projects *least* likely to be funded within 60 days, or perhaps projects that were unlikely to do so without a nudge or sorts, the potential intervention here could be additional fundraising support in the form of DonorsChoose staff coaching the teacher involved on how to better ask for donations from their networks. In this case, the metric of interest would be the precision at 50% (assuming projects that are 50% likely to be funded within 60 days are on that threshold), and the model that performs best on this metric is still the **boosting ensemble model trained on 18 months of data**.\n",
    "\n",
    "Perhaps the metric we want to maximize is the number of students reached by the 5% of projects we will intervene on. The specific interventions here could be any of the actions already mentioned above. In this case, we would select the combination of projects where the *expected* number of students reached (calculated by the likelihood of being fully funded in 60 days, multiplied by the number of students reached by that project) is the largest possible. Without prior knowledge of how the number of students reached is distributed among projects, we would do best here to maximize overall precision. Fortunately, the model that does this is still the **boosting ensemble model trained on 18 months of data*.\n",
    "\n",
    "There are other considerations that we have not explored here. The specific model we choose to inform our interventions can depend on a whole host of factors. Should projects in schools in low-income neighborhoods get priority? Should projects that ask for specific types of resources (e.g. stationery vs. computers) get priority? Should projects that are most cost-efficient, serving the largest number of children for the lowest per-child dollar amount, get priority? If we know more about the specific ways DonorsChoose might seek to use this model to identify projects, and the range of interventions available to them, we could provide more useful recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
