{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Improving the Pipeline, Again\n",
    "\n",
    "CAPP 30235 Machine Learning for Public Policy\n",
    "\n",
    "Jonathan Tan\n",
    "\n",
    "May 30, 2019\n",
    "\n",
    "# Part 1: Coding Assignment\n",
    "\n",
    "Since I use most of the new functions prompted below in my analysis of the DonorsChoose data, I'll just briefly explain in this section which new functions I wrote and what they do.\n",
    "\n",
    "### 1. Fix and improve the pipeline code you submitted for the last assignment based on the feedback from the TA. if something critical was pointed out in the feedback, you need to fix it. You'll get the last homework back by ends of thursday so you'll still have time before this one is due to address those comments.\n",
    "\n",
    "I received no feedback from my last assignment, so I have skipped this step.\n",
    "\n",
    "### 2. Add more classifiers to the pipeline. It should at least have Logistic Regression, KÂ­Nearest Neighbor, Decision Trees, SVM, Random Forests, Boosting, and Bagging. The code should have a parameter for running one or more of these classifiers and your analysis should run all of them.\n",
    "\n",
    "#### 2.1 `train_classifier()`\n",
    "\n",
    "`train_classifier()` takes 2 pandas DataFrames (features and labels of training data) and the name of a classifier to fit. It optionally takes a nested dictionary of hyperparameters to use for each. (For this notebook, these parameters can be changed in the `pipeline_config.py` file.) It returns a trained classifier object.\n",
    "\n",
    "### 3. Experiment with different parameters for these classifiers (different values of k for example, as well as parameters that other classifiers have). You should look at the sklearn documentation to see what parameter each classifier can take and what the default values sklearn selects. The labs should be helpful here.\n",
    "\n",
    "Results of experimenting with different parameters (e.g. penalty and C for LogisticRegression, n_estimators for Boosting) can be seen in the parameter dictionary in the `pipeline_config.py` file. I looped over several ranges of reasonable values for each parameter and selected the configuration producing the largest AUC-ROC. These will be the default parameters used in the DonorsChoose analysis part of this assignment.\n",
    "\n",
    "### 4. Add additional evaluation metrics that we've covered in class to the pipeline (accuracy, precision at different levels, recall at different levels, F1, area under curve, and precision-recall curves).\n",
    "\n",
    "#### 4.1 `validate_classifier()`\n",
    "\n",
    "`validate_classifier()` takes 2 dataframes (features and labels for test data) and a pre-trained classifier object as inputs, calculates several evaluation metrics (accuracy, precision, recall, F1, etc.) and returns a dictionary of those metrics.\n",
    "\n",
    "### 5. Create temporal validation function in your pipeline that can create training and test sets over time. You can choose the length of these splits based on analyzing the data. For example, the test sets could be six months long and the training sets could be all the data before each test set.\n",
    "\n",
    "#### 5.1 `split_data_temporal()`\n",
    "\n",
    "`split_data_temporal()` takes a pandas DataFrame and specified label/date column names as inputs, then splits the dataframe on the specified timeframe. The default test set duration is the most recent 1 year of data. It returns two dataframes and two series in order:\n",
    " 1. training features\n",
    " 2. test features\n",
    " 3. training labels\n",
    " 4. test labels\n",
    "\n",
    "\n",
    "# Part 2: Analysis\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "\n",
    "# Import pipeline library, hardcoded config file values\n",
    "import pipeline_library as library\n",
    "import pipeline_config as config\n",
    "\n",
    "# Tweak display settings for tables\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124976, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectid</th>\n",
       "      <th>teacher_acctid</th>\n",
       "      <th>schoolid</th>\n",
       "      <th>school_ncesid</th>\n",
       "      <th>school_latitude</th>\n",
       "      <th>school_longitude</th>\n",
       "      <th>school_city</th>\n",
       "      <th>school_state</th>\n",
       "      <th>school_metro</th>\n",
       "      <th>school_district</th>\n",
       "      <th>school_county</th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>primary_focus_subject</th>\n",
       "      <th>primary_focus_area</th>\n",
       "      <th>secondary_focus_subject</th>\n",
       "      <th>secondary_focus_area</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>datefullyfunded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001ccc0e81598c4bd86bacb94d7acb</td>\n",
       "      <td>96963218e74e10c3764a5cfb153e6fea</td>\n",
       "      <td>9f3f9f2c2da7edda5648ccd10554ed8c</td>\n",
       "      <td>1.709930e+11</td>\n",
       "      <td>41.807654</td>\n",
       "      <td>-87.673257</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>urban</td>\n",
       "      <td>Pershing Elem Network</td>\n",
       "      <td>Cook</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Visual Arts</td>\n",
       "      <td>Music &amp; The Arts</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>1498.61</td>\n",
       "      <td>31.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000fa3aa8f6649abab23615b546016d</td>\n",
       "      <td>2a578595fe351e7fce057e048c409b18</td>\n",
       "      <td>3432ed3d4466fac2f2ead83ab354e333</td>\n",
       "      <td>6.409801e+10</td>\n",
       "      <td>34.296596</td>\n",
       "      <td>-119.296596</td>\n",
       "      <td>Ventura</td>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Ventura Unif School District</td>\n",
       "      <td>Ventura</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>Civics &amp; Government</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Literature &amp; Writing</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Books</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>282.47</td>\n",
       "      <td>28.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>2012-04-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000134f07d4b30140d63262c871748ff</td>\n",
       "      <td>26bd60377bdbffb53a644a16c5308e82</td>\n",
       "      <td>dc8dcb501c3b2bb0b10e9c6ee2cd8afd</td>\n",
       "      <td>6.227100e+10</td>\n",
       "      <td>34.078625</td>\n",
       "      <td>-118.257834</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Los Angeles Unif Sch Dist</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Social Sciences</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>1012.38</td>\n",
       "      <td>56.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>2012-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001f2d0b3827bba67cdbeaa248b832d</td>\n",
       "      <td>15d900805d9d716c051c671827109f45</td>\n",
       "      <td>8bea7e8c6e4279fca6276128db89292e</td>\n",
       "      <td>3.600090e+11</td>\n",
       "      <td>40.687286</td>\n",
       "      <td>-73.988217</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "      <td>urban</td>\n",
       "      <td>New York City Dept Of Ed</td>\n",
       "      <td>Kings (Brooklyn)</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Books</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>175.33</td>\n",
       "      <td>23.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-10-11</td>\n",
       "      <td>2012-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004536db996ba697ca72c9e058bfe69</td>\n",
       "      <td>400f8b82bb0143f6a40b217a517fe311</td>\n",
       "      <td>fbdefab6fe41e12c55886c610c110753</td>\n",
       "      <td>3.606870e+11</td>\n",
       "      <td>40.793018</td>\n",
       "      <td>-73.205635</td>\n",
       "      <td>Central Islip</td>\n",
       "      <td>NY</td>\n",
       "      <td>suburban</td>\n",
       "      <td>Central Islip Union Free SD</td>\n",
       "      <td>Suffolk</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Literature &amp; Writing</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>3591.11</td>\n",
       "      <td>150.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>2013-03-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          projectid                    teacher_acctid  \\\n",
       "0  00001ccc0e81598c4bd86bacb94d7acb  96963218e74e10c3764a5cfb153e6fea   \n",
       "1  0000fa3aa8f6649abab23615b546016d  2a578595fe351e7fce057e048c409b18   \n",
       "2  000134f07d4b30140d63262c871748ff  26bd60377bdbffb53a644a16c5308e82   \n",
       "3  0001f2d0b3827bba67cdbeaa248b832d  15d900805d9d716c051c671827109f45   \n",
       "4  0004536db996ba697ca72c9e058bfe69  400f8b82bb0143f6a40b217a517fe311   \n",
       "\n",
       "                           schoolid  school_ncesid  school_latitude  \\\n",
       "0  9f3f9f2c2da7edda5648ccd10554ed8c   1.709930e+11        41.807654   \n",
       "1  3432ed3d4466fac2f2ead83ab354e333   6.409801e+10        34.296596   \n",
       "2  dc8dcb501c3b2bb0b10e9c6ee2cd8afd   6.227100e+10        34.078625   \n",
       "3  8bea7e8c6e4279fca6276128db89292e   3.600090e+11        40.687286   \n",
       "4  fbdefab6fe41e12c55886c610c110753   3.606870e+11        40.793018   \n",
       "\n",
       "   school_longitude    school_city school_state school_metro  \\\n",
       "0        -87.673257        Chicago           IL        urban   \n",
       "1       -119.296596        Ventura           CA        urban   \n",
       "2       -118.257834    Los Angeles           CA        urban   \n",
       "3        -73.988217       Brooklyn           NY        urban   \n",
       "4        -73.205635  Central Islip           NY     suburban   \n",
       "\n",
       "                school_district     school_county school_charter  \\\n",
       "0         Pershing Elem Network              Cook              f   \n",
       "1  Ventura Unif School District           Ventura              f   \n",
       "2     Los Angeles Unif Sch Dist       Los Angeles              f   \n",
       "3      New York City Dept Of Ed  Kings (Brooklyn)              f   \n",
       "4   Central Islip Union Free SD           Suffolk              f   \n",
       "\n",
       "  school_magnet teacher_prefix primary_focus_subject   primary_focus_area  \\\n",
       "0             f           Mrs.           Mathematics       Math & Science   \n",
       "1             f           Mrs.   Civics & Government     History & Civics   \n",
       "2             f            Ms.              Literacy  Literacy & Language   \n",
       "3             t            Ms.              Literacy  Literacy & Language   \n",
       "4             f           Mrs.              Literacy  Literacy & Language   \n",
       "\n",
       "  secondary_focus_subject secondary_focus_area resource_type    poverty_level  \\\n",
       "0             Visual Arts     Music & The Arts      Supplies  highest poverty   \n",
       "1    Literature & Writing  Literacy & Language         Books  highest poverty   \n",
       "2         Social Sciences     History & Civics    Technology     high poverty   \n",
       "3                     NaN                  NaN         Books     high poverty   \n",
       "4    Literature & Writing  Literacy & Language    Technology     high poverty   \n",
       "\n",
       "     grade_level  total_price_including_optional_support  students_reached  \\\n",
       "0  Grades PreK-2                                 1498.61              31.0   \n",
       "1     Grades 3-5                                  282.47              28.0   \n",
       "2     Grades 3-5                                 1012.38              56.0   \n",
       "3  Grades PreK-2                                  175.33              23.0   \n",
       "4  Grades PreK-2                                 3591.11             150.0   \n",
       "\n",
       "  eligible_double_your_impact_match date_posted datefullyfunded  \n",
       "0                                 f  2013-04-14      2013-05-02  \n",
       "1                                 t  2012-04-07      2012-04-18  \n",
       "2                                 f  2012-01-30      2012-04-15  \n",
       "3                                 f  2012-10-11      2012-12-05  \n",
       "4                                 f  2013-01-08      2013-03-25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATE_COLS = ['date_posted', 'datefullyfunded']\n",
    "\n",
    "df = pd.read_csv(config.DATA_PATH, parse_dates=DATE_COLS)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124976 entries, 0 to 124975\n",
      "Data columns (total 26 columns):\n",
      "projectid                                 124976 non-null object\n",
      "teacher_acctid                            124976 non-null object\n",
      "schoolid                                  124976 non-null object\n",
      "school_ncesid                             115743 non-null float64\n",
      "school_latitude                           124976 non-null float64\n",
      "school_longitude                          124976 non-null float64\n",
      "school_city                               124976 non-null object\n",
      "school_state                              124976 non-null object\n",
      "school_metro                              109752 non-null object\n",
      "school_district                           124804 non-null object\n",
      "school_county                             124976 non-null object\n",
      "school_charter                            124976 non-null object\n",
      "school_magnet                             124976 non-null object\n",
      "teacher_prefix                            124976 non-null object\n",
      "primary_focus_subject                     124961 non-null object\n",
      "primary_focus_area                        124961 non-null object\n",
      "secondary_focus_subject                   84420 non-null object\n",
      "secondary_focus_area                      84420 non-null object\n",
      "resource_type                             124959 non-null object\n",
      "poverty_level                             124976 non-null object\n",
      "grade_level                               124973 non-null object\n",
      "total_price_including_optional_support    124976 non-null float64\n",
      "students_reached                          124917 non-null float64\n",
      "eligible_double_your_impact_match         124976 non-null object\n",
      "date_posted                               124976 non-null datetime64[ns]\n",
      "datefullyfunded                           124976 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(5), object(19)\n",
      "memory usage: 24.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a226df5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of number of days to being fully funded\n",
    "df['days_to_funded'] = (df['datefullyfunded'] - df['date_posted']).dt.days\n",
    "df['days_to_funded'].hist(grid=False, edgecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2881353219818205"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of projects are not funded within 60 days?\n",
    "len(df.loc[df['days_to_funded'] > 60]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29% of projects were not funded within 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>124976.000000</td>\n",
       "      <td>124917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>654.011811</td>\n",
       "      <td>95.445760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1098.015854</td>\n",
       "      <td>163.481912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>345.810000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>510.500000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>752.960000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>164382.840000</td>\n",
       "      <td>12143.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_price_including_optional_support  students_reached\n",
       "count                           124976.000000     124917.000000\n",
       "mean                               654.011811         95.445760\n",
       "std                               1098.015854        163.481912\n",
       "min                                 92.000000          1.000000\n",
       "25%                                345.810000         23.000000\n",
       "50%                                510.500000         30.000000\n",
       "75%                                752.960000        100.000000\n",
       "max                             164382.840000      12143.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = ['total_price_including_optional_support', 'students_reached']\n",
    "\n",
    "library.describe_data(df, varlist=numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a1d162eb8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A lot of 999 values and above - discretize and add a topcode \n",
    "df[['students_reached']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a1d30ec50>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['total_price_including_optional_support']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1d8e34e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['school_metro']].groupby('school_metro').size().sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1da919e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAD8CAYAAAAc/1/bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGTFJREFUeJzt3XuwZWV95vHvI5cG5C7IdLg1YEcuERtoQFBRMBMcOwqMUuK0BhItBsepciYxigWFGicWSiYSFSREU+IMKKKAjVcuOoEoF7vbbhsUwq1JBEqCCChaXH/zx3oPvTmc1d2HPqf3bvr7qdq11373Wuv81ltsnn7ftfbaqSokSdKzvWDYBUiSNKoMSUmSehiSkiT1MCQlSephSEqS1MOQlCSphyEpSVIPQ1KSpB6GpCRJPTYcdgF6tu22265mzZo17DIkaZ2yaNGi+6tq+6ncpyE5gmbNmsXChQuHXYYkrVOS3DXV+3S6VZKkHoakJEk9DElJknoYkpIk9TAkJUnqYUhKktTDkJQkqYchKUlSD0NSkqQehqQkST0MSUmSehiSkiT18AbnI2jZ3Q8x6+RvDrsMSVqrlp8+b9glPIsjSUmSehiSkiT1MCQlSephSEqS1MOQlCSphyEpSVIPQ1KSpB6GpCRJPQxJSZJ6TGtIJtkhyQVJ7kiyKMm1SY5Zw31+OMn71nAfs5L8LsmSJD9Nck6SSfVFkuVJtmvLByS5M8l+E6z35SS3JLkxyeeSeJcjSVpHTFtIJglwKXB1Ve1eVQcAxwE7TbDuMILj9qqaA+wL7A0cPa6mDVZnJ0n2Bb4KvLWqfjzBKl8E9mx/ZyvgT9ekaEnS2jOdI8kjgMeq6pyxhqq6q6o+DZDkhCQXJbkMuDzJ5kmuSrI4ybIkR41tl+SUNhq7EnjpQPseSb7TRqnXJNmztR/bRm5Lk1y9siKr6gngh8BLkrw2yfeTXAAsa/t6e5Ib2qjz78eF5150/xB4R1Xd0LP/b1XnKeAGJvhHgiRpNE3nCG4fYPEq1jkE2LeqHmijyWOq6uE2jXldkgXA/nQj0P1avYuBRW37c4GTqurWJAcDZ9OF82nAkVV1d5KtV1ZAks2A17VtAA4C/qCq7kyyF/BW4JVV9XiSs4H5dKNDgK8Db6+qf15VZyTZuG377lWtK0kaDWttmjPJWcCr6EaXB7bmK6rqgbFVgI8lOQx4CtgR2AF4NXBJVf227WdBe94cOBS4qJvZBWBGe/4B8IUkXwEu7ilpjyRLgAK+XlXfTvJa4IaqurOt8zrgAOBH7W9sCtw3sI8rgXcl+W5VPbmKLjgHuLKqrp3ozSQnAicCbLDl9qvYlSRpbZjOkLwJePPYi6p6TxshLhxY55GB5fnA9sABbdS2HNhkbPMJ9v8C4MF2XvEZquqkNrKcByxJMqeqfjlutdsn2nZcTQHOq6oPTniE8N/pwu9s4L8CtCnh7YDrquqk1vZRuvOR7+rZD1V1Lt3ImBkzZ090vJKktWw6z0l+D9gkyeD04mYrWX8r4L4WkIcDu7b2q4FjkmyaZAvgjQBV9TBwZ5JjobtQKMnL2/IeVXV9VZ0G3A/s/ByP4SrgLUle3Pa7bZJdB95/Cngb8NIkf9Xq+sOqmjMQkCcBrwXmt/OSkqR1xLSFZFUV3RWjr2lfj7gBOA/4QM8m5wNzkyykG1Xe3PazGLgQWAJ8DbhmYJv5wDuTLKUbuY5d7HNGu/jnRrqQXfocj+GnwKl0Fxb9BLgCmDlunUfb331TkvcMvtcu8vlM2+a6dvHPKc+lFknS2pcuyzRKZsycXTOPP3PYZUjSWrX89HlrtH2SRVU1d4rKAbzjjiRJvQxJSZJ6GJKSJPUwJCVJ6mFISpLUw5CUJKmHISlJUg9DUpKkHv4A8Ah62Y5bsXANv1QrSVpzjiQlSephSEqS1MOQlCSphyEpSVIPQ1KSpB6GpCRJPQxJSZJ6GJKSJPUwJCVJ6mFISpLUw5CUJKmHISlJUg9DUpKkHoakJEk9DElJknoYkpIk9TAkJUnqYUhKktTDkJQkqYchKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1GPDYRegZ1t290PMOvmbwy5D0ghZfvq8YZewXnIkKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1MOQlCSphyEpSVKPkQ/JJDskuSDJHUkWJbk2yTFruM8PJ3nfGu5j11bPkiQ3JTmpZ71ZSX7X1luS5Jw1+buSpLVnpG9LlyTApcB5VfVfWtuuwJsmWHfDqnpiLZZ3L3BoVT2aZHPgxiQLquqeCda9varmrMXaJElTYNRHkkcAj1XV06Ovqrqrqj4NkOSEJBcluQy4PMnmSa5KsjjJsiRHjW2X5JQktyS5EnjpQPseSb7TRoXXJNmztR+b5MYkS5NcPb6wqnqsqh5tL2cw+n0pSZqkkR5JAvsAi1exziHAvlX1QJINgWOq6uEk2wHXJVkA7A8cB+xHd8yLgUVt+3OBk6rq1iQHA2fThfNpwJFVdXeSrSf6w0l2Br4JvAT4y55RJMBuSX4MPAycWlXXrNbRS5KGapUhmWTblb1fVQ9MXTmrrOUs4FV0o8sDW/MVAzUE+FiSw4CngB2BHYBXA5dU1W/bfha0582BQ4GLupldoBsVAvwA+EKSrwAXT1RPVf0bsG+S3wMuTfLVqvrFuNXuBXapql8mOaCtt09VPTzu2E4ETgTYYMvtJ9UvkqTpsTojyUVA0QXQeAXsPqUVPdNNwJuf/mNV72kjxIUD6zwysDwf2B44oKoeT7Ic2GSg1vFeADw40fnCqjqpjSznAUuSzKmqX05UZFXdk+Qm4NVJngQ+1N56V1UtBB5t6y1Kcjvw++OOgao6l25Uy4yZsyeqVZK0lq3yPFpV7VZVu7fn8Y/pDEiA7wGbJHn3QNtmK1l/K+C+FpCHA7u29quBY5JsmmQL4I0AbTR3Z5JjobtQKMnL2/IeVXV9VZ0G3A/sPPiHkuyUZNO2vA3wSuCWqrqkqua0x8Ik2yfZoK23OzAbuGMN+kSStJas9jnJdqXpfGC3qvpokl2A/1BVN0xXcVVVSY4GPpnk/cC/040cP9CzyfnAZUkWAkuAm9t+Fie5sLXdBQyeE5wPfDbJqcBGwJeBpcAZSWbTjaCvam2D9gL+d5KxUfbfVNWyCWo6DPirJE8AT9Kd/1xrU9SSpOcuVas3s5fks3Tn+Y6oqr3a6OnygXODmiIzZs6umcefOewyJI2Q5afPG3YJIy/JoqqaO5X7nMzVrQdX1f7tKk2q6ldJNp7KYiRJGiWT+W7f4+3cWgEk2Z5uZClJ0vPSZELyU8AlwIuT/DXwz8DHpqUqSZJGwGpPt1bV+UkWAa+ju1Dl6Kr62bRVJknSkE3m6ta/Ay6sqrOmsR5JkkbGZKZbFwOnJrktyRlJpvQKIkmSRs1qh2RVnVdVbwAOAv4F+HiSW6etMkmShuy5/HLFS4A9gVm0L+tLkvR8NJlzkh8H/jNwO3Ah8NGqenC6ClufvWzHrVjoF4claegmczOBO4FDqur+6SpGkqRRMpnp1nOB1yc5DSDJLkkOmp6yJEkavsmE5Fl0P3D8tvb6161NkqTnJe/dKklSD+/dKklSD+/dKklSD+/dKklSj1WGZJJtB17eB3xp8L2qemA6CpMkadhWZyS5iO48ZNrras9py7tPQ12SJA3dKkOyqnZbnR0l2aeqblrzkiRJGg3P5d6tff7PFO5LkqShm8qQzKpXkSRp3TGVIVmrXkWSpHXHVIakJEnPK1MZko9N4b4kSRq61Q7JdN7e9ysgVfWK6ShQkqRhmcxI8mz8FRBJ0nrEXwGRJKmHvwIiSVIPfwVEkqQe/gqIJEk9/BUQSZJ6TPZXQHYBftWWtwb+FVitG6BLkrSuWeU5yararap2B74LvLGqtquqFwF/DFw83QVKkjQsk7lw58Cq+tbYi6r6NvCaqS9JkqTRMJnvSd6f5FTg/9JNv74d+OW0VCVJ0giYzEjybcD2dF8DuRR4MSvuviNJ0vNOqvyFq1EzY+bsmnn8mcMuQ+ug5afPG3YJ0tAkWVRVc6dyn6s93drusPN+YB9gk7H2qjpiKguSJGlUTGa69XzgZrqvfHwEWA78aBpqkiRpJEwmJF9UVZ8HHq+qf6qqPwP8eSxJ0vPWZK5ufbw935tkHnAPsNPUlyRJ0miYTEj+ryRbAX8BfBrYEvif01KVJEkjYLVCsv1E1uyq+gbwEHD4tFYlSdIIWK1zklX1JPCmaa5FkqSRMpnp1h8m+QxwIfDIWGNVLZ7yqiRJGgGTCclD2/NH2nPobk/n9yQlSc9LkwnJb7DiJ7Noyw8nmVNVS6a8sibJDsAn6b5u8ivgMeATVXXJGuzzw8Bvqupv1rC2XYDPATvT9ccbqmr5BOt9AphHN719BfDe8lZHkjTyJvM9yQOAk4CZwO8BJ9L9Csg/JHn/NNRGktDdJ/bqqtq9qg4AjmOCr54kmUzgT5UvAmdU1V7AQXQ/Sv0MSQ4FXgnsC/wBcCD+eookrRMmdTMBYP+qel9V/QUwl+6G54cBJ0xDbdBN5T5WVeeMNVTVXVX1aYAkJyS5KMllwOVJNk9yVZLFSZYlOWpsuySnJLklyZXASwfa90jynSSLklyTZM/WfmySG5MsTXL1+MKS7A1sWFVXtLp+U1W/neAYiu42fhsDM4CNgF+seddIkqbbZEZfu9BNdY55HNi1qn6X5NGpLetp+wCrujDoEGDfqnqgjSaPqaqHk2wHXJdkAbA/3Qh0P7pjXgwsatufC5xUVbcmORg4my6cTwOOrKq7k2w9wd/9feDBJBfT3arvSuDkdiXw06rq2iTfB+6lm6r+TFX9bPzOkpxINzpngy23X8UhS5LWhsmE5AV0ofP19vqNwJeSvBD46ZRXNoEkZwGvohtdHtiar6iqB8ZWAT6W5DDgKWBHYAfg1cAlYyO9Fpwk2ZzugqSLupldoBvtAfwA+EKSrwAXT1DOhm2/+wH/SnfV7wnA58fV/BJgL1ZMEV+R5LCqesbotKrOpQtsZsyc7flKSRoBqx2SVfXRJN+iC6nQjb4WtrfnT0dxwE3AmwdqeE8bIS4cWOeRgeX5dFPAB1TV40mWs+IXSyYKnhcAD1bVnPFvVNVJbWQ5D1jSLlAa/JHpnwM/rqo7AJJcCrwiyY3A37d1TqOb2r2uqn7T1vs23UVIz5rClSSNlsmck6SqFlXV31XVmQMBOZ2+B2yS5N0DbZutZP2tgPtaQB4O7NrarwaOSbJpki3oRsFU1cPAnUmOhe5CoSQvb8t7VNX1VXUacD/dFayDfgRs035CDLop2p+2bea0xwK6UeZrkmyYZCO6i3aeNd0qSRo9kwrJta19TeJoupC5M8kNwHnAB3o2OR+Ym2Qh3ajy5rafxXTToUuArwHXDGwzH3hnkqV0I9exi33OaBf/3EgXskvH1fYk8D7gqiTL6EbX/zBBTV8FbgeWtX0srarLVr8XJEnDEr+uN3pmzJxdM48/c9hlaB20/PR5wy5BGpoki6pq7lTuc6RHkpIkDZMhKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1GMYv8GoVXjZjlux0C+FS9LQOZKUJKmHISlJUg9DUpKkHoakJEk9DElJknoYkpIk9TAkJUnqYUhKktTDkJQkqYchKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1MOQlCSphyEpSVIPQ1KSpB6GpCRJPQxJSZJ6GJKSJPUwJCVJ6mFISpLUw5CUJKmHISlJUo8Nh12Anm3Z3Q8x6+RvDrsMTdLy0+cNuwRJU8yRpCRJPQxJSZJ6GJKSJPUwJCVJ6mFISpLUw5CUJKmHISlJUg9DUpKkHoakJEk9RiIkk+yQ5IIkdyRZlOTaJMes4T4/nOR9U1Dbx5Pc2B5vXcl630nyYJJvjGs/P8ktbft/TLLRmtYkSVo7hh6SSQJcClxdVbtX1QHAccBOE6y7Vm+jl2QesD8wBzgY+MskW/asfgbwjgnazwf2BF4GbAq8axpKlSRNg6GHJHAE8FhVnTPWUFV3VdWnAZKckOSiJJcBlyfZPMlVSRYnWZbkqLHtkpzSRm1XAi8daN+jjfQWJbkmyZ6t/dg2wlua5OoJatsb+KeqeqKqHgGWAq+f6CCq6irg1xO0f6sa4AYmCH9J0mgahRuc7wMsXsU6hwD7VtUDbTR5TFU9nGQ74LokC+hGfMcB+9Ed12JgUdv+XOCkqro1ycHA2XThfBpwZFXdnWTrCf7uUuBDSf4W2Aw4HPjpcznINs36DuC9z2V7SdLaNwoh+QxJzgJeRTe6PLA1X1FVD4ytAnwsyWHAU8COwA7Aq4FLquq3bT8L2vPmwKHARd3MLgAz2vMPgC8k+Qpw8fhaquryJAcCPwT+HbgWeOI5HtrZdFPK1/Qc94nAiQAbbLn9c/wTkqSpNArTrTfRjQIBqKr3AK8DBpPikYHl+e29A6pqDvALYJOxzSfY/wuAB6tqzsBjr/a3TgJOBXYGliR50fiNq+qv2zb/kS6gb01ycJIl7fGmVR1gkg+1mv+8b52qOreq5lbV3A0222pVu5QkrQWjEJLfAzZJ8u6Bts1Wsv5WwH1V9XiSw4FdW/vVwDFJNk2yBfBGgKp6GLgzybHQXSiU5OVteY+qur6qTgPupwvLpyXZYCw4k+wL7Atc3rYZC9wFKzu4JO8CjgTeVlVPrUZ/SJJGxNCnW6uqkhwNfDLJ++mmNR8BPtCzyfnAZUkWAkuAm9t+Fie5sLXdBQxOa84HPpvkVGAj4Mt05xvPSDKbboR4VWsbtBFwTZumfRh4e1VNON2a5Bq6q1g3T/Jz4J1V9V3gnFbPtW0/F1fVX61W50iShirdRZcaJTNmzq6Zx5857DI0SctPnzfsEqT1WpJFVTV3Kvc5CtOtkiSNJENSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1MOQlCSpx9DvuKNne9mOW7HQL6ZL0tA5kpQkqYchKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1MOQlCSphyEpSVIPQ1KSpB6GpCRJPQxJSZJ6pKqGXYPGSfJr4JZh1zEitgPuH3YRI8B+WMG+WMG+WGE74IVVtf1U7tSfyhpNt1TV3GEXMQqSLLQv7IdB9sUK9sUKrS9mTfV+nW6VJKmHISlJUg9DcjSdO+wCRoh90bEfVrAvVrAvVpiWvvDCHUmSejiSlCSphyE5QpK8PsktSW5LcvKw65kqSf4xyX1Jbhxo2zbJFUlubc/btPYk+VTrg58k2X9gm+Pb+rcmOX6g/YAky9o2n0qStXuEqyfJzkm+n+RnSW5K8t7Wvj72xSZJbkiytPXFR1r7bkmub8d1YZKNW/uM9vq29v6sgX19sLXfkuTIgfZ16vOUZIMkP07yjfZ6veyLJMvbf8NLkixsbcP7jFSVjxF4ABsAtwO7AxsDS4G9h13XFB3bYcD+wI0DbZ8ATm7LJwMfb8tvAL4NBHgFcH1r3xa4oz1v05a3ae/dABzStvk28J+Gfcw9/TAT2L8tbwH8C7D3etoXATZvyxsB17dj/ApwXGs/B3h3W/5vwDlt+Tjgwra8d/uszAB2a5+hDdbFzxPw58AFwDfa6/WyL4DlwHbj2ob2GXEkOToOAm6rqjuq6jHgy8BRQ65pSlTV1cAD45qPAs5ry+cBRw+0f7E61wFbJ5kJHAlcUVUPVNWvgCuA17f3tqyqa6v7BHxxYF8jparurarFbfnXwM+AHVk/+6Kq6jft5UbtUcARwFdb+/i+GOujrwKvayOAo4AvV9WjVXUncBvdZ2md+jwl2QmYB3yuvQ7raV/0GNpnxJAcHTsC/zbw+uet7flqh6q6F7rwAF7c2vv6YWXtP5+gfaS1KbL96EZQ62VftOnFJcB9dP8Tux14sKqeaKsM1v/0Mbf3HwJexOT7aFSdCbwfeKq9fhHrb18UcHmSRUlObG1D+4x4x53RMdG8+Pp46XFfP0y2fWQl2Rz4GvA/qurhlZwSeV73RVU9CcxJsjVwCbDXRKu158ke80QDgJHsiyR/DNxXVYuSvHaseYJVn/d90byyqu5J8mLgiiQ3r2Tdaf+MOJIcHT8Hdh54vRNwz5BqWRt+0aY+aM/3tfa+flhZ+04TtI+kJBvRBeT5VXVxa14v+2JMVT0I/D+6c0pbJxn7x/tg/U8fc3t/K7op/Mn20Sh6JfCmJMvppkKPoBtZro99QVXd057vo/vH00EM8TNiSI6OHwGz2xVtG9OdkF8w5Jqm0wJg7Iqz44GvD7T/Sbtq7RXAQ2165bvAHyXZpl3Z9kfAd9t7v07yinZe5k8G9jVSWn2fB35WVX878Nb62BfbtxEkSTYF/pDuHO33gbe01cb3xVgfvQX4XjuntAA4rl3xuRswm+7CjHXm81RVH6yqnaq77+hxdMc2n/WwL5K8MMkWY8t0/23fyDA/I8O+ksnHM67gegPdFY+3A6cMu54pPK4vAfcCj9P9S+6ddOdQrgJubc/btnUDnNX6YBkwd2A/f0Z3McJtwJ8OtM9tH6Tbgc/QbpIxag/gVXRTOz8BlrTHG9bTvtgX+HHrixuB01r77nT/Y78NuAiY0do3aa9va+/vPrCvU9rx3sLAlYrr4ucJeC0rrm5d7/qiHfPS9rhprNZhfka8444kST2cbpUkqYchKUlSD0NSkqQehqQkST0MSUmSehiSkiT1MCQlSephSEqS1OP/A1SReHx1S0pPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['grade_level']].groupby('grade_level').size().sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1db0b160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of projects from the 10 largest states\n",
    "df[['school_state']].groupby('school_state').size().sort_values().tail(10).plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1dca2940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of projects from the 10 largest cities\n",
    "df[['school_city']].groupby('school_city').size().sort_values().tail(10).plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1e1c6be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['primary_focus_area']].groupby('primary_focus_area').size().sort_values().plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1d41f0b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['secondary_focus_area']].groupby('secondary_focus_area').size().sort_values().plot.barh(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocess data\n",
    "\n",
    "1. Based on data exploration, select features to be cleaned up and used in the model\n",
    "2. See what data is missing (for later imputation)\n",
    "\n",
    "### 3.1 Select features for the final model(s)\n",
    "\n",
    "Here, I've excluded extraneous features like the ID columns and lat/long data, as well as geographical features like `school_district` and `school_city` that should be highly collinear with the remaining `school_county` and `school_state` features. The remaining features are, as decribed by the [datasource](https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/data):\n",
    "\n",
    "| Feature | Type | Description\n",
    "| ---------- | ---------- | ----------\n",
    "| `school_city` | categorical | City in which school is located\n",
    "| `school_state` | categorical | State in which school is located\n",
    "| `school_metro` | categorical | Rural, urban, or suburban\n",
    "| `school_charter` | boolean | Is the school a public charter school?\n",
    "| `school_magnet` | boolean | Is the school a public magnet school?\n",
    "| `teacher_prefix` | categorical | Dr., Mr., Mrs., or Ms.\n",
    "| `primary_focus_area`| categorical | Main subject area for which project materials are intended\n",
    "| `secondary_focus_area` | categorical | Secondary subject area for which project materials are intended\n",
    "| `resource_type` | categorical | Books, supplies, technology, etc.\n",
    "| `poverty_level` | categorical | Low, moderate, high, highest\n",
    "| `grade_level` | categorical | Grade level for which project materials are intended\n",
    "| `total_price_including_optional_support` | categorical | Project cost including optional tip that donors give to DonorsChoose.org while funding a project \n",
    "| `students_reached` | numeric | Number of students impacted by a project (if funded)\n",
    "| `eligible_double_your_impact_match` | boolean | Project was eligible for a 50% off offer by a corporate partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['school_state', 'school_metro', 'school_charter',\n",
    "       'school_magnet', 'teacher_prefix', 'primary_focus_area', 'secondary_focus_area',\n",
    "       'resource_type', 'poverty_level', 'grade_level',\n",
    "       'total_price_including_optional_support', 'students_reached',\n",
    "       'eligible_double_your_impact_match', 'date_posted', 'datefullyfunded']\n",
    "\n",
    "df = df[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>num_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>school_metro</td>\n",
       "      <td>15224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>primary_focus_area</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>secondary_focus_area</td>\n",
       "      <td>40556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resource_type</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grade_level</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>students_reached</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  num_missing\n",
       "1           school_metro        15224\n",
       "5     primary_focus_area           15\n",
       "6   secondary_focus_area        40556\n",
       "7          resource_type           17\n",
       "9            grade_level            3\n",
       "11      students_reached           59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = df.isnull().sum(axis=0).reset_index().rename({'index': 'feature', 0: 'num_missing'}, axis=1)\n",
    "\n",
    "missing.loc[missing['num_missing'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split data into test and train sets by time\n",
    "\n",
    "Per the instructions, \"The data spans Jan 1, 2012 to Dec 31, 2013. You should have your validation/test set be a rolling window of 6 months (which should give you three test sets). The training sets should be everything from 1/1/12 to the beginning of the test set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test durations are recorded in conflg.TEMPORAL_SPLITS, which is a list of dictionaries \n",
    "# that keep start and end dates for test sets as entries.\n",
    "\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "for i in config.TEMPORAL_SPLITS:\n",
    "    train_dfs.append(df.loc[df['date_posted'] < pd.to_datetime(i['test_start'])])\n",
    "    test_dfs.append(df.loc[(df['date_posted'] >= pd.to_datetime(i['test_start'])) &\n",
    "                           (df['date_posted'] <= pd.to_datetime(i['test_end']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA:\n",
      "1 Start: 2012-01-01, End: 2012-06-30\n",
      "2 Start: 2012-01-01, End: 2012-12-31\n",
      "3 Start: 2012-01-01, End: 2013-06-30\n",
      "TEST DATA:\n",
      "1 Start: 2012-07-01, End: 2012-12-31\n",
      "2 Start: 2013-01-01, End: 2013-06-30\n",
      "3 Start: 2013-07-01, End: 2013-12-31\n"
     ]
    }
   ],
   "source": [
    "# Verify that train and test data were split properly\n",
    "print(\"TRAINING DATA:\")\n",
    "for idx, i in enumerate(train_dfs):\n",
    "    print(f\"{idx+1} Start: {i['date_posted'].min().date()}, End: {i['date_posted'].max().date()}\")\n",
    "print(\"TEST DATA:\")\n",
    "for idx, j in enumerate(test_dfs):\n",
    "    print(f\"{idx+1} Start: {j['date_posted'].min().date()}, End: {j['date_posted'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning, Feature Generation\n",
    "\n",
    "To avoid pre-processing data that will be omitted from the training data, below I pre-select the features I want to use and focus on cleaning those.\n",
    "\n",
    "Data cleaning steps:\n",
    "1. Define label: `funded_in_60_days`\n",
    "2. Clean missing data\n",
    "3. Normalize numeric data\n",
    "4. Make binary data into true binary\n",
    "5. Make categorical data into dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define label: `funded_in_60_days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "        \n",
    "        # Label = 1 if datefullyfunded is more than 60 days after date_posted\n",
    "        df['not_funded_60_days'] = \\\n",
    "            np.where(df['datefullyfunded'] - df['date_posted'] > pd.to_timedelta(60, unit='days'), 1, 0)\n",
    "        \n",
    "        # Leave a lag period of 60 days at the end of each dataset\n",
    "        df = df.loc[df['date_posted'].max() - df['date_posted'] > pd.to_timedelta(60, unit='days')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>school_metro</th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>primary_focus_area</th>\n",
       "      <th>secondary_focus_area</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>datefullyfunded</th>\n",
       "      <th>not_funded_60_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Books</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>282.47</td>\n",
       "      <td>28.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>2012-04-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Technology</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>1012.38</td>\n",
       "      <td>56.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>2012-04-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KY</td>\n",
       "      <td>suburban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>481.18</td>\n",
       "      <td>120.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-01-26</td>\n",
       "      <td>2012-05-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Books</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>296.68</td>\n",
       "      <td>30.0</td>\n",
       "      <td>t</td>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>2012-02-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CA</td>\n",
       "      <td>suburban</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Special Needs</td>\n",
       "      <td>Technology</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>2130.40</td>\n",
       "      <td>30.0</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-04-29</td>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_state school_metro school_charter school_magnet teacher_prefix  \\\n",
       "1            CA        urban              f             f           Mrs.   \n",
       "2            CA        urban              f             f            Ms.   \n",
       "16           KY     suburban              f             f            Ms.   \n",
       "18           CA        urban              f             f            Ms.   \n",
       "23           CA     suburban              f             f           Mrs.   \n",
       "\n",
       "     primary_focus_area secondary_focus_area resource_type    poverty_level  \\\n",
       "1      History & Civics  Literacy & Language         Books  highest poverty   \n",
       "2   Literacy & Language     History & Civics    Technology     high poverty   \n",
       "16       Math & Science       Math & Science      Supplies     high poverty   \n",
       "18  Literacy & Language                  NaN         Books  highest poverty   \n",
       "23     History & Civics        Special Needs    Technology  highest poverty   \n",
       "\n",
       "      grade_level  total_price_including_optional_support  students_reached  \\\n",
       "1      Grades 3-5                                  282.47              28.0   \n",
       "2      Grades 3-5                                 1012.38              56.0   \n",
       "16     Grades 6-8                                  481.18             120.0   \n",
       "18     Grades 3-5                                  296.68              30.0   \n",
       "23  Grades PreK-2                                 2130.40              30.0   \n",
       "\n",
       "   eligible_double_your_impact_match date_posted datefullyfunded  \\\n",
       "1                                  t  2012-04-07      2012-04-18   \n",
       "2                                  f  2012-01-30      2012-04-15   \n",
       "16                                 f  2012-01-26      2012-05-17   \n",
       "18                                 t  2012-01-23      2012-02-20   \n",
       "23                                 f  2012-04-29      2012-07-16   \n",
       "\n",
       "    not_funded_60_days  \n",
       "1                    0  \n",
       "2                    1  \n",
       "16                   1  \n",
       "18                   0  \n",
       "23                   1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that label has been created correctly\n",
    "train_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "for df_list in (train_dfs, test_dfs):\n",
    "    for i in range(len(df_list)):\n",
    "\n",
    "        df_list[i] = df_list[i].drop(labels=['date_posted', 'datefullyfunded'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Clean missing data\n",
    "\n",
    "For many categorical features, we have no reliable way of filling in the missing data with useful placeholders. I'll leave these as missing, and when the categorical variable is discretized into a series of binary feature later, all missing values will be indicated in their own binary feature.\n",
    "\n",
    "For the numeric column with missing data - students reached - we will impute missing values with the median value of the feature, and add a binary feature indicating the value was imputed (this is done with `fill_missing()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>num_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>school_metro</td>\n",
       "      <td>15224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>primary_focus_area</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>secondary_focus_area</td>\n",
       "      <td>40556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resource_type</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grade_level</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>students_reached</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  num_missing\n",
       "1           school_metro        15224\n",
       "5     primary_focus_area           15\n",
       "6   secondary_focus_area        40556\n",
       "7          resource_type           17\n",
       "9            grade_level            3\n",
       "11      students_reached           59"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing.loc[missing['num_missing'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "        df = library.fill_missing(df, ['students_reached'], median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Rescale/standardize numeric data\n",
    "\n",
    "Rescale numeric features to mean 0, standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.638600e+04</td>\n",
       "      <td>2.638600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.023244e-18</td>\n",
       "      <td>1.717551e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000019e+00</td>\n",
       "      <td>1.000019e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.137497e-01</td>\n",
       "      <td>-5.434016e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.645127e-01</td>\n",
       "      <td>-4.143174e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9.645843e-02</td>\n",
       "      <td>-3.651425e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.540535e-02</td>\n",
       "      <td>3.669342e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.976072e+01</td>\n",
       "      <td>5.659170e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_price_including_optional_support  students_reached\n",
       "count                            2.638600e+04      2.638600e+04\n",
       "mean                            -9.023244e-18      1.717551e-16\n",
       "std                              1.000019e+00      1.000019e+00\n",
       "min                             -5.137497e-01     -5.434016e-01\n",
       "25%                             -2.645127e-01     -4.143174e-01\n",
       "50%                             -9.645843e-02     -3.651425e-01\n",
       "75%                              8.540535e-02      3.669342e-03\n",
       "max                              8.976072e+01      5.659170e+01"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "\n",
    "        for i in numeric_features:\n",
    "            df[i] = scale(df[i])\n",
    "    \n",
    "train_dfs[0][numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Make binary data into true binary format\n",
    "\n",
    "The `school_charter`, `school_magnet` and `eligible_double_your_impact_match` features are also binary variables coded as string variables (\"t\" or \"f\"), so we'll convert them to a true binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_charter school_magnet eligible_double_your_impact_match\n",
       "1               f             f                                 t\n",
       "2               f             f                                 f\n",
       "16              f             f                                 f\n",
       "18              f             f                                 t\n",
       "23              f             f                                 f"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features = ['school_charter', 'school_magnet', 'eligible_double_your_impact_match']\n",
    "\n",
    "train_dfs[0][binary_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    school_charter  school_magnet  eligible_double_your_impact_match\n",
       "1                0              0                                  1\n",
       "2                0              0                                  0\n",
       "16               0              0                                  0\n",
       "18               0              0                                  1\n",
       "23               0              0                                  0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "        \n",
    "        for i in binary_features:\n",
    "            df[i] = np.where(df[i] == 't', 1, 0)\n",
    "\n",
    "train_dfs[0][binary_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Make categorical data into dummies\n",
    "\n",
    "For locational columns with high cardinality like `school_city` (4,276 unique values), I retain the values for the top 50 most frequent values (see Section 2 above) and bin all other city/state locations in \"other\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school_state has 51 unique values\n",
      "school_metro has 4 unique values\n",
      "teacher_prefix has 4 unique values\n",
      "resource_type has 6 unique values\n",
      "primary_focus_area has 7 unique values\n",
      "secondary_focus_area has 8 unique values\n",
      "poverty_level has 4 unique values\n",
      "grade_level has 4 unique values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>school_metro</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>primary_focus_area</th>\n",
       "      <th>secondary_focus_area</th>\n",
       "      <th>poverty_level</th>\n",
       "      <th>grade_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>Books</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KY</td>\n",
       "      <td>suburban</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>Math &amp; Science</td>\n",
       "      <td>high poverty</td>\n",
       "      <td>Grades 6-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CA</td>\n",
       "      <td>urban</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>Books</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades 3-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CA</td>\n",
       "      <td>suburban</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>History &amp; Civics</td>\n",
       "      <td>Special Needs</td>\n",
       "      <td>highest poverty</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_state school_metro teacher_prefix resource_type  \\\n",
       "1            CA        urban           Mrs.         Books   \n",
       "2            CA        urban            Ms.    Technology   \n",
       "16           KY     suburban            Ms.      Supplies   \n",
       "18           CA        urban            Ms.         Books   \n",
       "23           CA     suburban           Mrs.    Technology   \n",
       "\n",
       "     primary_focus_area secondary_focus_area    poverty_level    grade_level  \n",
       "1      History & Civics  Literacy & Language  highest poverty     Grades 3-5  \n",
       "2   Literacy & Language     History & Civics     high poverty     Grades 3-5  \n",
       "16       Math & Science       Math & Science     high poverty     Grades 6-8  \n",
       "18  Literacy & Language                  NaN  highest poverty     Grades 3-5  \n",
       "23     History & Civics        Special Needs  highest poverty  Grades PreK-2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['school_state', 'school_metro', 'teacher_prefix', \\\n",
    "                        'resource_type', 'primary_focus_area', 'secondary_focus_area', \\\n",
    "                        'poverty_level', 'grade_level']\n",
    "\n",
    "for i in categorical_features:\n",
    "    print(f'{i} has {df[i].unique().shape[0]} unique values')\n",
    "    \n",
    "train_dfs[0][categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_charter</th>\n",
       "      <th>school_magnet</th>\n",
       "      <th>total_price_including_optional_support</th>\n",
       "      <th>students_reached</th>\n",
       "      <th>eligible_double_your_impact_match</th>\n",
       "      <th>not_funded_60_days</th>\n",
       "      <th>students_reached_imputed</th>\n",
       "      <th>school_state_AK</th>\n",
       "      <th>school_state_AL</th>\n",
       "      <th>school_state_AR</th>\n",
       "      <th>school_state_AZ</th>\n",
       "      <th>school_state_CA</th>\n",
       "      <th>school_state_CO</th>\n",
       "      <th>school_state_CT</th>\n",
       "      <th>school_state_DC</th>\n",
       "      <th>school_state_DE</th>\n",
       "      <th>school_state_FL</th>\n",
       "      <th>school_state_GA</th>\n",
       "      <th>school_state_HI</th>\n",
       "      <th>school_state_IA</th>\n",
       "      <th>school_state_ID</th>\n",
       "      <th>school_state_IL</th>\n",
       "      <th>school_state_IN</th>\n",
       "      <th>school_state_KS</th>\n",
       "      <th>school_state_KY</th>\n",
       "      <th>school_state_LA</th>\n",
       "      <th>school_state_MA</th>\n",
       "      <th>school_state_MD</th>\n",
       "      <th>school_state_ME</th>\n",
       "      <th>school_state_MI</th>\n",
       "      <th>school_state_MN</th>\n",
       "      <th>school_state_MO</th>\n",
       "      <th>school_state_MS</th>\n",
       "      <th>school_state_MT</th>\n",
       "      <th>school_state_NC</th>\n",
       "      <th>school_state_ND</th>\n",
       "      <th>school_state_NE</th>\n",
       "      <th>school_state_NH</th>\n",
       "      <th>school_state_NJ</th>\n",
       "      <th>school_state_NM</th>\n",
       "      <th>school_state_NV</th>\n",
       "      <th>school_state_NY</th>\n",
       "      <th>school_state_OH</th>\n",
       "      <th>school_state_OK</th>\n",
       "      <th>school_state_OR</th>\n",
       "      <th>school_state_PA</th>\n",
       "      <th>school_state_RI</th>\n",
       "      <th>school_state_SC</th>\n",
       "      <th>school_state_SD</th>\n",
       "      <th>school_state_TN</th>\n",
       "      <th>school_state_TX</th>\n",
       "      <th>school_state_UT</th>\n",
       "      <th>school_state_VA</th>\n",
       "      <th>school_state_VT</th>\n",
       "      <th>school_state_WA</th>\n",
       "      <th>school_state_WI</th>\n",
       "      <th>school_state_WV</th>\n",
       "      <th>school_state_WY</th>\n",
       "      <th>school_metro_rural</th>\n",
       "      <th>school_metro_suburban</th>\n",
       "      <th>school_metro_urban</th>\n",
       "      <th>teacher_prefix_Mr.</th>\n",
       "      <th>teacher_prefix_Mrs.</th>\n",
       "      <th>teacher_prefix_Ms.</th>\n",
       "      <th>resource_type_Books</th>\n",
       "      <th>resource_type_Other</th>\n",
       "      <th>resource_type_Supplies</th>\n",
       "      <th>resource_type_Technology</th>\n",
       "      <th>resource_type_Trips</th>\n",
       "      <th>resource_type_Visitors</th>\n",
       "      <th>primary_focus_area_Applied Learning</th>\n",
       "      <th>primary_focus_area_Health &amp; Sports</th>\n",
       "      <th>primary_focus_area_History &amp; Civics</th>\n",
       "      <th>primary_focus_area_Literacy &amp; Language</th>\n",
       "      <th>primary_focus_area_Math &amp; Science</th>\n",
       "      <th>primary_focus_area_Music &amp; The Arts</th>\n",
       "      <th>primary_focus_area_Special Needs</th>\n",
       "      <th>secondary_focus_area_Applied Learning</th>\n",
       "      <th>secondary_focus_area_Health &amp; Sports</th>\n",
       "      <th>secondary_focus_area_History &amp; Civics</th>\n",
       "      <th>secondary_focus_area_Literacy &amp; Language</th>\n",
       "      <th>secondary_focus_area_Math &amp; Science</th>\n",
       "      <th>secondary_focus_area_Music &amp; The Arts</th>\n",
       "      <th>secondary_focus_area_Special Needs</th>\n",
       "      <th>poverty_level_high poverty</th>\n",
       "      <th>poverty_level_highest poverty</th>\n",
       "      <th>poverty_level_low poverty</th>\n",
       "      <th>poverty_level_moderate poverty</th>\n",
       "      <th>grade_level_Grades 3-5</th>\n",
       "      <th>grade_level_Grades 6-8</th>\n",
       "      <th>grade_level_Grades 9-12</th>\n",
       "      <th>grade_level_Grades PreK-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.330764</td>\n",
       "      <td>-0.377436</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>-0.205324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.120850</td>\n",
       "      <td>0.188075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315752</td>\n",
       "      <td>-0.365143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.621353</td>\n",
       "      <td>-0.365143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    school_charter  school_magnet  total_price_including_optional_support  \\\n",
       "1                0              0                               -0.330764   \n",
       "2                0              0                                0.440299   \n",
       "16               0              0                               -0.120850   \n",
       "18               0              0                               -0.315752   \n",
       "23               0              0                                1.621353   \n",
       "\n",
       "    students_reached  eligible_double_your_impact_match  not_funded_60_days  \\\n",
       "1          -0.377436                                  1                   0   \n",
       "2          -0.205324                                  0                   1   \n",
       "16          0.188075                                  0                   1   \n",
       "18         -0.365143                                  1                   0   \n",
       "23         -0.365143                                  0                   1   \n",
       "\n",
       "    students_reached_imputed  school_state_AK  school_state_AL  \\\n",
       "1                          0                0                0   \n",
       "2                          0                0                0   \n",
       "16                         0                0                0   \n",
       "18                         0                0                0   \n",
       "23                         0                0                0   \n",
       "\n",
       "    school_state_AR  school_state_AZ  school_state_CA  school_state_CO  \\\n",
       "1                 0                0                1                0   \n",
       "2                 0                0                1                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                1                0   \n",
       "23                0                0                1                0   \n",
       "\n",
       "    school_state_CT  school_state_DC  school_state_DE  school_state_FL  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_GA  school_state_HI  school_state_IA  school_state_ID  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_IL  school_state_IN  school_state_KS  school_state_KY  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                1   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_LA  school_state_MA  school_state_MD  school_state_ME  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_MI  school_state_MN  school_state_MO  school_state_MS  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_MT  school_state_NC  school_state_ND  school_state_NE  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_NH  school_state_NJ  school_state_NM  school_state_NV  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_NY  school_state_OH  school_state_OK  school_state_OR  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_PA  school_state_RI  school_state_SC  school_state_SD  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_TN  school_state_TX  school_state_UT  school_state_VA  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_VT  school_state_WA  school_state_WI  school_state_WV  \\\n",
       "1                 0                0                0                0   \n",
       "2                 0                0                0                0   \n",
       "16                0                0                0                0   \n",
       "18                0                0                0                0   \n",
       "23                0                0                0                0   \n",
       "\n",
       "    school_state_WY  school_metro_rural  school_metro_suburban  \\\n",
       "1                 0                   0                      0   \n",
       "2                 0                   0                      0   \n",
       "16                0                   0                      1   \n",
       "18                0                   0                      0   \n",
       "23                0                   0                      1   \n",
       "\n",
       "    school_metro_urban  teacher_prefix_Mr.  teacher_prefix_Mrs.  \\\n",
       "1                    1                   0                    1   \n",
       "2                    1                   0                    0   \n",
       "16                   0                   0                    0   \n",
       "18                   1                   0                    0   \n",
       "23                   0                   0                    1   \n",
       "\n",
       "    teacher_prefix_Ms.  resource_type_Books  resource_type_Other  \\\n",
       "1                    0                    1                    0   \n",
       "2                    1                    0                    0   \n",
       "16                   1                    0                    0   \n",
       "18                   1                    1                    0   \n",
       "23                   0                    0                    0   \n",
       "\n",
       "    resource_type_Supplies  resource_type_Technology  resource_type_Trips  \\\n",
       "1                        0                         0                    0   \n",
       "2                        0                         1                    0   \n",
       "16                       1                         0                    0   \n",
       "18                       0                         0                    0   \n",
       "23                       0                         1                    0   \n",
       "\n",
       "    resource_type_Visitors  primary_focus_area_Applied Learning  \\\n",
       "1                        0                                    0   \n",
       "2                        0                                    0   \n",
       "16                       0                                    0   \n",
       "18                       0                                    0   \n",
       "23                       0                                    0   \n",
       "\n",
       "    primary_focus_area_Health & Sports  primary_focus_area_History & Civics  \\\n",
       "1                                    0                                    1   \n",
       "2                                    0                                    0   \n",
       "16                                   0                                    0   \n",
       "18                                   0                                    0   \n",
       "23                                   0                                    1   \n",
       "\n",
       "    primary_focus_area_Literacy & Language  primary_focus_area_Math & Science  \\\n",
       "1                                        0                                  0   \n",
       "2                                        1                                  0   \n",
       "16                                       0                                  1   \n",
       "18                                       1                                  0   \n",
       "23                                       0                                  0   \n",
       "\n",
       "    primary_focus_area_Music & The Arts  primary_focus_area_Special Needs  \\\n",
       "1                                     0                                 0   \n",
       "2                                     0                                 0   \n",
       "16                                    0                                 0   \n",
       "18                                    0                                 0   \n",
       "23                                    0                                 0   \n",
       "\n",
       "    secondary_focus_area_Applied Learning  \\\n",
       "1                                       0   \n",
       "2                                       0   \n",
       "16                                      0   \n",
       "18                                      0   \n",
       "23                                      0   \n",
       "\n",
       "    secondary_focus_area_Health & Sports  \\\n",
       "1                                      0   \n",
       "2                                      0   \n",
       "16                                     0   \n",
       "18                                     0   \n",
       "23                                     0   \n",
       "\n",
       "    secondary_focus_area_History & Civics  \\\n",
       "1                                       0   \n",
       "2                                       1   \n",
       "16                                      0   \n",
       "18                                      0   \n",
       "23                                      0   \n",
       "\n",
       "    secondary_focus_area_Literacy & Language  \\\n",
       "1                                          1   \n",
       "2                                          0   \n",
       "16                                         0   \n",
       "18                                         0   \n",
       "23                                         0   \n",
       "\n",
       "    secondary_focus_area_Math & Science  \\\n",
       "1                                     0   \n",
       "2                                     0   \n",
       "16                                    1   \n",
       "18                                    0   \n",
       "23                                    0   \n",
       "\n",
       "    secondary_focus_area_Music & The Arts  secondary_focus_area_Special Needs  \\\n",
       "1                                       0                                   0   \n",
       "2                                       0                                   0   \n",
       "16                                      0                                   0   \n",
       "18                                      0                                   0   \n",
       "23                                      0                                   1   \n",
       "\n",
       "    poverty_level_high poverty  poverty_level_highest poverty  \\\n",
       "1                            0                              1   \n",
       "2                            1                              0   \n",
       "16                           1                              0   \n",
       "18                           0                              1   \n",
       "23                           0                              1   \n",
       "\n",
       "    poverty_level_low poverty  poverty_level_moderate poverty  \\\n",
       "1                           0                               0   \n",
       "2                           0                               0   \n",
       "16                          0                               0   \n",
       "18                          0                               0   \n",
       "23                          0                               0   \n",
       "\n",
       "    grade_level_Grades 3-5  grade_level_Grades 6-8  grade_level_Grades 9-12  \\\n",
       "1                        1                       0                        0   \n",
       "2                        1                       0                        0   \n",
       "16                       0                       1                        0   \n",
       "18                       1                       0                        0   \n",
       "23                       0                       0                        0   \n",
       "\n",
       "    grade_level_Grades PreK-2  \n",
       "1                           0  \n",
       "2                           0  \n",
       "16                          0  \n",
       "18                          0  \n",
       "23                          1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df_list in (train_dfs, test_dfs):\n",
    "    for i in range(len(df_list)):\n",
    "        \n",
    "        # Bin less-frequent cities as \"Other\"\n",
    "#         most_freq_cities = df_list[i].groupby('school_city').size().sort_values(ascending=False).head(10).index.tolist()\n",
    "#         df_list[i]['school_city'] = np.where(df_list[i]['school_city'].isin(most_freq_cities), df_list[i]['school_city'], \"Other\")\n",
    "        \n",
    "        # Make dummy variables for categorical features\n",
    "        for j in categorical_features:\n",
    "            df_list[i] = library.make_dummy_vars(df_list[i], j)\n",
    "\n",
    "train_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26386, 92)\n",
      "(59224, 92)\n",
      "(80809, 92)\n",
      "(32838, 92)\n",
      "(21585, 92)\n",
      "(44167, 93)\n"
     ]
    }
   ],
   "source": [
    "# Final feature set\n",
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "        print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'teacher_prefix_Dr.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_dfs[2].columns.tolist()) - set(test_dfs[1].columns.tolist())\n",
    "# Only the Dr. prefix appears in the last test set (7/1/2013 - 12/31/2013). This might be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if all train and test sets have any remaining missing values\n",
    "for df_list in (train_dfs, test_dfs):\n",
    "    for df in df_list:\n",
    "        print(df.isnull().sum(axis=0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26386 entries, 1 to 124974\n",
      "Data columns (total 92 columns):\n",
      "school_charter                              26386 non-null int64\n",
      "school_magnet                               26386 non-null int64\n",
      "total_price_including_optional_support      26386 non-null float64\n",
      "students_reached                            26386 non-null float64\n",
      "eligible_double_your_impact_match           26386 non-null int64\n",
      "not_funded_60_days                          26386 non-null int64\n",
      "students_reached_imputed                    26386 non-null int64\n",
      "school_state_AK                             26386 non-null int64\n",
      "school_state_AL                             26386 non-null int64\n",
      "school_state_AR                             26386 non-null int64\n",
      "school_state_AZ                             26386 non-null int64\n",
      "school_state_CA                             26386 non-null int64\n",
      "school_state_CO                             26386 non-null int64\n",
      "school_state_CT                             26386 non-null int64\n",
      "school_state_DC                             26386 non-null int64\n",
      "school_state_DE                             26386 non-null int64\n",
      "school_state_FL                             26386 non-null int64\n",
      "school_state_GA                             26386 non-null int64\n",
      "school_state_HI                             26386 non-null int64\n",
      "school_state_IA                             26386 non-null int64\n",
      "school_state_ID                             26386 non-null int64\n",
      "school_state_IL                             26386 non-null int64\n",
      "school_state_IN                             26386 non-null int64\n",
      "school_state_KS                             26386 non-null int64\n",
      "school_state_KY                             26386 non-null int64\n",
      "school_state_LA                             26386 non-null int64\n",
      "school_state_MA                             26386 non-null int64\n",
      "school_state_MD                             26386 non-null int64\n",
      "school_state_ME                             26386 non-null int64\n",
      "school_state_MI                             26386 non-null int64\n",
      "school_state_MN                             26386 non-null int64\n",
      "school_state_MO                             26386 non-null int64\n",
      "school_state_MS                             26386 non-null int64\n",
      "school_state_MT                             26386 non-null int64\n",
      "school_state_NC                             26386 non-null int64\n",
      "school_state_ND                             26386 non-null int64\n",
      "school_state_NE                             26386 non-null int64\n",
      "school_state_NH                             26386 non-null int64\n",
      "school_state_NJ                             26386 non-null int64\n",
      "school_state_NM                             26386 non-null int64\n",
      "school_state_NV                             26386 non-null int64\n",
      "school_state_NY                             26386 non-null int64\n",
      "school_state_OH                             26386 non-null int64\n",
      "school_state_OK                             26386 non-null int64\n",
      "school_state_OR                             26386 non-null int64\n",
      "school_state_PA                             26386 non-null int64\n",
      "school_state_RI                             26386 non-null int64\n",
      "school_state_SC                             26386 non-null int64\n",
      "school_state_SD                             26386 non-null int64\n",
      "school_state_TN                             26386 non-null int64\n",
      "school_state_TX                             26386 non-null int64\n",
      "school_state_UT                             26386 non-null int64\n",
      "school_state_VA                             26386 non-null int64\n",
      "school_state_VT                             26386 non-null int64\n",
      "school_state_WA                             26386 non-null int64\n",
      "school_state_WI                             26386 non-null int64\n",
      "school_state_WV                             26386 non-null int64\n",
      "school_state_WY                             26386 non-null int64\n",
      "school_metro_rural                          26386 non-null int64\n",
      "school_metro_suburban                       26386 non-null int64\n",
      "school_metro_urban                          26386 non-null int64\n",
      "teacher_prefix_Mr.                          26386 non-null int64\n",
      "teacher_prefix_Mrs.                         26386 non-null int64\n",
      "teacher_prefix_Ms.                          26386 non-null int64\n",
      "resource_type_Books                         26386 non-null int64\n",
      "resource_type_Other                         26386 non-null int64\n",
      "resource_type_Supplies                      26386 non-null int64\n",
      "resource_type_Technology                    26386 non-null int64\n",
      "resource_type_Trips                         26386 non-null int64\n",
      "resource_type_Visitors                      26386 non-null int64\n",
      "primary_focus_area_Applied Learning         26386 non-null int64\n",
      "primary_focus_area_Health & Sports          26386 non-null int64\n",
      "primary_focus_area_History & Civics         26386 non-null int64\n",
      "primary_focus_area_Literacy & Language      26386 non-null int64\n",
      "primary_focus_area_Math & Science           26386 non-null int64\n",
      "primary_focus_area_Music & The Arts         26386 non-null int64\n",
      "primary_focus_area_Special Needs            26386 non-null int64\n",
      "secondary_focus_area_Applied Learning       26386 non-null int64\n",
      "secondary_focus_area_Health & Sports        26386 non-null int64\n",
      "secondary_focus_area_History & Civics       26386 non-null int64\n",
      "secondary_focus_area_Literacy & Language    26386 non-null int64\n",
      "secondary_focus_area_Math & Science         26386 non-null int64\n",
      "secondary_focus_area_Music & The Arts       26386 non-null int64\n",
      "secondary_focus_area_Special Needs          26386 non-null int64\n",
      "poverty_level_high poverty                  26386 non-null int64\n",
      "poverty_level_highest poverty               26386 non-null int64\n",
      "poverty_level_low poverty                   26386 non-null int64\n",
      "poverty_level_moderate poverty              26386 non-null int64\n",
      "grade_level_Grades 3-5                      26386 non-null int64\n",
      "grade_level_Grades 6-8                      26386 non-null int64\n",
      "grade_level_Grades 9-12                     26386 non-null int64\n",
      "grade_level_Grades PreK-2                   26386 non-null int64\n",
      "dtypes: float64(2), int64(90)\n",
      "memory usage: 18.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Final feature list\n",
    "train_dfs[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNeighborsClassifier with params {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'auto'} on training set 0\n",
      "Training KNeighborsClassifier with params {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'auto'} on training set 1\n",
      "Training KNeighborsClassifier with params {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'auto'} on training set 2\n",
      "Training DecisionTreeClassifier with params {'max_depth': 1, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'random_state': 0} on training set 0\n",
      "Training DecisionTreeClassifier with params {'max_depth': 1, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'random_state': 0} on training set 1\n",
      "Training DecisionTreeClassifier with params {'max_depth': 1, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'random_state': 0} on training set 2\n",
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC with params {'penalty': 'l2', 'C': 1, 'random_state': 0} on training set 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier with params {'n_estimators': 10, 'max_depth': 1, 'max_features': 10, 'min_samples_leaf': 10, 'random_state': 0} on training set 0\n",
      "Training RandomForestClassifier with params {'n_estimators': 10, 'max_depth': 1, 'max_features': 10, 'min_samples_leaf': 10, 'random_state': 0} on training set 1\n",
      "Training RandomForestClassifier with params {'n_estimators': 10, 'max_depth': 1, 'max_features': 10, 'min_samples_leaf': 10, 'random_state': 0} on training set 2\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'algorithm': 'SAMME.R', 'random_state': 0} on training set 0\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'algorithm': 'SAMME.R', 'random_state': 0} on training set 1\n",
      "Training AdaBoostClassifier with params {'n_estimators': 10, 'algorithm': 'SAMME.R', 'random_state': 0} on training set 2\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0} on training set 0\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0} on training set 1\n",
      "Training BaggingClassifier with params {'n_estimators': 10, 'random_state': 0} on training set 2\n"
     ]
    }
   ],
   "source": [
    "# TODO - WHY DOES KNN BREAK PREDICT_PROBA?\n",
    "\n",
    "classifiers = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier',\n",
    "               'LinearSVC', 'RandomForestClassifier', 'AdaBoostClassifier', \n",
    "               'BaggingClassifier']\n",
    "parameters = config.GRID_TEST # dictionary of lists of parameters \n",
    "num_training_sets = len(config.TEMPORAL_SPLITS) # use to index into train_dfs\n",
    "label = config.LABEL\n",
    "thresholds = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5, 1]\n",
    "trained_classifiers = []\n",
    "\n",
    "for i in classifiers:\n",
    "    for j in parameters[i]:\n",
    "        for k in range(num_training_sets):\n",
    "            \n",
    "            print(f'Training {i} with params {j} on training set {k}')\n",
    "            trained = library.train_classifier(df=train_dfs[k],\n",
    "                                               label=label, \n",
    "                                               method=i,\n",
    "                                               param_dict=j)\n",
    "            \n",
    "            trained_classifiers.append(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                      weights='uniform'),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                      weights='uniform')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_classifiers[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision_0.01</th>\n",
       "      <th>precision_0.02</th>\n",
       "      <th>precision_0.05</th>\n",
       "      <th>precision_0.1</th>\n",
       "      <th>precision_0.2</th>\n",
       "      <th>precision_0.3</th>\n",
       "      <th>precision_0.5</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0.01</th>\n",
       "      <th>recall_0.02</th>\n",
       "      <th>recall_0.05</th>\n",
       "      <th>recall_0.1</th>\n",
       "      <th>recall_0.2</th>\n",
       "      <th>recall_0.3</th>\n",
       "      <th>recall_0.5</th>\n",
       "      <th>recall_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>test-start</th>\n",
       "      <th>test-end</th>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">LogisticRegression</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.714497</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.831797</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.879882</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842970</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.719263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.713952</td>\n",
       "      <td>0.655241</td>\n",
       "      <td>0.827432</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.728235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.709116</td>\n",
       "      <td>0.655238</td>\n",
       "      <td>0.819116</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.879882</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842970</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.738180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.655241</td>\n",
       "      <td>0.800381</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.753401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.609581</td>\n",
       "      <td>0.655239</td>\n",
       "      <td>0.687505</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.866061</td>\n",
       "      <td>0.886467</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.858894</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.811734</td>\n",
       "      <td>0.805073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.599899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.685542</td>\n",
       "      <td>0.647781</td>\n",
       "      <td>0.812406</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.883379</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834214</td>\n",
       "      <td>0.813993</td>\n",
       "      <td>0.775879</td>\n",
       "      <td>0.686294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.689814</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.812888</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.692028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.692707</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.810716</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.700582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.687379</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.799186</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.712857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.618582</td>\n",
       "      <td>0.647798</td>\n",
       "      <td>0.697130</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.867249</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.775696</td>\n",
       "      <td>0.763114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.741052</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.849416</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.747961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.729151</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.837436</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817161</td>\n",
       "      <td>0.755894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.711971</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.820562</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.764019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.685179</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.778355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.574654</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>0.658282</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.892007</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.861153</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.816865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">DecisionTreeClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.715813</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.834240</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.716173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.715314</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.833495</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.716939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.714746</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.832926</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.717184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.698876</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.813268</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.731282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.595482</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.686348</td>\n",
       "      <td>0.847727</td>\n",
       "      <td>0.827469</td>\n",
       "      <td>0.870572</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>0.843456</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.779629</td>\n",
       "      <td>0.771337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.618229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.812270</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.684324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.683797</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.811777</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.684730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.684808</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.810618</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.688188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.681547</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.804632</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.693290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.599385</td>\n",
       "      <td>0.610808</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.822273</td>\n",
       "      <td>0.787048</td>\n",
       "      <td>0.751630</td>\n",
       "      <td>0.737991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.851938</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.743474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.731500</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.843303</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.744607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.727228</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.839955</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.744714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.693754</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.791834</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.486070</td>\n",
       "      <td>0.592765</td>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.796947</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.873360</td>\n",
       "      <td>0.852151</td>\n",
       "      <td>0.819449</td>\n",
       "      <td>0.807986</td>\n",
       "      <td>0.807986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759538</td>\n",
       "      <td>0.404640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">AdaBoostClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.715904</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.834434</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.904529</td>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.715904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.714451</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.829282</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.904529</td>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.724910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.353275</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.186306</td>\n",
       "      <td>0.988636</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.953224</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>0.938417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796143</td>\n",
       "      <td>0.409262</td>\n",
       "      <td>0.283787</td>\n",
       "      <td>0.181681</td>\n",
       "      <td>0.103419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.284096</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.284096</td>\n",
       "      <td>0.663427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.684119</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.812435</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.797649</td>\n",
       "      <td>0.684119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.685634</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.809443</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.797649</td>\n",
       "      <td>0.691463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.400845</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.242569</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>0.897337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.549737</td>\n",
       "      <td>0.379335</td>\n",
       "      <td>0.240557</td>\n",
       "      <td>0.140239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.315881</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.315881</td>\n",
       "      <td>0.666147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.743188</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.852677</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.925847</td>\n",
       "      <td>0.898383</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>0.828207</td>\n",
       "      <td>0.743188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.723536</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.833523</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.925847</td>\n",
       "      <td>0.898383</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>0.828207</td>\n",
       "      <td>0.754349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.326508</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.181980</td>\n",
       "      <td>0.966463</td>\n",
       "      <td>0.945038</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809163</td>\n",
       "      <td>0.416950</td>\n",
       "      <td>0.286699</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.100801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.256934</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.256812</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">BaggingClassifier</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-02</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-12-31</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.706527</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.820467</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.729897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.694812</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.804058</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.743998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.665342</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.768341</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.761590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.665206</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.768181</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.761643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.615416</td>\n",
       "      <td>0.636271</td>\n",
       "      <td>0.704489</td>\n",
       "      <td>0.884091</td>\n",
       "      <td>0.874007</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.853105</td>\n",
       "      <td>0.833132</td>\n",
       "      <td>0.794887</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-07-01</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.683889</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.804854</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.696638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.674199</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.790317</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.706010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.655461</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.762640</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.721245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.655461</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.762565</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.721377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.808741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.617663</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.810792</td>\n",
       "      <td>0.795622</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.740608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-07-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2012-12-30</th>\n",
       "      <th>0.40</th>\n",
       "      <td>0.721308</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.830905</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.756643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.697263</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.768649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.761806</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.655061</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.761770</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.594397</td>\n",
       "      <td>0.614359</td>\n",
       "      <td>0.689686</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.867562</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.834401</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.799340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        accuracy   auc-roc  \\\n",
       "classifier             test-start test-end   threshold                       \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40       0.714497  0.655238   \n",
       "                                             0.50       0.713952  0.655241   \n",
       "                                             0.55       0.709116  0.655238   \n",
       "                                             0.60       0.695175  0.655241   \n",
       "                                             0.70       0.609581  0.655239   \n",
       "                       2012-12-31 2013-07-01 0.40       0.685542  0.647781   \n",
       "                                             0.50       0.689814  0.647798   \n",
       "                                             0.55       0.692707  0.647798   \n",
       "                                             0.60       0.687379  0.647798   \n",
       "                                             0.70       0.618582  0.647798   \n",
       "                       2012-07-01 2012-12-30 0.40       0.741052  0.630703   \n",
       "                                             0.50       0.729151  0.630703   \n",
       "                                             0.55       0.711971  0.630703   \n",
       "                                             0.60       0.685179  0.630703   \n",
       "                                             0.70       0.574654  0.630703   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40       0.715813  0.612613   \n",
       "                                             0.50       0.715314  0.612613   \n",
       "                                             0.55       0.714746  0.612613   \n",
       "                                             0.60       0.698876  0.612613   \n",
       "                                             0.70       0.595482  0.612613   \n",
       "                       2012-12-31 2013-07-01 0.40       0.684073  0.610808   \n",
       "                                             0.50       0.683797  0.610808   \n",
       "                                             0.55       0.684808  0.610808   \n",
       "                                             0.60       0.681547  0.610808   \n",
       "                                             0.70       0.599385  0.610808   \n",
       "                       2012-07-01 2012-12-30 0.40       0.742333  0.592765   \n",
       "                                             0.50       0.731500  0.592765   \n",
       "                                             0.55       0.727228  0.592765   \n",
       "                                             0.60       0.693754  0.592765   \n",
       "                                             0.70       0.486070  0.592765   \n",
       "...                                                          ...       ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40       0.715904  0.663427   \n",
       "                                             0.50       0.714451  0.663427   \n",
       "                                             0.55       0.353275  0.663427   \n",
       "                                             0.60       0.284096  0.663427   \n",
       "                                             0.70       0.284096  0.663427   \n",
       "                       2012-12-31 2013-07-01 0.40       0.684119  0.666147   \n",
       "                                             0.50       0.685634  0.666147   \n",
       "                                             0.55       0.400845  0.666147   \n",
       "                                             0.60       0.315881  0.666147   \n",
       "                                             0.70       0.315881  0.666147   \n",
       "                       2012-07-01 2012-12-30 0.40       0.743188  0.648891   \n",
       "                                             0.50       0.723536  0.648891   \n",
       "                                             0.55       0.326508  0.648891   \n",
       "                                             0.60       0.256934  0.648891   \n",
       "                                             0.70       0.256812  0.648891   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40       0.706527  0.636271   \n",
       "                                             0.50       0.694812  0.636271   \n",
       "                                             0.55       0.665342  0.636271   \n",
       "                                             0.60       0.665206  0.636271   \n",
       "                                             0.70       0.615416  0.636271   \n",
       "                       2012-12-31 2013-07-01 0.40       0.683889  0.621075   \n",
       "                                             0.50       0.674199  0.621075   \n",
       "                                             0.55       0.655461  0.621075   \n",
       "                                             0.60       0.655461  0.621075   \n",
       "                                             0.70       0.617663  0.621075   \n",
       "                       2012-07-01 2012-12-30 0.40       0.721308  0.614359   \n",
       "                                             0.50       0.697263  0.614359   \n",
       "                                             0.55       0.655091  0.614359   \n",
       "                                             0.60       0.655061  0.614359   \n",
       "                                             0.70       0.594397  0.614359   \n",
       "\n",
       "                                                              f1  \\\n",
       "classifier             test-start test-end   threshold             \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40       0.831797   \n",
       "                                             0.50       0.827432   \n",
       "                                             0.55       0.819116   \n",
       "                                             0.60       0.800381   \n",
       "                                             0.70       0.687505   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812406   \n",
       "                                             0.50       0.812888   \n",
       "                                             0.55       0.810716   \n",
       "                                             0.60       0.799186   \n",
       "                                             0.70       0.697130   \n",
       "                       2012-07-01 2012-12-30 0.40       0.849416   \n",
       "                                             0.50       0.837436   \n",
       "                                             0.55       0.820562   \n",
       "                                             0.60       0.791874   \n",
       "                                             0.70       0.658282   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40       0.834240   \n",
       "                                             0.50       0.833495   \n",
       "                                             0.55       0.832926   \n",
       "                                             0.60       0.813268   \n",
       "                                             0.70       0.686348   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812270   \n",
       "                                             0.50       0.811777   \n",
       "                                             0.55       0.810618   \n",
       "                                             0.60       0.804632   \n",
       "                                             0.70       0.686955   \n",
       "                       2012-07-01 2012-12-30 0.40       0.851938   \n",
       "                                             0.50       0.843303   \n",
       "                                             0.55       0.839955   \n",
       "                                             0.60       0.807230   \n",
       "                                             0.70       0.539232   \n",
       "...                                                          ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40       0.834434   \n",
       "                                             0.50       0.829282   \n",
       "                                             0.55       0.186306   \n",
       "                                             0.60       0.000000   \n",
       "                                             0.70       0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40       0.812435   \n",
       "                                             0.50       0.809443   \n",
       "                                             0.55       0.242569   \n",
       "                                             0.60       0.000000   \n",
       "                                             0.70       0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40       0.852677   \n",
       "                                             0.50       0.833523   \n",
       "                                             0.55       0.181980   \n",
       "                                             0.60       0.000328   \n",
       "                                             0.70       0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40       0.820467   \n",
       "                                             0.50       0.804058   \n",
       "                                             0.55       0.768341   \n",
       "                                             0.60       0.768181   \n",
       "                                             0.70       0.704489   \n",
       "                       2012-12-31 2013-07-01 0.40       0.804854   \n",
       "                                             0.50       0.790317   \n",
       "                                             0.55       0.762640   \n",
       "                                             0.60       0.762565   \n",
       "                                             0.70       0.708417   \n",
       "                       2012-07-01 2012-12-30 0.40       0.830905   \n",
       "                                             0.50       0.806302   \n",
       "                                             0.55       0.761806   \n",
       "                                             0.60       0.761770   \n",
       "                                             0.70       0.689686   \n",
       "\n",
       "                                                        precision_0.01  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.854545   \n",
       "                                             0.50             0.854545   \n",
       "                                             0.55             0.854545   \n",
       "                                             0.60             0.854545   \n",
       "                                             0.70             0.854545   \n",
       "                       2012-12-31 2013-07-01 0.40             0.899083   \n",
       "                                             0.50             0.899083   \n",
       "                                             0.55             0.899083   \n",
       "                                             0.60             0.899083   \n",
       "                                             0.70             0.899083   \n",
       "                       2012-07-01 2012-12-30 0.40             0.908537   \n",
       "                                             0.50             0.908537   \n",
       "                                             0.55             0.908537   \n",
       "                                             0.60             0.908537   \n",
       "                                             0.70             0.908537   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.847727   \n",
       "                                             0.50             0.847727   \n",
       "                                             0.55             0.847727   \n",
       "                                             0.60             0.847727   \n",
       "                                             0.70             0.847727   \n",
       "                       2012-12-31 2013-07-01 0.40             0.876147   \n",
       "                                             0.50             0.876147   \n",
       "                                             0.55             0.876147   \n",
       "                                             0.60             0.876147   \n",
       "                                             0.70             0.876147   \n",
       "                       2012-07-01 2012-12-30 0.40             0.820122   \n",
       "                                             0.50             0.820122   \n",
       "                                             0.55             0.820122   \n",
       "                                             0.60             0.820122   \n",
       "                                             0.70             0.820122   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.988636   \n",
       "                                             0.50             0.988636   \n",
       "                                             0.55             0.988636   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.972477   \n",
       "                                             0.50             0.972477   \n",
       "                                             0.55             0.972477   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.966463   \n",
       "                                             0.50             0.966463   \n",
       "                                             0.55             0.966463   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.884091   \n",
       "                                             0.50             0.884091   \n",
       "                                             0.55             0.884091   \n",
       "                                             0.60             0.884091   \n",
       "                                             0.70             0.884091   \n",
       "                       2012-12-31 2013-07-01 0.40             0.830275   \n",
       "                                             0.50             0.830275   \n",
       "                                             0.55             0.830275   \n",
       "                                             0.60             0.830275   \n",
       "                                             0.70             0.830275   \n",
       "                       2012-07-01 2012-12-30 0.40             0.868902   \n",
       "                                             0.50             0.868902   \n",
       "                                             0.55             0.868902   \n",
       "                                             0.60             0.868902   \n",
       "                                             0.70             0.868902   \n",
       "\n",
       "                                                        precision_0.02  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.866061   \n",
       "                                             0.50             0.866061   \n",
       "                                             0.55             0.866061   \n",
       "                                             0.60             0.866061   \n",
       "                                             0.70             0.866061   \n",
       "                       2012-12-31 2013-07-01 0.40             0.917241   \n",
       "                                             0.50             0.917241   \n",
       "                                             0.55             0.917241   \n",
       "                                             0.60             0.917241   \n",
       "                                             0.70             0.917241   \n",
       "                       2012-07-01 2012-12-30 0.40             0.908397   \n",
       "                                             0.50             0.908397   \n",
       "                                             0.55             0.908397   \n",
       "                                             0.60             0.908397   \n",
       "                                             0.70             0.908397   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.827469   \n",
       "                                             0.50             0.827469   \n",
       "                                             0.55             0.827469   \n",
       "                                             0.60             0.827469   \n",
       "                                             0.70             0.827469   \n",
       "                       2012-12-31 2013-07-01 0.40             0.882759   \n",
       "                                             0.50             0.882759   \n",
       "                                             0.55             0.882759   \n",
       "                                             0.60             0.882759   \n",
       "                                             0.70             0.882759   \n",
       "                       2012-07-01 2012-12-30 0.40             0.796947   \n",
       "                                             0.50             0.796947   \n",
       "                                             0.55             0.796947   \n",
       "                                             0.60             0.796947   \n",
       "                                             0.70             0.796947   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.968218   \n",
       "                                             0.50             0.968218   \n",
       "                                             0.55             0.968218   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.917241   \n",
       "                                             0.50             0.917241   \n",
       "                                             0.55             0.917241   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.945038   \n",
       "                                             0.50             0.945038   \n",
       "                                             0.55             0.945038   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.874007   \n",
       "                                             0.50             0.874007   \n",
       "                                             0.55             0.874007   \n",
       "                                             0.60             0.874007   \n",
       "                                             0.70             0.874007   \n",
       "                       2012-12-31 2013-07-01 0.40             0.818391   \n",
       "                                             0.50             0.818391   \n",
       "                                             0.55             0.818391   \n",
       "                                             0.60             0.818391   \n",
       "                                             0.70             0.818391   \n",
       "                       2012-07-01 2012-12-30 0.40             0.870229   \n",
       "                                             0.50             0.870229   \n",
       "                                             0.55             0.870229   \n",
       "                                             0.60             0.870229   \n",
       "                                             0.70             0.870229   \n",
       "\n",
       "                                                        precision_0.05  \\\n",
       "classifier             test-start test-end   threshold                   \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40             0.886467   \n",
       "                                             0.50             0.886467   \n",
       "                                             0.55             0.886467   \n",
       "                                             0.60             0.886467   \n",
       "                                             0.70             0.886467   \n",
       "                       2012-12-31 2013-07-01 0.40             0.883379   \n",
       "                                             0.50             0.884298   \n",
       "                                             0.55             0.884298   \n",
       "                                             0.60             0.884298   \n",
       "                                             0.70             0.884298   \n",
       "                       2012-07-01 2012-12-30 0.40             0.892007   \n",
       "                                             0.50             0.892007   \n",
       "                                             0.55             0.892007   \n",
       "                                             0.60             0.892007   \n",
       "                                             0.70             0.892007   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40             0.870572   \n",
       "                                             0.50             0.870572   \n",
       "                                             0.55             0.870572   \n",
       "                                             0.60             0.870572   \n",
       "                                             0.70             0.870572   \n",
       "                       2012-12-31 2013-07-01 0.40             0.876033   \n",
       "                                             0.50             0.876033   \n",
       "                                             0.55             0.876033   \n",
       "                                             0.60             0.876033   \n",
       "                                             0.70             0.876033   \n",
       "                       2012-07-01 2012-12-30 0.40             0.855400   \n",
       "                                             0.50             0.855400   \n",
       "                                             0.55             0.855400   \n",
       "                                             0.60             0.855400   \n",
       "                                             0.70             0.855400   \n",
       "...                                                                ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40             0.953224   \n",
       "                                             0.50             0.953224   \n",
       "                                             0.55             0.953224   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40             0.909091   \n",
       "                                             0.50             0.909091   \n",
       "                                             0.55             0.909091   \n",
       "                                             0.60             0.000000   \n",
       "                                             0.70             0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40             0.942038   \n",
       "                                             0.50             0.942038   \n",
       "                                             0.55             0.942038   \n",
       "                                             0.60             1.000000   \n",
       "                                             0.70             0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40             0.880109   \n",
       "                                             0.50             0.880109   \n",
       "                                             0.55             0.880109   \n",
       "                                             0.60             0.880109   \n",
       "                                             0.70             0.880109   \n",
       "                       2012-12-31 2013-07-01 0.40             0.837466   \n",
       "                                             0.50             0.837466   \n",
       "                                             0.55             0.837466   \n",
       "                                             0.60             0.837466   \n",
       "                                             0.70             0.837466   \n",
       "                       2012-07-01 2012-12-30 0.40             0.871263   \n",
       "                                             0.50             0.871263   \n",
       "                                             0.55             0.871263   \n",
       "                                             0.60             0.871263   \n",
       "                                             0.70             0.871263   \n",
       "\n",
       "                                                        precision_0.1  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.879882   \n",
       "                                             0.50            0.880109   \n",
       "                                             0.55            0.879882   \n",
       "                                             0.60            0.880109   \n",
       "                                             0.70            0.880109   \n",
       "                       2012-12-31 2013-07-01 0.40            0.867249   \n",
       "                                             0.50            0.867249   \n",
       "                                             0.55            0.867249   \n",
       "                                             0.60            0.867249   \n",
       "                                             0.70            0.867249   \n",
       "                       2012-07-01 2012-12-30 0.40            0.878853   \n",
       "                                             0.50            0.878853   \n",
       "                                             0.55            0.878853   \n",
       "                                             0.60            0.878853   \n",
       "                                             0.70            0.878853   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.875114   \n",
       "                                             0.50            0.875114   \n",
       "                                             0.55            0.875114   \n",
       "                                             0.60            0.875114   \n",
       "                                             0.70            0.875114   \n",
       "                       2012-12-31 2013-07-01 0.40            0.861736   \n",
       "                                             0.50            0.861736   \n",
       "                                             0.55            0.861736   \n",
       "                                             0.60            0.861736   \n",
       "                                             0.70            0.861736   \n",
       "                       2012-07-01 2012-12-30 0.40            0.873360   \n",
       "                                             0.50            0.873360   \n",
       "                                             0.55            0.873360   \n",
       "                                             0.60            0.873360   \n",
       "                                             0.70            0.873360   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.930064   \n",
       "                                             0.50            0.930064   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.899862   \n",
       "                                             0.50            0.899862   \n",
       "                                             0.55            0.899862   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.925847   \n",
       "                                             0.50            0.925847   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.878974   \n",
       "                                             0.50            0.878974   \n",
       "                                             0.55            0.878974   \n",
       "                                             0.60            0.878974   \n",
       "                                             0.70            0.878974   \n",
       "                       2012-12-31 2013-07-01 0.40            0.843822   \n",
       "                                             0.50            0.843822   \n",
       "                                             0.55            0.843822   \n",
       "                                             0.60            0.843822   \n",
       "                                             0.70            0.843822   \n",
       "                       2012-07-01 2012-12-30 0.40            0.867562   \n",
       "                                             0.50            0.867562   \n",
       "                                             0.55            0.867562   \n",
       "                                             0.60            0.867562   \n",
       "                                             0.70            0.867562   \n",
       "\n",
       "                                                        precision_0.2  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.858894   \n",
       "                                             0.50            0.858894   \n",
       "                                             0.55            0.858894   \n",
       "                                             0.60            0.858894   \n",
       "                                             0.70            0.858894   \n",
       "                       2012-12-31 2013-07-01 0.40            0.834214   \n",
       "                                             0.50            0.834443   \n",
       "                                             0.55            0.834443   \n",
       "                                             0.60            0.834443   \n",
       "                                             0.70            0.834443   \n",
       "                       2012-07-01 2012-12-30 0.40            0.861153   \n",
       "                                             0.50            0.861153   \n",
       "                                             0.55            0.861153   \n",
       "                                             0.60            0.861153   \n",
       "                                             0.70            0.861153   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.843456   \n",
       "                                             0.50            0.843456   \n",
       "                                             0.55            0.843456   \n",
       "                                             0.60            0.843456   \n",
       "                                             0.70            0.843456   \n",
       "                       2012-12-31 2013-07-01 0.40            0.822273   \n",
       "                                             0.50            0.822273   \n",
       "                                             0.55            0.822273   \n",
       "                                             0.60            0.822273   \n",
       "                                             0.70            0.822273   \n",
       "                       2012-07-01 2012-12-30 0.40            0.852151   \n",
       "                                             0.50            0.852151   \n",
       "                                             0.55            0.852151   \n",
       "                                             0.60            0.852151   \n",
       "                                             0.70            0.852151   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.904529   \n",
       "                                             0.50            0.904529   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.872560   \n",
       "                                             0.50            0.872560   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.898383   \n",
       "                                             0.50            0.898383   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.853105   \n",
       "                                             0.50            0.853105   \n",
       "                                             0.55            0.853105   \n",
       "                                             0.60            0.853105   \n",
       "                                             0.70            0.853105   \n",
       "                       2012-12-31 2013-07-01 0.40            0.810792   \n",
       "                                             0.50            0.810792   \n",
       "                                             0.55            0.810792   \n",
       "                                             0.60            0.810792   \n",
       "                                             0.70            0.810792   \n",
       "                       2012-07-01 2012-12-30 0.40            0.849710   \n",
       "                                             0.50            0.849710   \n",
       "                                             0.55            0.849710   \n",
       "                                             0.60            0.849710   \n",
       "                                             0.70            0.849710   \n",
       "\n",
       "                                                        precision_0.3  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.842970   \n",
       "                                             0.50            0.842894   \n",
       "                                             0.55            0.842970   \n",
       "                                             0.60            0.842894   \n",
       "                                             0.70            0.842894   \n",
       "                       2012-12-31 2013-07-01 0.40            0.813993   \n",
       "                                             0.50            0.814299   \n",
       "                                             0.55            0.814299   \n",
       "                                             0.60            0.814299   \n",
       "                                             0.70            0.814299   \n",
       "                       2012-07-01 2012-12-30 0.40            0.846506   \n",
       "                                             0.50            0.846506   \n",
       "                                             0.55            0.846506   \n",
       "                                             0.60            0.846506   \n",
       "                                             0.70            0.846506   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.815877   \n",
       "                                             0.50            0.815877   \n",
       "                                             0.55            0.815877   \n",
       "                                             0.60            0.815877   \n",
       "                                             0.70            0.815877   \n",
       "                       2012-12-31 2013-07-01 0.40            0.787048   \n",
       "                                             0.50            0.787048   \n",
       "                                             0.55            0.787048   \n",
       "                                             0.60            0.787048   \n",
       "                                             0.70            0.787048   \n",
       "                       2012-07-01 2012-12-30 0.40            0.819449   \n",
       "                                             0.50            0.819449   \n",
       "                                             0.55            0.819449   \n",
       "                                             0.60            0.819449   \n",
       "                                             0.70            0.819449   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.869608   \n",
       "                                             0.50            0.869608   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.843080   \n",
       "                                             0.50            0.843080   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.871020   \n",
       "                                             0.50            0.871020   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.833132   \n",
       "                                             0.50            0.833132   \n",
       "                                             0.55            0.833132   \n",
       "                                             0.60            0.833132   \n",
       "                                             0.70            0.833132   \n",
       "                       2012-12-31 2013-07-01 0.40            0.795622   \n",
       "                                             0.50            0.795622   \n",
       "                                             0.55            0.795622   \n",
       "                                             0.60            0.795622   \n",
       "                                             0.70            0.795622   \n",
       "                       2012-07-01 2012-12-30 0.40            0.834401   \n",
       "                                             0.50            0.834401   \n",
       "                                             0.55            0.834401   \n",
       "                                             0.60            0.834401   \n",
       "                                             0.70            0.834401   \n",
       "\n",
       "                                                        precision_0.5  \\\n",
       "classifier             test-start test-end   threshold                  \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40            0.811779   \n",
       "                                             0.50            0.811779   \n",
       "                                             0.55            0.811779   \n",
       "                                             0.60            0.811779   \n",
       "                                             0.70            0.811734   \n",
       "                       2012-12-31 2013-07-01 0.40            0.775879   \n",
       "                                             0.50            0.775696   \n",
       "                                             0.55            0.775696   \n",
       "                                             0.60            0.775696   \n",
       "                                             0.70            0.775696   \n",
       "                       2012-07-01 2012-12-30 0.40            0.817222   \n",
       "                                             0.50            0.817161   \n",
       "                                             0.55            0.817222   \n",
       "                                             0.60            0.817222   \n",
       "                                             0.70            0.817222   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40            0.779629   \n",
       "                                             0.50            0.779629   \n",
       "                                             0.55            0.779629   \n",
       "                                             0.60            0.779629   \n",
       "                                             0.70            0.779629   \n",
       "                       2012-12-31 2013-07-01 0.40            0.751630   \n",
       "                                             0.50            0.751630   \n",
       "                                             0.55            0.751630   \n",
       "                                             0.60            0.751630   \n",
       "                                             0.70            0.751630   \n",
       "                       2012-07-01 2012-12-30 0.40            0.791834   \n",
       "                                             0.50            0.791834   \n",
       "                                             0.55            0.791834   \n",
       "                                             0.60            0.791834   \n",
       "                                             0.70            0.807986   \n",
       "...                                                               ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40            0.815049   \n",
       "                                             0.50            0.815049   \n",
       "                                             0.55            0.938417   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40            0.797649   \n",
       "                                             0.50            0.797649   \n",
       "                                             0.55            0.897337   \n",
       "                                             0.60            0.000000   \n",
       "                                             0.70            0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40            0.828207   \n",
       "                                             0.50            0.828207   \n",
       "                                             0.55            0.934882   \n",
       "                                             0.60            1.000000   \n",
       "                                             0.70            0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40            0.794887   \n",
       "                                             0.50            0.794887   \n",
       "                                             0.55            0.794887   \n",
       "                                             0.60            0.794887   \n",
       "                                             0.70            0.794887   \n",
       "                       2012-12-31 2013-07-01 0.40            0.759897   \n",
       "                                             0.50            0.759897   \n",
       "                                             0.55            0.759897   \n",
       "                                             0.60            0.759897   \n",
       "                                             0.70            0.759897   \n",
       "                       2012-07-01 2012-12-30 0.40            0.805505   \n",
       "                                             0.50            0.805505   \n",
       "                                             0.55            0.805505   \n",
       "                                             0.60            0.805505   \n",
       "                                             0.70            0.805505   \n",
       "\n",
       "                                                        precision_1  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          0.719263   \n",
       "                                             0.50          0.728235   \n",
       "                                             0.55          0.738180   \n",
       "                                             0.60          0.753401   \n",
       "                                             0.70          0.805073   \n",
       "                       2012-12-31 2013-07-01 0.40          0.686294   \n",
       "                                             0.50          0.692028   \n",
       "                                             0.55          0.700582   \n",
       "                                             0.60          0.712857   \n",
       "                                             0.70          0.763114   \n",
       "                       2012-07-01 2012-12-30 0.40          0.747961   \n",
       "                                             0.50          0.755894   \n",
       "                                             0.55          0.764019   \n",
       "                                             0.60          0.778355   \n",
       "                                             0.70          0.816865   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          0.716173   \n",
       "                                             0.50          0.716939   \n",
       "                                             0.55          0.717184   \n",
       "                                             0.60          0.731282   \n",
       "                                             0.70          0.771337   \n",
       "                       2012-12-31 2013-07-01 0.40          0.684324   \n",
       "                                             0.50          0.684730   \n",
       "                                             0.55          0.688188   \n",
       "                                             0.60          0.693290   \n",
       "                                             0.70          0.737991   \n",
       "                       2012-07-01 2012-12-30 0.40          0.743474   \n",
       "                                             0.50          0.744607   \n",
       "                                             0.55          0.744714   \n",
       "                                             0.60          0.758400   \n",
       "                                             0.70          0.807986   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          0.715904   \n",
       "                                             0.50          0.724910   \n",
       "                                             0.55          0.938417   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          0.684119   \n",
       "                                             0.50          0.691463   \n",
       "                                             0.55          0.897337   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          0.743188   \n",
       "                                             0.50          0.754349   \n",
       "                                             0.55          0.934882   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          0.729897   \n",
       "                                             0.50          0.743998   \n",
       "                                             0.55          0.761590   \n",
       "                                             0.60          0.761643   \n",
       "                                             0.70          0.782931   \n",
       "                       2012-12-31 2013-07-01 0.40          0.696638   \n",
       "                                             0.50          0.706010   \n",
       "                                             0.55          0.721245   \n",
       "                                             0.60          0.721377   \n",
       "                                             0.70          0.740608   \n",
       "                       2012-07-01 2012-12-30 0.40          0.756643   \n",
       "                                             0.50          0.768649   \n",
       "                                             0.55          0.782535   \n",
       "                                             0.60          0.782550   \n",
       "                                             0.70          0.799340   \n",
       "\n",
       "                                                        recall_0.01  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.012618   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.02  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.006462   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.05  \\\n",
       "classifier             test-start test-end   threshold                \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "...                                                             ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.000000   \n",
       "                                             0.70          0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          0.002591   \n",
       "                                             0.70          0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40          1.000000   \n",
       "                                             0.50          1.000000   \n",
       "                                             0.55          1.000000   \n",
       "                                             0.60          1.000000   \n",
       "                                             0.70          1.000000   \n",
       "\n",
       "                                                        recall_0.1  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.796143   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.809163   \n",
       "                                             0.60         0.001318   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.2  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.409262   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.549737   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.416950   \n",
       "                                             0.60         0.000679   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.3  \\\n",
       "classifier             test-start test-end   threshold               \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "...                                                            ...   \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.283787   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.379335   \n",
       "                                             0.60         0.000000   \n",
       "                                             0.70         0.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         0.286699   \n",
       "                                             0.60         0.000467   \n",
       "                                             0.70         0.000000   \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000   \n",
       "                                             0.50         1.000000   \n",
       "                                             0.55         1.000000   \n",
       "                                             0.60         1.000000   \n",
       "                                             0.70         1.000000   \n",
       "\n",
       "                                                        recall_0.5  recall_1  \n",
       "classifier             test-start test-end   threshold                        \n",
       "LogisticRegression     2013-07-02 2013-12-31 0.40         1.000000  0.986078  \n",
       "                                             0.50         1.000000  0.957916  \n",
       "                                             0.55         1.000000  0.919986  \n",
       "                                             0.60         1.000000  0.853609  \n",
       "                                             0.70         1.000000  0.599899  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.995301  \n",
       "                                             0.50         1.000000  0.984895  \n",
       "                                             0.55         1.000000  0.961936  \n",
       "                                             0.60         1.000000  0.909305  \n",
       "                                             0.70         1.000000  0.641649  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.982714  \n",
       "                                             0.50         1.000000  0.938698  \n",
       "                                             0.55         1.000000  0.886142  \n",
       "                                             0.60         1.000000  0.805871  \n",
       "                                             0.70         1.000000  0.551263  \n",
       "DecisionTreeClassifier 2013-07-02 2013-12-31 0.40         1.000000  0.998922  \n",
       "                                             0.50         1.000000  0.995306  \n",
       "                                             0.55         1.000000  0.993213  \n",
       "                                             0.60         1.000000  0.915958  \n",
       "                                             0.70         1.000000  0.618229  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.999060  \n",
       "                                             0.50         1.000000  0.996711  \n",
       "                                             0.55         1.000000  0.986037  \n",
       "                                             0.60         1.000000  0.958579  \n",
       "                                             0.70         1.000000  0.642521  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.997454  \n",
       "                                             0.50         1.000000  0.972162  \n",
       "                                             0.55         1.000000  0.963129  \n",
       "                                             0.60         1.000000  0.862780  \n",
       "                                             0.70         0.759538  0.404640  \n",
       "...                                                            ...       ...  \n",
       "AdaBoostClassifier     2013-07-02 2013-12-31 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.968762  \n",
       "                                             0.55         0.181681  0.103419  \n",
       "                                             0.60         0.000000  0.000000  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.975967  \n",
       "                                             0.55         0.240557  0.140239  \n",
       "                                             0.60         0.000000  0.000000  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  1.000000  \n",
       "                                             0.50         1.000000  0.931267  \n",
       "                                             0.55         0.180900  0.100801  \n",
       "                                             0.60         0.000295  0.000164  \n",
       "                                             0.70         0.000000  0.000000  \n",
       "BaggingClassifier      2013-07-02 2013-12-31 0.40         1.000000  0.936699  \n",
       "                                             0.50         1.000000  0.874667  \n",
       "                                             0.55         1.000000  0.775212  \n",
       "                                             0.60         1.000000  0.774832  \n",
       "                                             0.70         1.000000  0.640334  \n",
       "                       2012-12-31 2013-07-01 0.40         1.000000  0.952873  \n",
       "                                             0.50         1.000000  0.897489  \n",
       "                                             0.55         1.000000  0.809076  \n",
       "                                             0.60         1.000000  0.808741  \n",
       "                                             0.70         1.000000  0.678907  \n",
       "                       2012-07-01 2012-12-30 0.40         1.000000  0.921330  \n",
       "                                             0.50         1.000000  0.847834  \n",
       "                                             0.55         1.000000  0.742147  \n",
       "                                             0.60         1.000000  0.742065  \n",
       "                                             0.70         1.000000  0.606487  \n",
       "\n",
       "[90 rows x 19 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.set_index(['classifier', 'test-start', 'test-end', 'threshold'])\n",
    "results_df.to_excel(\"output/results.xlsx\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Report\n",
    "\n",
    "### 3.1 Overview\n",
    "\n",
    "DonorsChoose is a platform on which teachers may start projects to raise funds for required materials for their classrooms. Once projects are fully funded, DonorsChoose purchases the materials with the funds they collected on the projects' behalf. \n",
    "\n",
    "We were asked to develop a model to predict which projects would be fully funded within 60 days. By using several classification techniques, we have trained a series of models which may be used towards this purpose. The information from each project that we've used to train the models are as follows: \n",
    "\n",
    "| Feature | Type | Description\n",
    "| --- | --- | ---\n",
    "`school_state` | categorical | State in which school is located\n",
    "`school_metro` | categorical | Rural, urban, or suburban\n",
    "`school_charter` | boolean | Is the school a public charter school?\n",
    "`school_magnet` | boolean | Is the school a public magnet school?\n",
    "`teacher_prefix` | categorical | Dr., Mr., Mrs., or Ms.\n",
    "`primary_focus_area`| categorical | Main subject area for which project materials are intended\n",
    "`secondary_focus_area` | categorical | Secondary subject area for which project materials are intended\n",
    "`resource_type` | categorical | Books, supplies, technology, etc.\n",
    "`poverty_level` | categorical | Low, moderate, high, highest\n",
    "`grade_level` | categorical | Grade level for which project materials are intended\n",
    "`total_price_including_optional_support` | categorical | Project cost including optional tip that donors give to DonorsChoose.org while funding a project \n",
    "`students_reached` | numeric | Number of students impacted by a project (if funded)\n",
    "`eligible_double_your_impact_match` | boolean | Project was eligible for a 50% off offer by a corporate partner\n",
    "\n",
    "\n",
    "### 3.2 Which classifier does better on which metrics?\n",
    "\n",
    "The overall results can be found in this [Excel spreadsheet](https://github.com/jtanwk/capp30254/blob/master/HW3/output/results.xlsx).\n",
    "\n",
    "The models using logistic regression, decision trees, and support vector machines all seemed to perform comparably at overall accuracy. Depending on the length of time used for the training data, and the probability score threshold beyond which we would consider a project likely to be fully funded in 60 days, 71-73% of their predictions were correct.\n",
    "\n",
    "Interestingly enough, if considering instead the precision metric, the models using a boosted ensemble of logistic regression classifiers performed the best. Overall, 84-87% of the projects predicted to be fully funded within 60 days actually turned out to be so. When looking only at the top 1% of projects, ranked by likelihood of being fully funded in 60 days, 96-99% of them are correctly predicted.\n",
    "\n",
    "Given that a baseline of 71% of projects in the dataset are actually funded within 60 days, most of our measures of recall - the proportion of projects actually fully funded within 60 days that were  correctly predicted by the model - are irrelevant. Overall, here, models using decision trees performed the best, correctly identifying 97-99% of projects actually meeting the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xm8VfP++PHXu9NwShMlQzNCxhAVRSSFlDkhQ+GGLnGN1/2R4d7rul8zoZSEMlwklaFSQoYGilJJoqM5Ko2nznn//niv7eyzO8OuztrrnH3ez8djP85ew177/dl7nf1en7U+6/MRVcU555wraRWiDsA551x68gTjnHMuFJ5gnHPOhcITjHPOuVB4gnHOORcKTzDOOedC4QnGOedcKNIiwYjIbBFpX8w6jURkvYhkpCisEiciTURERaRiMD1JRK6KOq6SICJVReRdEVkrIm9EFEPafJ6JRGSoiDwQ4vbXi8h+wfN836WIXCIiH4b13ulERP4uIs9HHUdJCTXBiMgiEdkU7HzLReQFEale0u+jqoeq6qRi1vlFVaurak5Jv78rEecDewF1VPWCktqoiDQVkVwRGVBS2wy2G79v/y4iY0SkYUm+RwHvWWoTYPC/tTCYzPddquorqnpaFHGJyL4iklXIsvtF5FsR2SYi/QtYfrGI/CwiG0RkpIjsUcKxtU+MTVX/paqhfMfBPntqGNsuTCpqMGepanXgaOBY4B+JK4hJi9oUQKyGkQ5SWJbGwHxV3bajLywmxsuA34GLRKTKzgZXiNi+vQ+wHHiyhLdfVu30d5moBM44nAG8X8iyBcBtwJgC3vdQ4DmgJ5YsNwIlepBSluz074CqhvYAFgGnxk3/FxgdPJ8E/BP4DNgEHADUAgYDS4FfgQeAjLjXXw18D/wBzAGOTnwf4DhgGrAO+6d/JJjfBFCgYjC9LzAK+A3b0a6Oe5/+wOvAsOC9ZgMtiyhnf+B/wMvB+16FJe87gB+B1cH29oh7TVtgCrAGWAxcEcw/E/g62M5ioH/caxLLMAm4qpCYMoC/B+//BzAdaJi4jcTtAFcE38mjwWfz7yDGw+LW3zP4zuoF012Ab4L1pgBHxK17e/Bd/gHMAzoUEOu9QDawFVgP9A4+v38APwMrgu+iVsLn0Bv4BZhcxHfzI3BtsC+cn7CsIzAXWAs8BXwc9znsD3wUfHergFeA2kXs22dgP6qx6VpBzCuDMvwDqBAsK6psmdh+tDr4PKdiP3D/BHKAzcFn9FQh5S1svxoKPBA83x0YHcT2e/C8Qdw2rgAWBt/ZT8AlwfwDgs9obfCZvBb3Gg2WF/RdXgF8GrfuwcA4bP+aB1wYt2wo8AwwFtgQ/xkHy08Gvo2bHg98FTf9KXB23PRbwLnF/E69TNz/WTDvX8DwuOn9g3LVKGQbRZXpDOz36g/sf+EWYDfsfyg3+JzWY79J/YGXE/bzK4Pv8negD3agPiv4jp9KiLHAfRZ4KXivTcF73RbM74r9vq3BfgeaJ+zjtwfvtQWoSBL/z/k+lx1JGDv6IP8Pf8OgIPfH/aj9AhwaBF4JGIkdNewG1AO+Av4SrH9BULBjAcF25sYFvM/nQM/geXWgdSE/zh9jRySZQAvsn61DsKw/9o98BvZD/W/giyLK2R/7hzob+/GoCvQDvgAaAFWCco0I1m8UfEE9gnLXAVoEy9oDhwfbOQL7YTy7kDJMovAEcyvwLXBQ8HkdGbxPvm0kbgf7MdgG/DX4XqoCQ4B/xq1/PfB+8Pxo7EeyVfBZXR58H1WC914M7BsX//5FfIYvx033whL/fsH3+BbwUsLnMAzbV6oWss122D/G7ljtYlTcsrpYEj8/+A5uCsod+xwOwBJQFSyhTgYeK2Tfrga8CAyLWz4MeAeoEcQ7H+idRNn+ArwbbDMDOAaoWdz3ncR+NZS8BFMHOC94jxrAG8DIYNluwedyUDC9D3Bo8HwEcBe2b2YCbePeW4EDCvkuryBIMMH2F2M/mhWx/WdV3HsMxRLYCbH3SShjJvYjWTd4/TJgSVCOqsGyOsG6lYJtF5gU4rZZUIJ5B7g9Yd564JgCXl9cmZYC7YLnu5N3YNweyCrs/4C8/fzZoNynYb9LI7Hfx/rY/95JO7rPBtMHYkm8Y/BZ3Ybtl5Xj1v8G++2uyg78P//5HjuSMHb0EQS4HsuOP2M/6FXj/lnui1t3L+zHoGrcvB7AxOD5B8CNRbxP7J99MnYUVTdhndiXVTH4wHLidzwsiQyN+5LHxy07BNhURDn7k3AUjdW0OsRN74MloYrAncDbSX6GjwGPJpahuB8c7OiiWwHz820jcTvYj8EvCa85FVgYN/0ZcFnw/BmCg4aE9z4J2+FXBK+vVEw5+5P/R2kCcF3c9EFxn1+sDPsVs83nyfvhbBO8Plbruoy4gwYsCWcV8XmeDXxdyL69DfuROzxYloHty4fErf8XYFISZetFQi2woO+pkBgL3a+ISzAFLGsB/B483y0o03kkJG4saQ4krrYTtyzZBNMd+CThtc8B98TFOaygOOPW/wQ4F2gNfIidHeiM1W5mxa3XAZiQxP9YQQlmAtAnYd6vQPsCXl9cmX4Jvv+aCeu0J7kEUz9u+Wqge9z0m0C/Hdhn4xPM/wNej5uuEF/GYP1eccuT/n+OPVJx3eNsVa2tqo1V9TpV3RS3bHHc88ZYFl0qImtEZA32JdULljfETncUpzeWmeeKyFQR6VLAOvsCv6nqH3HzfsaOCGKWxT3fCGSKSMWgRcz64PFeIWWJleftuLJ8jyW1vYoqi4i0EpGJIrJSRNZiVeK6xZZ6e8l+XgVJLMtHQNUgtsbYD9LbwbLGwN9i5QzK2hA7ylmA1eT6AytE5FUR2TfJGPbFvpOYn7Ef4L2KiPNPIlIVq/W+AqCqn2P/6BfHbf/P16v9By2Oe329IN5fRWQd9iOU+D2craq1sSPGvsDHIrJ3sF7lAuKP7V9Fle0l7GDqVRFZIiIPiUilwsqZIKnvXESqichzwQXsddhBWW0RyVDVDdgPZh/sf3GMiBwcvPQ2LBF/FbTc7JVkXPEaA60S9pdLgL3j1in0ew18jP04nxg8n4Qd0JwUTMecgZ1q2xnrgZoJ82piNcRExZXpvCCWn0XkYxFps4OxLI97vqmA6eqQ9D4bL99+qKq52Gcf/zsY/z+yw//PUV9Y17jni7GjvrpBQqqtqjVV9dC45fsXu0HVH1S1B5aY/gP8T0R2S1htCbCHiNSIm9cIy97Fbf8VtRYz1VX19ELKEov39Liy1FbVTFX9tZiyDMeuDTVU1VpY9ViKi6sAhb3HhuBvtbh5eyesk68swY73OlajvBi7jhb7R1uMnT6LL2c1VR0RvHa4qrbF/gkV+06SsSR4TUwjrKYQ/8+V+JnHOwf7QRggIstEZBn2j3NZsHwp9oMMWEOT+GmsRqtYTaImcCmFfA+qmqOqb2EHEG2x0yNbC4g/tn8VWjZV3aqq96rqIcDx2PWtWMxFlReS/B8B/obVmloFZTsxmC9BeT5Q1Y5YrXsuMCiYv0xVr1bVfbEj8gEickAS75cY48cJ+0t1Vb02bp3iypmYYD6m8ASz3QX8JM3GTisDEDTBroKd6kxUZJlUdaqqdsN+k0Zi/0tQfDl3VHH7bOL75dsP4/4Hfi3sNTv6/xx1gvmTqi7FqrsPi0hNEakgIvuLyEnBKs8Dt4jIMUGrswOCo+l8RORSEdkz+FFcE8zO1zRZVRdjpyH+LSKZInIEVvN5pQSL9Czwz1iMIrKniHQLlr0CnCoiFwa1ojoi0iJYVgOrXW0WkePIO+LeUc8D94tIs+DzOkJE6qjqSmwHulREMoKj0GR+lIZjR7aXBM9jBgF9gtqNiMhuInKmiNQQkYNE5JSg9dZm7Ggr2WbiI4CbxJoZV8cuur6mybdMuhy7dnQ4VuNqgZ3XbyEih2M/PIeKyLlBC5kbyJ9oaxCcAhOR+tg1rQIF5e6GnV//Xq0p/OvY918j2Aduxo4oiyybiJwsIoeLtZ5ahyWq2Ge2HLtuU5ii9qt4NbDvYo1Y09t74sqyl4h0DQ7KtgSfQU6w7AIRaRCs+jv2A7Ojzf5HAweKSE8RqRQ8jhWR5juwjSlYgjwOu8A/m6AWgdXGEJGmQBVVnVvYRoL3zsR+BysGvwWxVmuvAGeJSLvgs7gPeCvhrEexZRKRymJnPWqp6lbsO43/PuuISK0dKHtRittnE/ef14EzRaSDWC35b9h3PqWgje/M/3OpSTCBy7BTC3OwHfh/2FEUqvoG1pJmOFZNHQkU1C69MzBbRNYDjwMXqermAtbrgZ3jXIKd7rlHVceVYFkex2oiH4rIH9gF/1ZBWX7Bjq7+hrU6+Ya8o6XrgPuC19xN3tHOjnokeO2H2E49GLtQB9Ya71bsfO6hFLJDxVPVL7Haz77Ae3HzpwXbewr7zhZg59zBjvgexI7ol2FHcH9PMv4h2OmiyVhLps1Yw4NiBf9cHbALnMviHtOxJquXq+oq7BTag9jn0Ay7thRzL3axdi2WjN4q4K3eDfazddi+eXnwY0cQ6wasNdan2H47JImy7Y3t9+uw06ofk5eYHgfOF7vv5onEYIrZr+I9hu0Lq7D9Mr4Zb4Xg9UuCbZyE7ZNgDWy+DMo8Crsm+lMB2y9U8AN9GnBR8B7LsKPgpJuQB6fxZgCzVTU7mP058LOqrgimz6T402ODsB/JHljjhU1Ys2SC77EPlmhWYD/e1xW0kSTK1BNYFJy26oPVLAiS3whgodiptWRPHxemuH3238A/gve6RVXnBbE8ie0LZ2FN77Mp2A7/P0tw8cY559KGiIzFmvDu7DUYVwJKWw3GOedKwiRgYtRBlHdeg3HOORcKr8E455wLRZnrM6tu3brapEmTqMNwzrkyZfr06atUdc9UvmeZSzBNmjRh2rRpUYfhnHNlioj8XPxaJctPkTnnnAuFJxjnnHOh8ATjnHMuFJ5gnHPOhcITjHPOuVCElmBEZIiIrBCR7wpZLiLyhIgsEJFZInJ0WLE455xLvTBrMEOxjicLczrWwWAz4Bps4CrnnHNpIrQEo6qTsd5YC9MNG7lOVfULbMCjfcKK59NP4e674V//gnXrwnoX55wLwfLl8P/+H8ybF3UkOyTKazD1yT9yXRb5R1L7k4hcIyLTRGTaypUrd+rNPv8c7r8f7roLatWCfv12ajPOOZc6c+fCNddA48bwz3/CRx9FHdEOiTLBFDQ6YIE9b6rqQFVtqaot99xz53o6uPVWyM21gwCAr7/eqc0451y4VGHyZOjaFZo3h5degiuvtGRz7bXFv74UibKrmCzyD1HbABusJzQicN99drpsW7LjIjrnXCps2wZvvw3//S9MnQp168I998D118NOHlhHLcoazCjgsqA1WWtgbTBscugqVLCDBOeci9yGDfDkk3DggXDhhfD77/DMM/Dzz9C/f5lNLhBiDUZERgDtgboikoWN+10JQFWfxYYzPQMbYncjcGVYsSSqUAFWroSRIy3RHHqofbfOOZcyy5bBU0/BgAGWVI4/Hh55BM46CzIyoo6uRISWYFS1RzHLFbg+rPcvyh57wLhxcM45efPWrLGL/845F5pt22D6dBg0yK6tbN1qP0R/+5slmDRT5rrrLwmDB8Ntt9k1mXvugXfftRqNJxjnXIlShdmzYcIEawE2aZLdJ5GZCb17w003QbNmUUcZmnKZYHbbDY4O+g3o188SzFlnQdu20KMH1K9vLc62bbNH06ZQu3a0MTvnyoiFCy2ZxJLKihU2f//94aKLoEMHOPVUO5WS5splgol38sl22vPmm60V4PPPF7zee+9Bw4ZQsaJdr5GCGlk758qfZcsskcSSyqJFNn+ffaBjR0sop5xi97KUM6JlrDlVy5YtNYwRLTdtgjffhGrVYPNmu8ZWsSKMGgWjR8NvcX0SPPCA3bDpnCuH1qyBjz+2ZDJhAsyZY/Nr17Yj1lNOsaRy8MGl6khURKarasuUvqcnmOJt2GCNArKzoXt3m9e3Lzz0EFStmtJQnHOptnEjfPZZXg1l+nQ7h161KrRrZ8mkQwdo0aJUt/7yBJOEKBJMvJ9+gocfhqeftms1Dz0ENWrYtZqcHJvXpk1k4TnndtXWrXajY+waypQpdnRZsSK0bp1XQ2nVCqpUiTrapHmCSULUCSZm/HjrvSEra/tlPXvCIYdYLad69dTH5pzbQQsW2PnwCROsm5b16+30VosWeTWUtm3L9D+0J5gklJYEA7YPzp6dd73m11/hscesJeK2bVCpElx+ubVaiz2qVbP5lSpBo0a231aqFHVJnCvH5s6Fo46yi68HHZRXQ2nfHurUiTq6EuMJJgmlKcEUJicHzjwTfvzRrt/EHjk5Ba+fmWmJ58QTbd+OV62a1YgqVw4/bufKnZwcq5nMnw9ffJHW96REkWDKfTPlMGRkwPvv55+naqdxN260U7zZ2XbdcP58WLvWruuMHGmPRIMHW/N5sG6JzjnHkpJzbhc99pgllldeSevkEhWvwZQSW7bYKbdEzz0H//53/mWXXmoNCSpUsIOvww5LXZzOpY158+waS6dO1otxKWpSHAY/RZaEdE0wRdm82Wo+ublWe/n00/zLY2PcxK9/zDHWO0G1aqmL07kyIyfHzkl//71dSN0ntMF0Sw0/ReYKlJmZd0ps4kTreDU313ofGD7cbvyMiT9e2H13uOIKSzKxRgb16sF553nDAlfOPfGENT9+6aVykVyi4jWYNDRvHvTqZV0irV9vDQziv+Zatawm1KSJ3bdTvz507pz2ZwicMz/8AEccYd24vPNOudnxvQbjSsRBB1kDghhVO222bh3ceac1LBg3DpYsyUs8J54IRx5pNZsbb7Qm1M6lnZwcu4EtMxOefbbcJJeoeIIpB0SsV4uqVWHIkLz52dnw1VfQrRvMnAnffmun32bPhi5d7BaAOnVs5NY6dawFm1/TcWXak0/a0dewYbDvvlFHk/b8FJnL58wzYezYwpc3amRNqs8/P3UxOVcifvjBqukdOthd++Ws9uKnyFzkxoyxXgh++w1Wr87/WLoUHn8cLrjAhpmuXdsSTb9+UUftXDFyc+3CZJUq1va/nCWXqHiCcdupWNFam9Wrt/2y9u1tzJx16+Ctt+yg0BOMK/Weesra9w8d6qfGUshPkbmddt99NuR07drQoEHhjz32yH/AKGLJyw8iXUosWGCtxk4+2QZ3Kqc7np8ic2VKnz52xmHxYutVOisLvv4ali8v/rU9e8L//Z89F7FH7dpWe3KuxOTmQu/e1pnfwIHlNrlExf+d3U6rVw9uv337+dnZ1gQ6K8t6mP7tt/z/1zNnWgvRl17K/7p27ayndOdKzP332041ZIjd8OVSyhOMK3GVK9tNnE2aFLxc1e5xW7rUnqvCiBE2xlP37tbHWoUK1mlo/N8KFazG1KkTnHaa90bgivHYY9C/P1x2mXVp4VLOE4xLORE499z88/beG+69F2bNsrMaOTn5/8aeL19utzJUqmSn1Nq3txtLr7wS9tsvkuK40mjwYLjpJusXafBgPzUWEb/I78qUpUvtjMfMmTBjBkybZjeHNmliz3ffPeoIXeRefRUuvtiquu+844MpBbw35SR4gnGJPv8cTjrJOvM8+GAb1uPAA/P+HnAA1KgRdZQuJT7+GE49FY4/Ht57z7ueiOOtyJzbCW3aWOvTt96yftYmTty+AUHDhtC6ta3bpo2NkFulSjTxuhC98Yb1ifTuu55cSgFPMC4tnHaaPWI2brTbH374wZLOrFlW03njDVtepYq1gos1Ioh/xOZVqgR//zucfXY0ZUp727bBP/5hX1CstceuPmbPtmprzZpRl87hCcalqWrV7N66I47IP3/pUks0X3wBq1ZZw4H4BgXxj3ffhb59rV9EyLtfB+z36/HH/dTbLrn9dnjkEWje3G6Ain3Au/I49FC7ycqVCn4NxrlCvPCCtXSNP0AGO0iOOfxwqwl16QJHH22n4qpWtd7gq1a168vegKkAr74KPXpYBn/yyaijKRf8In8SPMG4qGVnw4MPwi+/WCeg331np+MKc9xx8Oijdt3ZYR9Yq1Z2Ieyjj7yVV4qk3UV+EekMPA5kAM+r6oMJyxsBLwK1g3XuUNUiOot3LnqVK8Pdd+dNq1p3ObNn2306mzfDpk32d/1661/xhBOsK6zbbrPWs+W2VrNmjQ2nWrOmXRDz5JLWQqvBiEgGMB/oCGQBU4Eeqjonbp2BwNeq+oyIHAKMVdUmRW3XazCurFm/3rrGefRR60KneXM47LC8Hqvr1YO99sr727SpNTJIO7m5Nrrd++/DpEmWdV3KpFsN5jhggaouBBCRV4FuwJy4dRSINfeoBSwJMR7nIlG9OtxyC9xwg3WJM2yYtWpbscJuEk1Uu7bd13PyyfY47DBr2VbmPfCAtSd/6ilPLuVEmDWY84HOqnpVMN0TaKWqfePW2Qf4ENgd2A04VVWnF7Cta4BrABo1anTMzz//HErMzqVadra1Zluxwh6//gpTpti9PD/+uPPbzcy0FsB9+thw15EbMwbOOstaeA0dWo7PEUYnrS7yi8gFQKeEBHOcqv41bp2bgxgeFpE2wGDgMFXNLWy7forMlRe//GLXwH/6acdeN2uWDTu/cqVN169v9/TUrGlj+HTrVvKxFmnBAjj2WOvPZ8oUa17nUi7dTpFlAQ3jphuw/Smw3kBnAFX9XEQygbrAihDjcq5MaNRo1zoBHjcO3nwTtm61x9df202jDRrk9XYdezRtCm3bhnDNfcMG69m0QgXrasGTS7kSZoKZCjQTkabAr8BFwMUJ6/wCdACGikhzIBNYGWJMzpUbHTvaI2bTJrv8MXs2LFoEn3wCw4fbtXewyyJjx5bgTfCqcPXV1iz5/fcti7lyJbQEo6rbRKQv8AHWBHmIqs4WkfuAaao6CvgbMEhEbsIu+F+hZe3GHOfKiKpV4dZb88/butWu+4wfD9deawnp/fdLqFfqxx+3Vg3/+lf+fnxcueE3WjrnABg1Ci64wE6ZtWxp3e3stps9knpeVan500yqTRxDxvtjrD+es8+283R+UT9y6XYNxjlXhnTtav2v3XUXfPmlXT7ZuNH+5uQU/JpqbKADEziTMZzBWGqQBcA0WjIusz/jsm6mWR/5s1+4ww+3ZtiufPAajHOuSKrWnDqWbLLn/USlD8ew28djqDVjIhlbt7A1szpZzU9j/kFdmNPodFZm7M2aNfD99zY4XPz9Po0asV3CSeyzskKFwvuzLG7ZAQd4Z8oFKZU1GBGphl0raaSqV4tIM+AgVR0denTOucjJtq1U+WIKVcaMYfcxY2BOcK90s2Zw/bXQpQuV2rWjaeXKNAU6Jbxe1XowmDUr/+P9963H/pJ2+OF2uq9Jk5LfttsxyZwiewGYDrQJprOANwBPMM6lq1WrbETIMWMsE6xdazfTnHiitQw780xLMEkQsXtx6teH00/Pm79lC8ybZ7WigoZ2yc0tfNiXwpa99x489xzstx907mw3mp5xho0G4FKv2FNkIjJNVVuKyNeqelQwb6aqHpmSCBP4KTLnQrR4sY1n/9ln9ou91172C92liw1FXAbOPS1eDIMHw6BBVnPae28rQqdOVoQ99og6wmiUylNkQLaIVMWaESMi+wNbQo3KOReNgQNtRLa7784b5KaMdYTWsCH0729d5YweDa+8Yvd4DhliRWnVymo3nTvDMcekaceipUQyNZiOwD+AQ7B+w07A7leZFHp0BfAajHMhatPGzmlNmRJ1JCVq2zb46iv44AM74zd1qlXQ9tjDbtHp1An23z9/s+tYM+wqVdKjlXWp64tMRATr4mUj0BoQ4AtVXZWa8LbnCca5kKxdaz1j3nkn3H9/1NGEatUq60rn/fct6SxfXvi6FSoUff9P0vcJxT2vU8ceqUxcpe4UmaqqiIxU1WOAMSmKyTkXhY8/thteTj016khCV7eujdjco4c1GIgNFhd/708yz3/7bfv52dnJxVCjhvWes99+2/9t0iQ9um1L5hrMFyJyrKpODT0a51x0xo+3w+zWraOOJKUqVLCmzYcfXjLb27o1L+kUlqBWrLBeshcuhB9+sFrUpk35t7PPPtsnnnbt7FReWZFMgjkZ+IuI/AxswE6TqaoeEWpkzrnUGj/emiFXqRJ1JGVapUpQq5Y9kqVqNahY0on/O3lyXqekzz6bfgnm9OJXcc6Vab/+arfd9+4ddSTlkog1p957b2tnkSg728YHKpFOSFOo2ASjqj+LyJFAu2DWJ6o6M9ywnHMpNWGC/S0H11/KosqVrQucsqbYBu4iciPwClAveLwsIn8t+lXOuTJl/HjYc8+SuxDhHMmdIusNtFLVDQAi8h/gc+DJMANzzoVo1Sr49tu8jsFGjrTuX8rYTZWudEsmwQgQ31l3TjDPOVeWbNoEPXvanfpL4kYv33NPu729X7/oYnNpKdnOLr8UkbeD6bOBweGF5JwLxQcf2OBf554Lxx+f12f+XntFHZlLU8lc5H9ERCYBbbGay5Wq+nXYgTnnStg779jgK6++am1pnQtZMuPBtAZmq+qMYLqGiLRS1S9Dj845VzJWrrSeH884w5OLS5lkrug9A6yPm94QzHPOlWajRtmF+/r1oV49u7B/7rlRR+XKkaQu8mtcj5iqmisiPnyPc6XdfffBokVWa2nRwvqmb9eu2Jc5V1KSSRQLReQG8mot1wELwwvJObdLtmyBsWNh+nS4/np46qmoI3LlVDIJpg/wBDYmjAITgGvCDMo5Fxg4EB55pOAxghPnxaaXLrXXVqwI3bpFG78r15JpRbYCuCgFsTjn4mVlwY03woEHwiGHWIdVFSrY38RH4vx69eCOO6B69ahL4cqxZFqRPQQ8AGwC3geOBPqp6sshx+Zc+XbvvTY+y8iR1l+7c2VMMq3ITlPVdUAXIAs4ELg11KicK+++/94Gkb/uOk8ursxKJsHEGs2fAYxQ1d9CjMc5B/D3v9vYunfdFXUkzu20ZBLMuyIyF2gJTBCRPYHN4YblXDk2ZYqdFrv1VusnzLkySuJucSl8JZHdgXWqmiMiuwE1VHVZ6NEVoGXLljpt2rQo3tq58KnCSSfB/Pkhjrb9AAAgAElEQVSwYIFfpHclRkSmq2rLVL5nUjdMqurvcc83YHfzO+dK2pgx8MknMGCAJxdX5oU6+IOIdBaReSKyQETuKGSdC0VkjojMFpHhYcbjXKmVmwsTJ8Itt9jQhVddFXVEzu2y0Lp8EZEM4GmgI9b6bKqIjFLVOXHrNAPuBE5Q1d9FpF5Y8ThXKs2ZAy+9BK+8AosXQ40a8Prr3iGlSwuFJhgRObqoF8Z6Vy7CccACVV0YbO9VoBswJ26dq4GnY6fggps6nUtvy5fDiBGWWGbMgIwM6NQJHnoIunaFatWijtC5ElFUDebhIpYpcEox264PLI6bzgJaJaxzIICIfAZkAP1V9f3EDYnINQTd0zRq1KiYt3WuFBs7Fs45B7KzrfPJxx6Diy7yQb9cWio0wajqybu47YKGVU5sslYRaAa0BxoAn4jIYaq6JiGWgcBAsFZkuxiXc9FYsAAuuQSaN7caTPPmUUfkXKiKOkVW5MARqvpWMdvOAhrGTTcAlhSwzhequhX4SUTmYQlnajHbdq5s2bDBxmIRgbff9rvzXblQ1Cmys4pYpkBxCWYq0ExEmgK/Yh1mXpywzkigBzBUROpip8x8KACXXlTh6qvhu+/gvfc8ubhyo6hTZFfuyoZVdZuI9AU+wK6vDFHV2SJyHzBNVUcFy04TkTlADnCrqq7elfd1rtR5/HE7JfbPf9rFfOfKiWTv5D8TOBTIjM1T1ftCjKtQfie/KxNUYckSmDQJLr/cWoe9+aadInMuAqXyTn4ReRaoBpwMPA+cD3wVclzOlT0TJ8KwYXZvy9y5sG6dzW/eHIYO9eTiyp1kbrQ8XlWPEJFZqnqviDxM8ddfnCt/+vSBZcus+fGll9ogYc2bQ6tW1jOyc+VMMglmU/B3o4jsC6wG/Cqlc/HmzbMOKp98Evr2jToa50qFZBLMaBGpDfwXmIG1IHs+1KicK2veecf+du0abRzOlSLFJhhVvT94+qaIjAYyVXVtuGE5V4ZMmAAPPwxHHw3e04Rzfyq2N2URuT6owaCqW4AKInJd6JE5V5rNnw///S+ccgqceirUqQMvvhh1VM6VKsmcIrtaVZ+OTQS9Hl8NDAgvLOdKAVVYtcq60o9Nq8L330OHDjbviCPgxhvhjjtg772ji9W5UiiZBFNBRESDG2aCbvgrhxuWc6XAHXdYD8eFee896Nw5dfE4V8Ykk2A+AF4P7odRoA+wXY/HzqWdOXOgYUNLNGD3scTuZalZEzp2jC4258qAZBLM7cBfgGuxHpI/xFuRufLgt9+gWTO4zi85OrczkmlFlisiQ4GPVHVe+CE5V0qsXm3XWJxzOyWZVmRdgW8ITouJSAsRGRV2YM6FZts2G/O+eXM44ABrWrzPPlC3LtSqBVWrQsWKdvPknntGHa1zZVYyp8juwYY/ngSgqt+ISJPwQnIuZDfdBIMHQ5cullAqVcr/qFw57+8ll0QdrXNlVjIJZpuqrhXvqM+lgwED4Kmn4JZb7D4W51xokkkw34nIxUCGiDQDbgCmhBuWcyEYNw5uuMFqLg8+GHU0zqW9Yq/BAH/FxoLZAgwH1gL9wgzKuRI3dy5ccIFddxk+HDIyoo7IubRXZA0muKnyXlW9FbgrNSE5V8JWroSzzrJrKu++CzVqRB2Rc+VCkTUYVc0BjklRLM6VHFX45BO44gpo3Bh++QVGjoQmTaKOzLlyI5lrMF8HzZLfADbEZqqqDzrmSp9ly6zTySFDrEPKGjWgZ0+7WfLII6OOzrlyJZkEswc2yNgpcfMUH9XSlSbTp8P998Po0ZCTA23bwp132nUXH03SuUgUmmBEpAfwoapemcJ4nNs53bvDmjVw883QqxccfHDUETlX7hVVg2kMvCEilYAJwHvAV7FelZ0rNX76CX780Ycrdq6UKfQiv6o+qKqnAGcAM4FewAwRGS4il4nIXqkK0rkiffSR/T3llKLXc86lVDKdXf4BvB08EJFDgNOBYUCnUKNzLhkTJthgX82bRx2Jcy5OMhf5EZH62Cmz2PpTVfXh0KJyLhmbNsELL8CYMXZ3vndn5FypUmyCEZH/AN2BOUBOMFuBySHG5VzxevWCV1+FVq3gH/+IOhrnXIJkajBnAwep6pawg3GuWGvWwKBB8PLLMHs2XH651WK89uJcqZNMglkIVML6InMuOqpw2GHw66/Qrp21GLv+ek8uzpVSySSYjcA3IjKBuCSjqjeEFpUrf774wrrR37IFsrO3f2zZAuvWWXK5+264996oI3bOFSOZBDMqeDgXnhdfhBEj7AbJypXzP6pXz3t+4onW9YtzrtRLppnyiyJSGTgwmDVPVbeGG5Yrd3JzoV49u67inEsLxY4HIyLtgR+Ap4EBwHwROTGZjYtIZxGZJyILROSOItY7X0RURFomGbdLN7m5fi3FuTSTzCmyh4HTVHUegIgcCIygmG78g7FkngY6AlnAVBEZpapzEtargY2S+eWOh+/SRm4uVEhm/DvnXFmRzH90pVhyAVDV+VirsuIcByxQ1YWqmg28CnQrYL37gYeAzUls06UrTzDOpZ1kajDTRGQw8FIwfQkwPYnX1QcWx01nAa3iVxCRo4CGqjpaRG4pbEMicg1wDUCjRo2SeGtXqqnafSzLl9vz3FyYNcsTjHNpJpkEcy1wPXYaS7A7+Ack8bqCTqj/2ROziFQAHgWuKG5DqjoQGAjQsmVL7825rJs1Cy67bPv5p56a+licc6FJphXZFuCR4LEjsoCGcdMNgCVx0zWAw4BJYhd39wZGiUhXVZ22g+/lypJPP7W/c+ZAo0ZWcxGxZsjOubRR1IBjr6vqhSLyLXE1jxhVPaKYbU8FmolIU+BX4CLg4rjXrwXqxr3fJOAWTy7lwKefwr772j0v3nLMubRVVA3mxuBvl53ZsKpuE5G+wAdABjBEVWeLyH3ANFX1mzfT2e+/w1tv2c2TM2fadRZVe6xbB+ed58nFuTRXaIJR1aXB01XAJlXNDZooH4yNblksVR0LjE2Yd3ch67ZPZpuuFNu4EUaPhuHD4b33rIuXAw6Ac8+FSpUsoYjYKbErfSRu59JdMhf5JwPtRGR3bOjkaVj3/ZeEGZgrI7ZuhXHjLKm88w6sX2+nv66/Hnr0gJYtvabiXDmVTIIRVd0oIr2BJ1X1IRH5OuzAXBkwcSJccAGsXg27724JpUcP6y8sIyPq6JxzEUsqwYhIG6zG0nsHXufS3eTJllzeeQc6d/ZWYM65fJJJFP2AO4G3g4v0+wETww3LlQlbtkDFitC1a9SROOdKoWTug/kY+DhueiF206Ur77KzvdbinCtUUffBPKaq/UTkXQq+D8YPW8u77GyoUiXqKJxzpVRRNZhY32P/l4pAXBm0ZYvXYJxzhSrqPphYh5bTCO6DgT+74ffD1vJq5Uq7x2XxYhg40Lp6cc65AiRzkX8CcCqwPpiuCnwIHB9WUK4Uu+02GDrUntetC+ecE2k4zrnSK5kEk6mqseSCqq4XkWohxuRKq+++g/ffh/btYexYqFo16oicc6VYMglmg4gcraozAETkGGBTuGG5yLzwAowfD2vWWH9ia9bkPTZtsm5eHnjAk4tzrljJ3gfzhojEutrfB+sqxqWjf/wDNmyAZs2gdm2oX9/+xp5feqmdGnPOuWIkcx/MVBE5GDgIG0RsrqpuDT0yF42NG20wsCeeiDoS51wZV+wYtcH1ltuBG1X1W6CJiOxUF/6uDNi0yU9/OedKRDKDoL8AZANtguks4IHQInLRycmxe1uqeRsO59yuS+YazP6q2l1EegCo6iYR7389baxcCd272yBgubk2z2swzrkSkEyCyRaRqgTdxYjI/sCWUKNyqfPtt9btfuvWUKcONG4MZ54ZdVTOuTSQTIK5B3gfaCgirwAnAFeEGZRLgWXLYOpUGDDAph95BNq0Kfo1zjm3A4pMMMGpsLnAuUBrrBXZjaq6KgWxuZK2dSv06mU1ll9/zZtfp453+eKcK3FFJhhVVREZqarHAGNSFJMLy+LF8PLLcNJJcMstNpzxYYdBrVo+rLFzrsQlc4rsCxE5VlWnhh6NC9e2bfb3mmvg4oujjcU5l/aSSTAnA31EZBGwATtNpqp6RJiBuRK0YYNdbzn3XJv2MVyccymQTII5PfQoXMm7+254+23IyrJ+xAAaNoTbb4dOnaKNzTlXLhQ1omUm0Ac4APgWGKyq21IVmNtFzz9vN0xecgk0aGD9iHXqBPXqRR2Zc66cKKoG8yKwFfgEq8UcAtyYiqBcCdi4ES68EB57LOpInHPlVFEJ5hBVPRxARAYDX6UmJLdLVOGvf7U78/2OfOdchIpKMH/2mKyq27x3mFLoP/+xGyVVbVrVxnDZsMGmzzoruticc+VeUQnmSBFZFzwXoGowHWtFVjP06FzRJk60U2Fdgs6tYwcBVarAnXf6zZPOuUgVmmBUNSOVgbidoAoHHGCjUDrnXCmTTHf9zjnn3A4LNcGISGcRmSciC0TkjgKW3ywic0RklohMEJHGYcaTFl580ZoeX3IJzJoVdTTOOVeoZG603CkikgE8DXTEBimbKiKjVHVO3GpfAy1VdaOIXAs8BHQPK6a00L8//Pab3c9SvTqcdlrUETnnXIFCSzDAccACVV0IICKvAt2APxOMqk6MW/8L4NIQ4yn7li+HRYvgrrvgAR9U1DlXuoV5iqw+sDhuOiuYV5jewHsFLRCRa0RkmohMW7lyZQmGWEZ88IGN17L33jbdrFm08TjnXBLCrMEUdOOMFriiyKVAS+Ckgpar6kBgIEDLli0L3EbaGjIEevfOm77sMrjooujicc65JIWZYLKAhnHTDYAliSuJyKnAXcBJqlp+h2LeuNFukoyXlWXJ5bTTYNgwu7+ldu1o4nPOuR0UZoKZCjQTkabAr8BFQL5BSETkKOA5oLOqrggxltKvWTNYsl3+NbfeCnvtldp4nHNuF4WWYILuZfoCHwAZwBBVnS0i9wHTVHUU8F+gOvBG0BXNL6raNayYSi1VSy5dukDXhOLvtpuNQOmcc2VMmDUYVHUsMDZh3t1xz08N8/3LjFhfYsceC1dfHW0szjlXQvxO/tIgN9f+VvCvwzmXPkKtwbhivPeedViZk2PTnmCcc2nEE0yUbr8dZs+GzEyoVQsOOyzqiJxzrsR4gonK1q3w3Xdw3nnwxhtRR+OccyXOE0yq/PILDBwI48ZBdjZ8843Nr1Ur2riccy4knmDClJtrCWXAABg92lqLtWtn97Q0bQp77AFPPBF1lM45FwpPMGEZOBAeegh+/NF6Pr79drjmGmjSJOrInCvVtm7dSlZWFps3b446lDIpMzOTBg0aUKlSpahD8QQTitxcuPZaOOQQGDECzj0XKleOOirnyoSsrCxq1KhBkyZNECmoS0NXGFVl9erVZGVl0bRp06jD8ftgQrF5syWZnj2tY0pPLs4lbfPmzdSpU8eTy04QEerUqVNqan+eYMKwcaP9rVYt2jicK6M8uey80vTZeYIJw6ZN9rdq1WjjcM65CHmCKUlLlsAll9i1F7COKp1zDjj++OOLXH7GGWewZs2aFEWTGn6Rf1ds2gTz5tl9LdnZ8O67MHy49Yp85JHQqVPUETrnQpCTk0NGRsYOvWbKlClFLh87dmyRy8siTzC7olcvePXV/PMyM+3O/MzMaGJyLo3065d3T3JJadECHnus8OWLFi2ic+fOtGrViq+//poDDzyQYcOGccghh9CrVy8+/PBD+vbty7HHHsv111/PypUrqVatGoMGDeLggw9m+fLl9OnTh4ULFwLwzDPPcPzxx1O9enXWr1/P0qVL6d69O+vWrWPbtm0888wztGvXjiZNmjBt2jTq1q3LI488wpAhQwC46qqr6NevH4sWLeL000+nbdu2TJkyhfr16/POO+9QtRSfivcEsyu++srGarntNqhUyVqLNWjgycW5Mm7evHkMHjyYE044gV69ejFgwADA7jH59NNPAejQoQPPPvsszZo148svv+S6667jo48+4oYbbuCkk07i7bffJicnh/Xr1+fb9vDhw+nUqRN33XUXOTk5bIw1CgpMnz6dF154gS+//BJVpVWrVpx00knsvvvu/PDDD4wYMYJBgwZx4YUX8uabb3LppZem5kPZCZ5gdtaGDfDTT3DFFXDGGVFH41xaKqqmEaaGDRtywgknAHDppZfyRNDjRvfu3QFYv349U6ZM4YILLvjzNVu22IjvH330EcOGDQMgIyODWgndQR177LH06tWLrVu3cvbZZ9OiRYt8yz/99FPOOeccdguu4Z577rl88skndO3alaZNm/65/jHHHMOiRYtKuOQlyy/y74xly2DoUOv6xXtAdi7tJDb1jU3HfvRzc3OpXbs233zzzZ+P77//Pqltn3jiiUyePJn69evTs2fPP5NRjMYGICxAlSpV/nyekZHBtm3bknrPqHiC2VGffAKNGkHfviACRx0VdUTOuRL2yy+/8PnnnwMwYsQI2rZtm295zZo1adq0KW8EPaGrKjNnzgTs1NkzzzwDWGOAdevW5Xvtzz//TL169bj66qvp3bs3M2bMyLf8xBNPZOTIkWzcuJENGzbw9ttv065du1DKGTZPMMnIzYWpU2HyZDjxRGjYEMaMgblzvW8x59JQ8+bNefHFFzniiCP47bffuPbaa7db55VXXmHw4MEceeSRHHroobzzzjsAPP7440ycOJHDDz+cY445htmzZ+d73aRJk2jRogVHHXUUb775JjfeeGO+5UcffTRXXHEFxx13HK1ateKqq67iqDJ6ICtFVcdKo5YtW+q0adNS+6Zvvgnnn583/cgjcNNNqY3BuXLi+++/p3nz5pG9/6JFi+jSpQvfffddZDHsqoI+QxGZrqotUxmHX+RPxqRJkJFhQxxXrgytW0cdkXPOlXqeYIqyfj3ccgs895zdONmxY9QROedC1qRJkzJdeylNPMEkWrECXnrJ7sz/z39g7VqbP3hwtHE551wZ4wkmXm4unHceBDdSAdC4sd1QWa9edHE551wZ5K3I4j33nCWX55+3MV02b7abKT25OOfcDvMaTIwq3HUXnHKK9TFWisZUcM65sshrMADbtsHNN8Pvv0OHDp5cnHMlatGiRRwW9PoxadIkunTpEnFEqeE1mHXr4OSTYcYMG7/lvPOijsg5V0qoKqpKhQp+LL4zymeCefVVePllez5mjP295x7o0wf23ju6uJxz+UXQX3+sW/yTTz6Zzz//nH79+vHss8+yZcsW9t9/f1544QWqV6/O1KlTufHGG9mwYQNVqlRhwoQJrF69mp49e7JhwwYAnnrqqWIHGktn5TMtDxpkN08uWwZHHw3dulmC8eTinMO667/ssssYN24cgwcPZvz48cyYMYOWLVvyyCOPkJ2dTffu3Xn88ceZOXMm48ePp2rVqtSrV49x48YxY8YMXnvtNW644YaoixKp8lmDWboUjj8ePvww6kicc0WJqL/+xo0b07p1a0aPHs2cOXP+7Lo/OzubNm3aMG/ePPbZZx+OPfZYwDq/BNiwYQN9+/blm2++ISMjg/nz50cSf2kRaoIRkc7A40AG8LyqPpiwvAowDDgGWA10V9VFoQW0davdjf/9997NvnOuULFu+VWVjh07MmLEiHzLZ82atV2X/gCPPvooe+21FzNnziQ3N5fMcj74YGinyEQkA3gaOB04BOghIockrNYb+F1VDwAeBf4TVjyoWhf7H38MZ51lHVY651wRWrduzWeffcaCBQsA2LhxI/Pnz+fggw9myZIlTJ06FYA//viDbdu2sXbtWvbZZx8qVKjASy+9RE5OTpThRy7MazDHAQtUdaGqZgOvAt0S1ukGvBg8/x/QQQo6LCgJzz8PAwfafS4vv2xDGzvnXBH23HNPhg4dSo8ePTjiiCNo3bo1c+fOpXLlyrz22mv89a9/5cgjj6Rjx45s3ryZ6667jhdffJHWrVszf/78P2tC5VVo3fWLyPlAZ1W9KpjuCbRS1b5x63wXrJMVTP8YrLMqYVvXANcANGrU6Jiff/55xwP65BN4+GF47TWIGxXOOVe6RN1dfzooLd31h1mDKagmkpjNklkHVR2oqi1VteWee+65c9G0awcjR3pycc65FAkzwWQBDeOmGwBLCltHRCoCtYDfQozJOedcioSZYKYCzUSkqYhUBi4CRiWsMwq4PHh+PvCRlrUhNp1zJc5/BnZeafrsQkswqroN6At8AHwPvK6qs0XkPhHpGqw2GKgjIguAm4E7worHOVc2ZGZmsnr16lL1Q1lWqCqrV68uNc2jQ7vIH5aWLVvqtGnTog7DOReSrVu3kpWVxebNm6MOpUzKzMykQYMGVKpUKd/8KC7yl887+Z1zpValSpVo2rRp1GG4ElA++yJzzjkXOk8wzjnnQuEJxjnnXCjK3EV+EVkJ7MSt/ADUBVYVu1b68vKX3/KX57JD+S5/rOyNVXUn71TfOWUuwewKEZmW6lYUpYmXv/yWvzyXHcp3+aMsu58ic845FwpPMM4550JR3hLMwKgDiJiXv/wqz2WH8l3+yMperq7BOOecS53yVoNxzjmXIp5gnHPOhSItE4yIdBaReSKyQES266FZRKqIyGvB8i9FpEnqowxPEuW/WUTmiMgsEZkgIo2jiDMMxZU9br3zRURFJK2ariZTfhG5MPj+Z4vI8FTHGKYk9v1GIjJRRL4O9v8zoogzDCIyRERWBCMFF7RcROSJ4LOZJSJHhx6UqqbVA8gAfgT2AyoDM4FDEta5Dng2eH4R8FrUcae4/CcD1YLn16ZL+ZMpe7BeDWAy8AXQMuq4U/zdNwO+BnYPputFHXeKyz8QuDZ4fgiwKOq4S7D8JwJHA98VsvwM4D1sJOHWwJdhx5SONZjjgAWqulBVs4FXgW4J63QDXgye/w/oICIFDd9cFhVbflWdqKobg8kvsNFG00Ey3z3A/cBDQLr1B59M+a8GnlbV3wFUdUWKYwxTMuVXoGbwvBbbj7JbZqnqZIoeEbgbMEzNF0BtEdknzJjSMcHUBxbHTWcF8wpcR21gtLVAnZREF75kyh+vN3ZUkw6KLbuIHAU0VNXRqQwsRZL57g8EDhSRz0TkCxHpnLLowpdM+fsDl4pIFjAW+GtqQisVdvS3YZel43gwBdVEEttiJ7NOWZV02UTkUqAlcFKoEaVOkWUXkQrAo8AVqQooxZL57itip8naYzXXT0TkMFVdE3JsqZBM+XsAQ1X1YRFpA7wUlD83/PAil/LfvXSswWQBDeOmG7B9NfjPdUSkIlZVLqpqWZYkU35E5FTgLqCrqm5JUWxhK67sNYDDgEkisgg7Dz0qjS70J7vvv6OqW1X1J2AelnDSQTLl7w28DqCqnwOZWGeQ5UFSvw0lKR0TzFSgmYg0FZHK2EX8UQnrjAIuD56fD3ykwVWwNFBs+YPTRM9hySWdzsEXWXZVXauqdVW1iao2wa4/dVXVdBmDO5l9fyTWyAMRqYudMluY0ijDk0z5fwE6AIhIcyzBrExplNEZBVwWtCZrDaxV1aVhvmHanSJT1W0i0hf4AGtVMkRVZ4vIfcA0VR0FDMaqxguwmstF0UVcspIs/3+B6sAbQduGX1S1a2RBl5Aky562kiz/B8BpIjIHyAFuVdXV0UVdcpIs/9+AQSJyE3Z66Ip0ObgUkRHYqc+6wTWme4BKAKr6LHbN6QxgAbARuDL0mNLks3XOOVfKpOMpMuecc6WAJxjnnHOh8ATjnHMuFJ5gnHPOhcITjHPOuVB4gnE7TUTqiMg3wWOZiPwaPF8TNIMt6fdrLyI71MWLiEwq6EZKEblCRJ4qYH4VERkflKP7LsR6mYh8F/RYPEdEbgnmDxWR83d2uwnvsa+I/C9uekTQS+5NInJfcDPtjm6ziYhcHDfdUkSeKIl4XfmTdvfBuNQJ7p9oASAi/YH1qvp/YsMfFJsIRKRi0BdcaXIUUElVWyT7AhHJUNWcuOnTgX7Aaaq6REQygZ4lHaiqLsFuFEZE9gaOV9VdHXqhCXAxMDx4j2lAutyI6lLMazAuLBkiMig4gv9QRKrCnzWKf4nIx8CNIrKniLwpIlODxwnBeifF1Y6+FpEawXari8j/RGSuiLwS6wVbRDoE630rNi5GlcSARORKEZkfvPcJBSyvB7wMtAjed//Ctisii0TkbhH5FLggYVN3ArcECQBV3ayqgwp4v7uDMn8nIgPjynKD5I3X82phn0dQ24iN/fEhUC9Y3i6+piQix4rIFBGZKSJfxb32ExGZETyOD7bzINAu2M5N8bVGEdlDREYGcX0hIkcE8/sHn80kEVkoIjcUt3O4ciKV4xX4I30fWC+1twTPmwDbgBbB9OvApcHzScCAuNcNB9oGzxsB3wfP3wVOCJ5Xx2rb7bGerxtgB0efA22x7j4WAwcG6w8D+sW9X0tgH6ybkD2xsUI+A54qoBztgdHB86K2uwi4rZDP4jegViHLhgLnB8/3iJv/EnBW8HwJUCV4XruIz6MJwdgf8c/j3yco60Lg2GB+zeC11YDMYF4z7E73fOUv4PN4ErgneH4K8E3cdz8FqIL167UaqwVGvl/6I9qH12BcWH5S1W+C59OxH8CY1+Kenwo8JSLfYH0l1QxqK58BjwRHw7U171TaV6qapdb77TfBdg8K3m9+sM6L2OBL8VoBk1R1pdpYIa9RvOK2m8w2inKy2Iiq32I/2IcG82cBr4j1dh0rd2GfR3EOApaq6lQAVV0XvLYS1mXKt8Ab2OBbxWmLJUJU9SOgjojUCpaNUdUtqroKWAHslWR8Lo15gnFhie+hOYf81/s2xD2vALRR1RbBo76q/qGqDwJXAVWBL0Tk4CK2m+xgcTvaL1Jx291QyPzZwDFFbtiuywzAajOHA4OwGhPAmcDTwTamB9eqCvs8iiMUXO6bgOXAkVgNr3KS20oU23ZR37crpzzBuKh9CPSNTYhIrHWsd3UAAAFXSURBVNHA/qr6rar+B7vIXNQP6lygiYgcEEz3BD5OWOdLoL1Yy7dKbH/dZGe3W5B/Aw8FF95jLdMSr0vEkskqEalO3sX6CtiAaBOB24Da2HWnHfk8Esuwr4gcG2y/huQNUbE0qAn2xDqHBPgDG9agIJOBS4LttAdWqeq6JONw5ZAfZbio3QA8LSKzsP1xMtAH6CciJ2NHw3OwUTfbFLQBVd0sIldivUNXxLptfzZhnaVBS7fPgaXADPJ+VAuUzHYLed1YEdkLGB9cuFdgSMI6a0RkEPAtdj1narAoA3g5OPUkwKPBuvcX8HkUO9ytqmaLNbd+UqyhxSbstOQA4E0RuQCYSF5tbBawTURmYtdxvo7bXH/gheC72kjekBfOFch7U3bOORcKP0XmnHMuFJ5gnHPOhcITjHPOuVB4gnHOORcKTzDOOedC4QnGOedcKDzBOOecC8X/B/W2d20dFrgzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot precision-recall curve for suggested model. Code adapted from Lab 4.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get 18-month training data\n",
    "x_train, x_test, y_train, y_test = test_train[0]\n",
    "x_train = x_train.drop(labels=['date_posted'], axis=1)\n",
    "x_test = x_test.drop(labels=['date_posted'], axis=1)\n",
    "\n",
    "# Train\n",
    "boost = AdaBoostClassifier(n_estimators=10, random_state=0)\n",
    "trained = boost.fit(x_train, y_train)\n",
    "y_scores = boost.predict_proba(x_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores, pos_label=1)\n",
    "population = [sum(y_scores > threshold)/len(y_scores) for threshold in thresholds]\n",
    "\n",
    "# Plot\n",
    "p, = plt.plot(population, precision[:-1], color='b')\n",
    "r, = plt.plot(population, recall[:-1], color='r')\n",
    "plt.title('Precision-recall curves for AdaBoost classifier w/ 10 estimators')\n",
    "plt.xlabel('Threshold for Classification')\n",
    "plt.ylabel('Precision/recall score')\n",
    "plt.legend([p, r], ['precision', 'recall'])\n",
    "plt.savefig('output/best_model_precision.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 How do the results change over time?\n",
    "\n",
    "For each classifier used, we trained models on training data over three different lengths: 6, 12, and 18 months prior to the 6 months of data used to test our models. For example, since we have data from Jan 1, 2012 to Dec 31, 2013, the longest training data used was from Jan 1, 2012 to June 30, 2013, with July 1, 2013 to Dec 1, 2013 forming the test set. \n",
    "\n",
    "In almost every case, models trained on 18 months of training data outperformed the other two variants. However, models trained on 6 months of training data outperformed those trained on 12 months of training data across almost every metric (accuracy, f1, precision at various thresholds). If looking only at the AUC-ROC - a measure of balance between true and false positive rates, called the area under the curve (AUC) for the receiver operating characteristics (ROC) curve - the expected order appears, where models trained on 18th months of data outperform those trained on 12 months of data, which themselves outperform those trained on 6 months of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What would be your recommendation to someone who's working on this model to identify 5% of posted projects to intervene with? Which model should they decide to go forward with and deploy?\n",
    "\n",
    "As always, this depends on the specific intervention and its goal. Is it to maximize overall fundraising dollars? To maximize the number of projects that are successfuly funded? (Fortunately, since DonorsChoose stops fundraising for a project once the intended goal amount is met, these are functionally the same thing.)\n",
    "\n",
    "But the question remains. Which 5% of projects do we want to intervene on? If selecting projects deemed *most* likely to be fully funded in 60 days, those would be the projects probably likely to be fully funded without intervention at all. In this case, DonorsChoose could use \"exciting\" projects like these as a source of marketing material, or target the donor pools to those projects for additional giving opportunities on other projects. If so, I would recommend using the **boosting ensemble model trained on 18 months of data**, which performed the best on precision at 5%. In other words, of the top 5% of projects it identified as likely to be fully funded in 60 days, 95% of them were actually so. \n",
    "\n",
    "If intervening on the 5% of projects *least* likely to be funded within 60 days, or perhaps projects that were unlikely to do so without a nudge or sorts, the potential intervention here could be additional fundraising support in the form of DonorsChoose staff coaching the teacher involved on how to better ask for donations from their networks. In this case, the metric of interest would be the precision at 50% (assuming projects that are 50% likely to be funded within 60 days are on that threshold), and the model that performs best on this metric is still the **boosting ensemble model trained on 18 months of data**.\n",
    "\n",
    "Perhaps the metric we want to maximize is the number of students reached by the 5% of projects we will intervene on. The specific interventions here could be any of the actions already mentioned above. In this case, we would select the combination of projects where the *expected* number of students reached (calculated by the likelihood of being fully funded in 60 days, multiplied by the number of students reached by that project) is the largest possible. Without prior knowledge of how the number of students reached is distributed among projects, we would do best here to maximize overall precision. Fortunately, the model that does this is still the **boosting ensemble model trained on 18 months of data*.\n",
    "\n",
    "There are other considerations that we have not explored here. The specific model we choose to inform our interventions can depend on a whole host of factors. Should projects in schools in low-income neighborhoods get priority? Should projects that ask for specific types of resources (e.g. stationery vs. computers) get priority? Should projects that are most cost-efficient, serving the largest number of children for the lowest per-child dollar amount, get priority? If we know more about the specific ways DonorsChoose might seek to use this model to identify projects, and the range of interventions available to them, we could provide more useful recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
